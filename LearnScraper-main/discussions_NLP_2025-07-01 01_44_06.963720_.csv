,name,date,board,text
0,Pawel Orzechowski,12 Dec 2024 12:40,Help us to make this course better: Bugs and Features,deal more gracefully with downloads to new folders
1,Pawel Orzechowski,7 Dec 2024 16:14,Help us to make this course better: Bugs and Features,in some places eg. badge 5 many code chunks have the same id eg. 'wordcloud' so cannot be knitted
2,Franz KrÃ¤mer,26 Nov 2024 22:38,Help us to make this course better: Bugs and Features,"In badge 11, in the chunk after the MLmetrics package is loaded, the argument order for Precision() seems to be mixed up a bit â€“ it's prediction, actual while the documentation says it must be y_true, ypred. I calculated precision and recall by hand and it just wouldn't add up with the printed results. ğŸ˜…"
3,Pawel Orzechowski,25 Nov 2024 13:26,Help us to make this course better: Bugs and Features,"Error in hint hint_11_3. Well spotted @Dylan

---
In the notes we have:

T1 Disease 10 18 Diabetes - FALSE POSITIVE (**THIS ONE WAS WRONG**)
T2 Drug 30 36 Insulin - TRUE POSITIVE (in both gold and pred)
T3 Disease 50 58 Diabetes - FALSE NEGATIVE

PRED:

T1 Disease 10 19 Diabetes - FALSE POSITIVE
T2 Drug 30 36 Insulin - TRUE POSITIVE (in both gold and pred)


So in the notes it should be:

GOLD:

T1 Disease 10 18 Diabetes - FALSE NEGATIVE (it was not predicted, but it IS there. Because of the typo in 18/19)
T2 Drug 30 36 Insulin - TRUE POSITIVE (in both gold and pred)
T3 Disease 50 58 Diabetes - FALSE NEGATIVE (it was not predicted, but it IS there)

PRED:

T1 Disease 10 19 Diabetes - FALSE POSITIVE (it was predicted, but it IS NOT there. because of the typo in 18/19)
T2 Drug 30 36 Insulin - TRUE POSITIVE (in both gold and pred)"
4,Pawel Orzechowski,"20 Nov 2024 15:08
(Edited by Pawel Orzechowski on 20 Nov 2024 15:15)",Help us to make this course better: Bugs and Features,"What to do when HINTS are not opening correctly?
When hints are not opening correctly (usually after restarting the RStudio) it means that RStudio LOST CONTEXT of what project it should be referring to (so it does not where to look for hints)."
5,Franz KrÃ¤mer,15 Nov 2024 18:35,Help us to make this course better: Bugs and Features,"I would kindly like to make a suggestion regarding the documentation of the code. I stumbled upon this when working on Badge 8. In the notebook there are some custom functions. When trying to work through them, I found it occasionally a bit hard to grasp what their arguments are and where these arguments will come from and what the result is expected to be. Maybe it would make things a bit clearer if there was a short docstring above such functions that tells about the parameters and expected results. Maybe such docstrings could even be used as a way of explaining things. Howevery, this is probably more relevant for the next round of the course, where a short introduction into how to read these docstrings could be provided."
6,Franz KrÃ¤mer,14 Nov 2024 17:20,Help us to make this course better: Bugs and Features,"In badge 7, where we start using the quanteda package for feature extraction, corpus() from quanteda uses ""df"" as the first argument. For the code to work, it should be ""Corona_NLP_DF"". I typed a few notes for myself, so I cannot provide the exact line (sorry!), but it's in the code chunk below ""## Method 2: Using *quanteda*""."
7,Pawel Orzechowski,12 Nov 2024 18:27,Help us to make this course better: Bugs and Features,"in badge 07, uncomment line 304 or pacman that library"
8,Rebecca Sewell,10 Nov 2024 16:44,Help us to make this course better: Bugs and Features,"In badge 7, I am having trouble retrieving the GloVE file

Running this:
options(timeout = 600)
download.file(""http://nlp.uoregon.edu/download/embeddings/glove.6B.100d.txt"", 
       ""./data/glove.6B.100d.txt"") 

I am receiving this error:
Warning messages:
1: In download.file(""http://nlp.uoregon.edu/download/embeddings/glove.6B.100d.txt"",  :
  URL http://nlp.uoregon.edu/download/embeddings/glove.6B.100d.txt: cannot open destfile './data/glove.6B.100d.txt', reason 'No such file or directory'
2: In download.file(""http://nlp.uoregon.edu/download/embeddings/glove.6B.100d.txt"",  :
  download had nonzero exit status"
9,Rebecca Sewell,10 Nov 2024 15:00,Help us to make this course better: Bugs and Features,"In badge 7, creating the DTM with tm package seemed to remove stop words and lower the case in the process. Then in the next step we were asked to try doing these things to the corpus, but they were in fact already done I think?

I may have got this wrong, if so please let me know!"
10,Pawel Orzechowski,12 Dec 2024 15:03,"Forum: Coding, Course and Assessment Questions",How to download all of my course materials so that I have them for later or to run them on my own computer?
11,Joseph Oxley,11 Dec 2024 13:49,"Forum: Coding, Course and Assessment Questions","Hi,

I was just wondering if there's an expected timeline to get results back for the assignment?

(Just so I know when to check back!)

Thanks,

Joseph"
12,Isabel Santonja,8 Dec 2024 21:52,"Forum: Coding, Course and Assessment Questions","Hi,
I wanted to answer the feedback survey, but I cannot open the link. Is someone having the same problem?
Best,
Isabel"
13,Nisha Daniel,7 Dec 2024 06:39,"Forum: Coding, Course and Assessment Questions","hi,

i think i am asking a basic question in the end:
is guardian_articles which is given in tibble format or we need to change it to as.tibble?

i see that its working in some areas it works and somewhere else it does not.

thanks"
14,Rebecca Sewell,7 Dec 2024 00:32,"Forum: Coding, Course and Assessment Questions","Hello, I am having trouble with the knit part and exporting. I am using only one of the five sets of articles and not getting any errors, and my R (Noteable) isn't crashing but when I go to knit it, it just runs and never completes (progress circle keeps going). I have waited 10min+ and no change."
15,Franz KrÃ¤mer,6 Dec 2024 14:31,"Forum: Coding, Course and Assessment Questions","Hi course team, I'm currently using (somewhat granular) markdown headings and the outline function of RStudio in my rmd to keep track of my work (even within a learning). Do you want these headings deleted before submittal or can/should they stay in the text? I think they would make the code more human-readable but in case these headings counted towards the total word count I'd have to delete them as they take up too much of the allowed character count."
16,Nisha Daniel,5 Dec 2024 11:45,"Forum: Coding, Course and Assessment Questions","hi

iam constantly encountering this problem while trying to make bi-gram:

Error in `pull()`:
! Can't extract column with `!!enquo(var)`.
âœ– `!!enquo(var)` must be numeric or character, not a function.
Backtrace:
  1. my_guardian %>% ...
  2. tidytext::unnest_tokens(...)
  4. dplyr:::pull.data.frame(tbl, !!input)
  5. tidyselect::vars_pull(names(.data), !!enquo(var))
  6. tidyselect:::pull_as_location2(...)
  9. vctrs::vec_as_subscript2(...)
 10. vctrs:::result_get(...)
 
Show Traceback
Error in pull(tbl, !!input) :

what should be the issue? and how can i rectify it?

thanks"
17,Dylan Delmar,4 Dec 2024 23:28,"Forum: Coding, Course and Assessment Questions","Hi, I was wondering: in the assessment final file there is a lot of text before the code chunks start regarding the assignment and marking, should we leave this in our final knit file or can/should we delete that for our final file?

Thanks!

Also a note for anyone else running into memory issues in Noteable, I've added iterations of this:
rm(list=setdiff(ls(), c(""list of global variables"", ""to keep""))) #all variables not listed will be cleared from the environment
gc() #free unused memory
throughout my code which is helping my code run top to bottom and knit without any crashing issues"
18,Amit Mishra,3 Dec 2024 20:13,"Forum: Coding, Course and Assessment Questions","I wish to let you know that I have not been able to work on Notable since this afternoon. It doesn't open most of the time and if opens it hangs and throws this ugly ""out of memory"" message. Though there is this message of the university system not working from early morning tomorrow for an hour on the learn page Notable has stopped working from today afternoon. It was working fine till the afternoon. "
19,Nisha Daniel,2 Dec 2024 11:42,"Forum: Coding, Course and Assessment Questions","hi,
https://raw.githubusercontent.com/drpawelo/data/refs/heads/main/health/nlp_guardian_batch_1.csv
is there an alterative path to receive the file in my laptop Rstudio?

as this works only in noteable for me. did anyone face this? if yes, howd you get through it?
Warning in download.file(remote_file_name, local_file_name) :
  URL https://raw.githubusercontent.com/drpawelo/data/refs/heads/main/health/nlp_guardian_batch_1.csv: cannot open destfile './data/guardian_articles_1.csv', reason 'No such file or directory'
this is the error that shows. need help here!"
20,Nasser Gaafar,"1 Dec 2024 18:32
(Edited by Nasser Gaafar on 1 Dec 2024 18:32)","Forum: Coding, Course and Assessment Questions","Timeout of 60 seconds was reachedError in download.file(remote_file_name, local_file_name) : 
  download from 'https://raw.githubusercontent.com/drpawelo/data/refs/heads/main/health/nlp_guardian_batch_1.csv' failed
It seems the Rstudio network is experiencing technical difficulties tonight.."
21,Isabel Santonja,1 Dec 2024 16:39,"Forum: Coding, Course and Assessment Questions","Hi,
I have a couple of questions regarding the assessment:
1) Do I need to reference some of the ideas in the introduction?
2) I often use GenAI to proof read my manuscripts or to help my de-bug some code. Can I do that for this assessment too?
3) In case yes, should I ackowledge that in my assessment with a statement like ""I acknowledge the use of ELM to help me de-bug some of the code and proof read my final draft.""? Where should I place the ackowledgent and does it contribute to the word count?

Thanks in advance for your reply!"
22,Pawel Orzechowski,27 Nov 2024 13:31,"Forum: Coding, Course and Assessment Questions","Q: I keep running out of memory, or my computer/noteable keeps crashing. What should I do?"
23,Pawel Orzechowski,27 Nov 2024 13:29,"Forum: Coding, Course and Assessment Questions",Q: What's the best order in which to recap badges and use them for assessment?
24,Pawel Orzechowski,27 Nov 2024 13:26,"Forum: Coding, Course and Assessment Questions","Q: How can I shrink the dataset, so that it does not overwhelm my computer?"
25,Pawel Orzechowski,27 Nov 2024 13:18,"Forum: Coding, Course and Assessment Questions",Q: I am stuck at not being able to download the files! Can you help me start?
26,Pawel Orzechowski,27 Nov 2024 12:31,"Forum: Coding, Course and Assessment Questions","Q: whole dataset (guardian_articles) is too large, and crashes my noteable/laptop. Can I just use a section of it? like the guardian_articles_1 which has just 20% of the data?"
27,Pawel Orzechowski,27 Nov 2024 12:25,"Forum: Coding, Course and Assessment Questions",Q: How much should I separate code for each learning (eg. data Cleanup code)
28,Pawel Orzechowski,"27 Nov 2024 11:34
(Edited by Pawel Orzechowski on 27 Nov 2024 11:34)","Forum: Coding, Course and Assessment Questions",SCALING! WHAT TO DO WHEN YOUR R IS CLOSING DOWN (usually because it run out of memory)
29,Joseph Oxley,26 Nov 2024 13:33,"Forum: Coding, Course and Assessment Questions","Hi, just a couple questions about the assignment:

Does all of the assignment explanation etc need to be submitted in the files or should this be removed? (So it would start with loading the data on line 133)
Can you use work from prior learnings in the next one? Or does each learning need to be standalone?

Thanks,

Joseph"
30,Dylan Delmar,20 Nov 2024 23:56,"Forum: Coding, Course and Assessment Questions","Hello,

In the badge 11 notebook when working on the gold vs. pred annotations, I found the differences fine and worked out some code for positives and negatives, but when I checked the hint to make sure I didn't miss anything there was this information on types (false negative vs. false positive):

GOLD:

T1 Disease 10 18 Diabetes - FALSE POSITIVE
T2 Drug 30 36 Insulin - TRUE POSITIVE (in both gold and pred)
T3 Disease 50 58 Diabetes - FALSE NEGATIVE

PRED:

T1 Disease 10 19 Diabetes - FALSE POSITIVE
T2 Drug 30 36 Insulin - TRUE POSITIVE (in both gold and pred)

I'm unsure where this information came from. Was it randomly assigned or am I missing an additional text file to check the annotations against?

Thanks!"
31,Nasser Gaafar,19 Nov 2024 11:11,"Forum: Coding, Course and Assessment Questions","Hi everyone, I'm in a bit of a pickle on badge 7. I'm trying to use Quanteda with the Corona dataset as requested in the task starting from line 93 to create a dfm but then the code chunk starting on line 117 reverts back to the 3 sentence Taylor Swift example from before. I tried running the code with the Corona corpus code from the previous code chunk lines 95-104 but kept getting an error relating to tokens(). Has anyone attempted this task? Would really appreciate the help!"
32,Amit Mishra,29 Oct 2024 15:45,"Forum: Coding, Course and Assessment Questions","Hi,

I was reviwing the 2b regex and stumbled upon this code-

cat(""bana\nas are yellow, or\t brown\\yellow "") (line# 146)

the output was like this-

bana
as are yellow, or  brown\yellow 

Why in the output after the first \ (bana) the line changed and not after the second \ (or)?"
33,Nasser Gaafar,3 Nov 2024 07:45,Your turn: Say Hello and tell everyone what do you EXPECT to gain from this course,"Hey everyone, my name is Nasser and I'm a student on the MSc Data Science for Health & Social care programme. I've been in and around healthcare for around 13 years, and now I work in a university. I am excited to learn about NLP in order to understand what goes on in the background and inner workings of the analytical process, and hopefully learn enough to be able to apply it to my own work and research."
34,Kate Rose,"28 Oct 2024 22:18
(Edited by Kate Rose on 28 Oct 2024 22:22)",Your turn: Say Hello and tell everyone what do you EXPECT to gain from this course,"Hey everyone, my name is Kate and I'm a student on the MSc Data Science for Health & Social care programme. I've worked in the healthcare space for 9 years in the UK and Australia, both in clinical and 'digital health' roles.

What I expect to get out of the course: I was introduced to R in our Intro to Data Science module(s) during year 1 of the MSc DSHC programme, so I am excited to build on this knowledge and skills in a new context. Outside of the programming elements, I'm really looking forward to learning about application(s) of NLP in the acute hospital setting. I've worked on creating clinical decision support tools for early warning scores/deteriorating patients as part of my current role, so I'm particularly interested to learn about the feasibility of NLP application in this context and current healthcare practice generally. Without being super familiar at this point, I'm expecting NLP capability to be more flexible and powerful; I'm curious to find out if my outset assumptions are true as we progress through the course"
35,Isabel Santonja,27 Oct 2024 07:50,Your turn: Say Hello and tell everyone what do you EXPECT to gain from this course,"Hi everyone!
My name is Isabel, and Iâ€™m a student in the MSc program in Data Science for Health and Social Care. I work as an epidemiologist, so Iâ€™m mostly involved with quantitative data and not as much with language. Iâ€™m taking this course out of curiosity, and Iâ€™m very excited about it! ğŸ¤©"
36,Franz KrÃ¤mer,25 Oct 2024 16:05,Your turn: Say Hello and tell everyone what do you EXPECT to gain from this course,"Hi everyone! My name is Franz, I'm an educational scientist (so not exactly from the health or social care sector) and I'm enrolled in the Data Science, Technology and Innovation programme. I hope that at the end of the course I will be able to use NLP for my research and some EdTech ideas I'm developing at the moment.
I'm very much looking forward to the course!"
37,Yi mei Low,25 Oct 2024 11:28,Your turn: Say Hello and tell everyone what do you EXPECT to gain from this course,"Hi all! I'm Yi Mei, a physician working in internal medicine with an interest in health informatics. I'm excited to dive into NLP and understand how we can better integrate AI into our health systems"
38,Amit Mishra,23 Oct 2024 19:30,Your turn: Say Hello and tell everyone what do you EXPECT to gain from this course,"Hello there,

My name is Amit Mishra. I am a public health physician who has worked with health information systems implementation in many settings. I see a lot of value in NLP to extract maximum value from HIS. I am excited to be in this place and look forward to learning NLP."
39,Joseph Oxley,23 Oct 2024 14:28,Your turn: Say Hello and tell everyone what do you EXPECT to gain from this course,"Hi everyone, I'm Joseph and I'm very much looking forward to this course. I currently work as a data analyst in the NHS in England, but I'm looking to develop further technical skills to go further down the data science or medical research route.

My background is in theoretical physics, so I'm very much interested in learning more about natural language processing and machine learning. I find the mathematics behind a lot of the concepts incredibly interesting, so very excited to learn more in a practical context!"
40,Swe Lynn,23 Oct 2024 12:30,Your turn: Say Hello and tell everyone what do you EXPECT to gain from this course,"Hello, my name is Swe Lynn. Excited about this course. I am paediatrician and interested in data science and programming. Enjoyed last year python course thoroughly and so impressed with Pawel's teaching style.

I believe clinicians need to know about AI and machine learning and I expect this course will become one of solid foundations for my attempt to understand AI and machine learning as a clinician."
41,Pawel Orzechowski,21 Oct 2024 14:17,Your turn: Say Hello and tell everyone what do you EXPECT to gain from this course,"Hello, My name is Pawel Orzechowski and I'll be one of your lecturers on this course. I've been programming for about 15 years and teaching for about 10 of those years. At Usher I teach courses about the good crft of programming and about using specialistic coding skills in Health and Scila Care context. In the past I worked in healthcare as a carer, as a programmer and as an entrepreneur. Those gave me a lot of interesting insights into what works well in this context. Teaching-wise I am very passionate about the 'human' elements of coding (here's a video introduction to one of my other courses https://media.ed.ac.uk/media/Introduction+to+Software+Development+in+Health+and+Social+Care/1_wk3zo6cn).

What I expect from this course:
I've taught NLP courses before but they were in Python and in context of Humanities (mainly Law, History and Politics). I am excited to learn from Arlene more about how things are done in R, and how NLP is used in Care."
42,Yingqian Tang,8 Dec 2024 16:38,Badge 01 - Mini-Diary: 3 stars and 1 wish â­ï¸â­ï¸â­ï¸ğŸš€,"â­ Impact in Health & Social Care:
helps extract valuable insights from free-text health data, improving outcomes and efficiency.
â­Challenges of Text Data:
Ambiguities, typos, and inconsistencies in health records make text data hard to analyze.
â­Ethics in NLP for Health:
Data privacy, consent, and bias in models are crucial ethical concerns when working with health data.
ğŸš€ One thing I wish I understood better:
Iâ€™d like to understand how deep learning models are applied specifically to health data, especially for handling large datasets."
43,Nisha Daniel,4 Dec 2024 16:36,Badge 01 - Mini-Diary: 3 stars and 1 wish â­ï¸â­ï¸â­ï¸ğŸš€,"â­ What is NLP and its relevance/applications till speech recognition
â­ Evolution of NLP from preprocessing to feature extraction to ml/statistical model
â­ and that, despite being technical, Ethics should be central to our approach in working with data
â“ how to work with highly challenging labelled data especially in the hospital set up."
44,Paolo Pricoco,18 Nov 2024 09:51,Badge 01 - Mini-Diary: 3 stars and 1 wish â­ï¸â­ï¸â­ï¸ğŸš€,"â­ Patterns in the language are used to predict words and parts of speech in texts. For example, after an ""a"" or an ""an"", it's very likely that we'll see a noun. Rule-based methods in NLP are relatively easy to understand by humans and don't require a huge datasets to be kept track of. However, they require all rules to be set manually.

â­ In ML-based NLP, after pre-processing we perform ""Feature extraction"". Features are qualities that characterize words; for example, they can be different parts of speech. Limitations of ML-based approaches: they require supervised learning, that is, words must be labelled. This implies the action of professionals (e.g., clinicians in the health care field) to manually establish features and labels.

â­ The outstanding feature of DL is that it doesn't require manually input data. As larger and larger datasets are being fed into DL-based AI, these programs are becoming increasingly better at understanding the intricacies of human language, including its ambiguities. However, DL requires large amounts of data and computationally expensive infrastructure (GPU, memory, etc.). Also, sometimes they can be like a ""black box"", meaning that it can be difficult for us to explain why certain decisions are made, which is can be ethically crucial in health care.

â“ I am wondering about how health care institutions and ethics boards can assess whether public interventions based of AI-made decisions are fully appropriate and ethical. I'm also wonder whether, in order to achieve this in the complexity of the scenario, we are going to need further AI-based programs for the decision making on the decision making."
45,Kate Rose,13 Nov 2024 11:56,Badge 01 - Mini-Diary: 3 stars and 1 wish â­ï¸â­ï¸â­ï¸ğŸš€,"â­ Deep learning applications of NLP outperform other models, but this approach still relies on intensive and expensive computational infrastructure
â­ Deep learning model risk being a 'black box' - decision making modelling is not transparent nor always explainable which is a huge consideration ethically and from a clinical safety perspective when applied to H&SC
â­ Text data is challenging for a variety of reasons: synonymy, syntactic ambiguity, abbreviations, typos, and negation are all variables in language which impact standardisation and consistency
â“ Curious to learn more about sentiment analysis in unstructured text and how we can objectively translate this into a model output"
46,Robin Nicholls,9 Nov 2024 17:53,Badge 01 - Mini-Diary: 3 stars and 1 wish â­ï¸â­ï¸â­ï¸ğŸš€,"â­ï¸ ELIZA was the first chat bot in 1964!
â­ï¸ low resource languages are challenging
â­ï¸ sparsity of words can be challenging for NLP solutions
ğŸš€ How can bias be identified and mitigated in a domain like Health and Social care, which must pay great attention to the potential impact of bias? I will try to answer this by reading literature."
47,Louise Lau,7 Nov 2024 16:25,Badge 01 - Mini-Diary: 3 stars and 1 wish â­ï¸â­ï¸â­ï¸ğŸš€,"â­- The concept of deep learning and its elements like hidden layers, and how it has been applied in NLP.
â­- Example of benefits or tasks that are enabled with NLP, such as sentiment analysis and translation. 
â­- Challenges and ethics consideration when applying NLP in the health domain. 
ğŸš€- Given the week 1 video mentioned that NLP has mostly shifted towards deep-learning, I am curious why R is chosen for this course considered that python is seemingly the dominant choice for deep learning? "
48,Adijat Adenaike,4 Nov 2024 23:27,Badge 01 - Mini-Diary: 3 stars and 1 wish â­ï¸â­ï¸â­ï¸ğŸš€,"â­ Its great learning about the history of Natural Language processing. Particularly learning about the distinction between the methods - rule based, statistical or mechanical learning and deep learning. I had previously thought machine learning and deep learning were the same thing. Its great knowing the differences in the methods.
â­ I enjoyed learning about the distinctness between Text Mining, Text Analytics and Natural Language Processing and how they are also all linked. I had assumed they are all one and the same thing but just given different tags.
â­ Learning about the challenges to NLP gives one room for thought about how complex the whole process can be. I had always wondered about how idioms and multiple meanings in human language are handled in NLP but I had presumed there is some magic that happens when training the modelsğŸ˜‚.
ğŸš€ How do we train models to identify biases? Knowing that humans have their own inherent biases are there methods in NLP that focus on keeping bias out?"
49,Joseph Oxley,4 Nov 2024 14:04,Badge 01 - Mini-Diary: 3 stars and 1 wish â­ï¸â­ï¸â­ï¸ğŸš€,"â­ï¸The history of Natural language processing and how it developed with improvements in processing technology.
â­ï¸The value in being able to access data insights contained within free text, particularly in a health & social care context.
â­ï¸Why NLP is challenging to perform - such as alternate meanings or abbreviations, particularly in medical records.
ğŸš€ More detail on neural networks! Including, what type of activation functions work best for NLP models and what type of output models can try to predict. (e.g can a model be used to predict meaning, a specific word etc)"
50,Kate Browne,"3 Nov 2024 14:39
(Edited by Kate Browne on 3 Nov 2024 19:45)",Badge 01 - Mini-Diary: 3 stars and 1 wish â­ï¸â­ï¸â­ï¸ğŸš€,"â­ï¸ Three main methods of NLP:
rule-based methods;
statistical methods (e.g. Hidden Markov Models, n-grams, Bayesian networks)
deep learning (e.g. recurrent neural networks).
â­ï¸ Advantages & disadvantages of each of these methods.
â­ï¸ Challenges related to using NLP on text data (e.g. lexical, syntactic & semantic ambiguity, contextual understanding etc.)
ğŸš€ Co-reference resolution in NLP."
51,Nasser Gaafar,3 Nov 2024 07:57,Badge 01 - Mini-Diary: 3 stars and 1 wish â­ï¸â­ï¸â­ï¸ğŸš€,"â­ï¸How important it is to include free text in analysis rather than just focusing on numbers in health and social care context
â­ï¸ Just how difficult it can be to analyze free text data due to dialects, acronyms and synonyms among other things
â­ï¸ R can be used in NLP (there seems not to be a limit to what Rstudio can be used for)
ğŸš€ A better understanding of how to analyse qualitative interview text."
52,Helen Wu,2 Nov 2024 15:29,Badge 01 - Mini-Diary: 3 stars and 1 wish â­ï¸â­ï¸â­ï¸ğŸš€,"â­ï¸ The history of NLP over the last few decades starting from the 60's and how it transforms from rule-based t deep learning
â­ï¸ Thinking about how complex language is yet how we can utilise the different methods of machine learning to overcome this
â­ï¸ How helpful NLP can be in terms of healthcare and social care advancements
ğŸš€ Very keen to get to understanding see how the codes come together when it comes to NLP"
53,Swe Lynn,"2 Nov 2024 07:42
(Edited by Swe Lynn on 4 Nov 2024 19:16)",Badge 01 - Mini-Diary: 3 stars and 1 wish â­ï¸â­ï¸â­ï¸ğŸš€,"â­ï¸ The progress of NLP over last six decades and how deep learning has made the progress faster.

â­ï¸ Majority of clinical data are in text form and importance of NLP to analyse this.

â­ï¸ I did not associate R to NLP before and learned a whole new field of R usage.

ğŸš€ Although basic in early development of NLP, I wish to know more about rule based NLP as this is something I can use for smaller set of data at my work."
54,Rebecca Sewell,29 Oct 2024 12:51,Badge 01 - Mini-Diary: 3 stars and 1 wish â­ï¸â­ï¸â­ï¸ğŸš€,"â­ï¸ The possibility of utilising free text health and social care data for informing advancement in healthcare is exciting
â­ï¸ This made me curious about how NLP could be used to enable people to be more informed about their own healthcare, eg. through instant translation into their native language or translation of technical vocabulary
â­ï¸ The specialised input required for labelling of data for supervised learning is an interesting challenge . Could this be overcome by a separate model built to do this or is this adding another 'level of error'?
ğŸš€ I am curious about what is 'under the hood' of the hidden layers in deep learning models and how these were created."
55,Isabel Santonja,29 Oct 2024 05:47,Badge 01 - Mini-Diary: 3 stars and 1 wish â­ï¸â­ï¸â­ï¸ğŸš€,"ğŸŒŸ Deep learning is a very powerful tool for NLP, but it is a black box and computationally very expensive.

ğŸŒŸ NLP is also used for google searches!

ğŸŒŸ 70-80% of data in health and social care is free text

ğŸ Funnily enough, I am looking forward to learning more about data cleaning and see if I can apply any of the techniques to my work. Even if I have spent a lot of time cleaning data, no one has ever taught me how to ğŸ˜‚"
56,Dylan Delmar,28 Oct 2024 21:25,Badge 01 - Mini-Diary: 3 stars and 1 wish â­ï¸â­ï¸â­ï¸ğŸš€,"â­ The history of NLP methods and how they have evolved from manual rule creation to deep learning methods
â­ How NLP is currently in use and the potential for further use in health research
â­ The many challenges that come with NLP, with the complexity of language, abbreviation, slang, temporal affects, and more
ğŸš€ I want to better understand the process of feature engineering and data labelling - what is the process of doing these things"
57,Franz KrÃ¤mer,28 Oct 2024 16:58,Badge 01 - Mini-Diary: 3 stars and 1 wish â­ï¸â­ï¸â­ï¸ğŸš€,"â­ï¸ With 70-80%, so much text data (probably not only in healthcare) is unused â€“ what a treasure.
â­ï¸ Clearer terminology: NLP as the core technology for both text mining and analysis.
â­ï¸ Much work on deep learning explainability remains to be done to harness the full potential of this method for fields where data are most sensitive.
ğŸš€ I wish I understood better how weighting works in the hidden layers in neural networks."
58,Yi mei Low,28 Oct 2024 16:27,Badge 01 - Mini-Diary: 3 stars and 1 wish â­ï¸â­ï¸â­ï¸ğŸš€,"â­ï¸The history of NLP!
â­ï¸ The underlying factors that make it language learning challenge (for example nuance, sayings)
â­ï¸ The current applications of NLP in healthcare
ğŸš€ A better understanding on the process of training AI and how to optimize the datasets that we feed to them"
59,Amit Mishra,28 Oct 2024 16:15,Badge 01 - Mini-Diary: 3 stars and 1 wish â­ï¸â­ï¸â­ï¸ğŸš€,"â­ï¸NLP drives text processing and supports text mining and text analytics.
â­ï¸Usage of NLP includes machine translation, information retrieval, information extraction, sentiment analysis, and Q&A (most popular).
â­ï¸Text data is highly dimensional and each word can be treated as a separate feature, increasing computational requirements.

ğŸš€ I wish there was a simple example of how key information is extracted from a page of text. I hope that this will be addressed as part of this course."
60,Pawel Orzechowski,19 Oct 2024 09:33,Badge 01 - Mini-Diary: 3 stars and 1 wish â­ï¸â­ï¸â­ï¸ğŸš€,"â­ï¸ this is an example answer
â­ï¸ write three things you learned (or which brought you joy)
â­ï¸ while watching video and completing the notebook
ğŸš€ and one thing you wish you understood better (and maybe what step you will take to address it)
---
(write your mini-diary in the above format AS A RESPONSE TO THIS THREAD).
Instead of emoji you can use * ? or any other symbol that you like."
61,Swe Lynn,20 Dec 2024 23:05,Badge 02 - Mini-Diary: 3 stars and 1 wish â­ï¸â­ï¸â­ï¸ğŸš€,"â­ï¸ Importance of type of quotes (single/double) at the beginning and end of string
â­ï¸ String manipulation: packages (stringr, tidyverse)
â­ï¸ Regular expression (RegEX): Anchors, Quantifiers, Contents, Groups
ğŸš€ sep function and collapse function are a bit confusing and wish to know more clearly. It seems there are a few different ways of RegEx and I presume we will get our own style."
62,Yingqian Tang,"8 Dec 2024 17:39
(Edited by Yingqian Tang on 9 Dec 2024 11:44)",Badge 02 - Mini-Diary: 3 stars and 1 wish â­ï¸â­ï¸â­ï¸ğŸš€,"ğŸŒŸString manipulation: Using functions like paste(), substring(), and tolower() to manipulate text in R.
ğŸŒŸRegular Expressions: How to search, replace, and match patterns using regex in R.
ğŸŒŸPractical tasks: Counting characters with nchar() and word occurrences with str_count().
â“Regex syntax: It's still tricky for more complex patterns. Iâ€™ll practice with regex tutorials and real data to improve."
63,Nisha Daniel,4 Dec 2024 16:40,Badge 02 - Mini-Diary: 3 stars and 1 wish â­ï¸â­ï¸â­ï¸ğŸš€,"â­ REgex, yes that's huge logic!! str_ functions and the gsub functions are good enough for most of practical cases.
â­ Concatenation. it's totally new for me.
â­ Some of the string manipulations.
â“I want to know how to become an expert in REgex operations. It is really confusing."
64,Paolo Pricoco,18 Nov 2024 09:54,Badge 02 - Mini-Diary: 3 stars and 1 wish â­ï¸â­ï¸â­ï¸ğŸš€,"â­ When using single and double quotes to define strings, our code may be more prone to error. It would be best for us to choose and adopt only one type of quotes, when possible.

â­ Regular expressions are almost a coding language in themselves. They are very useful as they can be used to find different patterns of characters in strings.

â­ Logical operators, such as and (AND; &) and or (OR; |), can be used within regular expressions.

â“ I am wondering if literally ANY sequence of characters can be identified by using RegEx, or if there are any exceptions to this. For example, could I identify ""\\$"", considering that ""\\"" is generally used as an ""escaped"" sign?"
65,Adijat Adenaike,"13 Nov 2024 20:42
(Edited by Adijat Adenaike on 13 Nov 2024 20:43)",Badge 02 - Mini-Diary: 3 stars and 1 wish â­ï¸â­ï¸â­ï¸ğŸš€,"â­ I learnt about the str_ functions and the gsub functions and how to combine both for greater data manipulation.
â­ Quantifiers control how many times a pattern can match when using regex.
â­ paste function join texts together and the collapse condition serves as a delimiter specifier. paste0 is the shortcut for both.
â“Regex is complicated. Just when I get excited over conquering a pattern, I get stuck on the next problem. I wish there is a cheat sheet that actually helps. I intend to master regex with lots of practice."
66,Kate Rose,13 Nov 2024 14:54,Badge 02 - Mini-Diary: 3 stars and 1 wish â­ï¸â­ï¸â­ï¸ğŸš€,"â­ stringr package gives us access to a multitude of different string manipulation techniques
â­ Regex is a sequence of characters that defines a pattern to be found
â­ Need to double escape a string in ""`.*\s"" R because \ is already reserved for other special characters (\n, \t)
â“ Further practice with regex - there are many quantifiers/anchors and groups and I'd like to better understand practical application of these"
67,Robin Nicholls,9 Nov 2024 17:53,Badge 02 - Mini-Diary: 3 stars and 1 wish â­ï¸â­ï¸â­ï¸ğŸš€,"â­ï¸ When using regex be aware of quotes and escape characters etc...
â­ï¸ stringr documentation was useful to read
â­ï¸R functions for RegEx is useful to know as I am used to Python
ğŸš€ More examples on using OR would be useful. Detail was easy to find online."
68,Louise Lau,7 Nov 2024 18:00,Badge 02 - Mini-Diary: 3 stars and 1 wish â­ï¸â­ï¸â­ï¸ğŸš€,"â­ï¸- A collection of text processing functions / tools available in R, such as strsplit.
â­ï¸- Introduction to regular expression with some examples of transformation, such as changing date format.
â­ï¸- The use of escape string and how different combination of them will result differently.
ğŸš€- What are some potential application of regex - is it common when it comes to large-scale transformation / real-world health analysis?"
69,Joseph Oxley,5 Nov 2024 09:28,Badge 02 - Mini-Diary: 3 stars and 1 wish â­ï¸â­ï¸â­ï¸ğŸš€,"â­ï¸ How a regular expression is used to define a string pattern. Regex was totally new to me so I didn't have an understanding of what it actually meant before this week.
â­ï¸The importance of escaping using \\ because R as a language requires double escaping for some characters. The first \ escapes the character for the R string and the second \ escapes the character within the regex pattern itself.
â­ï¸Use of groups through brackets to identify parts of a pattern. This allows the pattern to be modified and rearranged.
ğŸš€Don't fully understand the difference between collapse and sep arguments in the paste function. It seems like both can be used to obtain the same result?"
70,Kate Browne,3 Nov 2024 17:28,Badge 02 - Mini-Diary: 3 stars and 1 wish â­ï¸â­ï¸â­ï¸ğŸš€,"â­ï¸String manipulation techniques using both base R functions and the stringr package.
â­ï¸Use of metacharacters, quantifiers, character classes and anchors in regular expressions.
â­ï¸Double escaping (\\): used to escape special characters in R strings.
ğŸš€Getting used to applying different functions (e.g. grep(), grepl(), regexpr(), gregexpr() )"
71,Nasser Gaafar,3 Nov 2024 11:18,Badge 02 - Mini-Diary: 3 stars and 1 wish â­ï¸â­ï¸â­ï¸ğŸš€,"â­ï¸ The Stringr package
â­ï¸ Regex and other data manipulations
â­ï¸ the familiar thrill of coding in Rstudio, well for me at least :)
ğŸš€ Double escaping"
72,Helen Wu,3 Nov 2024 11:11,Badge 02 - Mini-Diary: 3 stars and 1 wish â­ï¸â­ï¸â­ï¸ğŸš€,"â­ï¸ Double escaping in R using \\ to ensure that our regex is interpreted correctly
â­ï¸Utilising regex to clean up data and formatting strings
â­ï¸How to think critically about the outcomes and to structure and plan our code around that.
ğŸš€Continue to practice using regex in different scenarios"
73,Yi mei Low,31 Oct 2024 12:25,Badge 02 - Mini-Diary: 3 stars and 1 wish â­ï¸â­ï¸â­ï¸ğŸš€,"â­ï¸ How to use Stringr package, in particular I found the tools to extract information very useful
â­ï¸ The basics of regex and its features such as escaping, quantifiers, anchors
â­ï¸ Double escaping... it threw me off at first but with explanation and practice it did become more comfortable
ğŸš€ I really need to practice and familiarise myself with forming and utilising regex, looking forward to live pair coding sessions!"
74,Amit Mishra,30 Oct 2024 05:04,Badge 02 - Mini-Diary: 3 stars and 1 wish â­ï¸â­ï¸â­ï¸ğŸš€,"â­Use of regex for preprocessing of unstructured data.
â­use of \\ as a special feature of R
â­use quantifiers, contents, and anchors while working with unstructured data in R.
ğŸš€ I would like to understand more about using different combinations of quantifiers. This is still not very clear. Hopefully, it will be clear with practice."
75,Dylan Delmar,29 Oct 2024 23:01,Badge 02 - Mini-Diary: 3 stars and 1 wish â­ï¸â­ï¸â­ï¸ğŸš€,"â­ Stringr package and helpful functions for evaluating and extracting information from strings
â­ Important components of regex such as quantifiers, anchors, groups, and character classes
â­ How to read out the syntax and components of a regular expression to know what the pattern is looking for
ğŸš€ I'd like to learn more about the different ways to form regex and how they can be used with large amounts of string data"
76,Franz KrÃ¤mer,29 Oct 2024 17:07,Badge 02 - Mini-Diary: 3 stars and 1 wish â­ï¸â­ï¸â­ï¸ğŸš€,"â­ï¸ The notebook introduced me to the stringr package (I was aware of tidyverse).
â­ï¸ Interesting that in R, the R string syntax first processes the strings in a specific way before passing them to the regex engine which results in needing double slashes as escape characters.
â­ï¸ Being able to group sequences of regex expressions with brackets is very helpful!
ğŸš€ I wish I had a better overview of the various things regex + the diverse string manipulation functions can and cannot do. At the moment I'm just thinking: Whoa, powerful!"
77,Isabel Santonja,29 Oct 2024 13:23,Badge 02 - Mini-Diary: 3 stars and 1 wish â­ï¸â­ï¸â­ï¸ğŸš€,"ğŸŒŸThe difference between the â€œsepâ€ and â€œcollapseâ€ arguments of the paste function: sep is used for separating string elements within a vector, while collapse is used for collapsing the entire result into a single string.

ğŸŒŸBoth str_length(), part of the tidyverse, and nchar() , a base are function, are most often used to determine the length of strings, but nchar has an argument (type) to measure the length of the stain in other units, such as bytes!

ğŸŒŸThe quantifier * indicates matching â€œnothingâ€ or any number of the sequence specified before, but not any other sequence. (I had to ask a NLP tool to understand that ğŸ˜…)

ğŸ I wish I could understand better when to use base R, such as nchar() or gsub(), rather than tidyverse functions, like str_length() or str_replace()."
78,Rebecca Sewell,29 Oct 2024 12:52,Badge 02 - Mini-Diary: 3 stars and 1 wish â­ï¸â­ï¸â­ï¸ğŸš€,"â­ï¸ When using regex in R, the code is interpreted twice (by R, and then by Regex) so need to use double escaping \\
â­ï¸ Dividing a regex expression up with brackets allows you to utilise different parts of
the pattern individually
â­ï¸ str_replace doesnâ€™t seem to permanently alter the original string, it seems to need to be
saved to a new variable to save the replacement.
ğŸš€ What are all the characters with special meanings in a regex, I will look for a list
as I was expecting â€˜:â€™ to have one for example and it didnâ€™t."
79,Pawel Orzechowski,19 Oct 2024 09:33,Badge 02 - Mini-Diary: 3 stars and 1 wish â­ï¸â­ï¸â­ï¸ğŸš€,"â­ï¸ this is an example answer
â­ï¸ write three things you learned (or which brought you joy)
â­ï¸ while watching video and completing the notebook
ğŸš€ and one thing you wish you understood better (and maybe what step you will take to address it)
---
(write your mini-diary in the above format AS A RESPONSE TO THIS THREAD).
Instead of emoji you can use * ? or any other symbol that you like."
80,Swe Lynn,21 Dec 2024 09:21,Badge 03 - Mini-Diary: 3 stars and 1 wish â­ï¸â­ï¸â­ï¸ğŸš€,"â­ï¸ setwd( ) is going to be very useful. Difference between read.csv and read_csv
â­ï¸ tibble vs dataframe and importance of mastering dplyr functions
â­ï¸ functions to view data and find information e.g. glimpse( ), head( ), dim( ), names( ), str( ), summary( )
ğŸš€ I understand we are actually learning different use of R throughout the programme so far (statistic, visualisation, markdown, NLP) and learning R along the way but I found this badge particularly useful for understanding the essential basics of R for NLP and this should be the way for all other modules. "
81,Yingqian Tang,8 Dec 2024 17:59,Badge 03 - Mini-Diary: 3 stars and 1 wish â­ï¸â­ï¸â­ï¸ğŸš€,"*Tibbles vs Data Frames: Tibbles handle text better and treat text as characters by default, unlike data frames that convert them to factors.
*Loading Data in R: Commands like head(), dim(), and glimpse() are useful for quickly exploring data after loading it.
*Working Directory: Setting a working directory makes it easier to load files without typing long paths.
?Data Frame vs Tibbles: I need to explore when to use tibbles vs data frames, especially since some functions donâ€™t work with tibbles. Iâ€™ll experiment to learn more."
82,Nisha Daniel,4 Dec 2024 16:44,Badge 03 - Mini-Diary: 3 stars and 1 wish â­ï¸â­ï¸â­ï¸ğŸš€,"â­ It is more of a revision of R and loading data which we learnt in the first year.
â­ some useful commands to be used in data manipulation.
â­ Tibble vs data frame. this is clarified today.
â“i want to work with tibble vs data frame in other applications, outside of academic works."
83,Louise Lau,20 Nov 2024 10:36,Badge 03 - Mini-Diary: 3 stars and 1 wish â­ï¸â­ï¸â­ï¸ğŸš€,"â­The collection of data manipulation functions available in Tidyverse, such as mutate and vignette
â­ The functions for loading and eventually saving the data files as a refresher
â­ Comparison between dataframe and tibble, which is described as the modern version of the data frame - with the note that some older function may not work with tibble.
ğŸš€ More tips / experience sharing around tackling potential issue with importing and saving data (e.g. potential encoding problem)"
84,Paolo Pricoco,19 Nov 2024 10:05,Badge 03 - Mini-Diary: 3 stars and 1 wish â­ï¸â­ï¸â­ï¸ğŸš€,"â­ Base R and other R packages like dplyr provide handy functions to have a brief look into the data or some characteristics of it, such as number of rows and columns, column names, the first X number of records, etc.

â­ There are subtle, but important, differences in R between data-frames and tibbles. Tibbles are modern versions of traditional R data-frames. Although they do not support row names, they are more user-friendly than data-frames as they automatically infer column types, their subtypes always return tibbles (maintaining structure), and they integrate seamlessly with other widely used packages from the tidyverse. Unfortunately, though, some older R functions do not work with tibbles, but this generally compensated for by the presence of newer and more user-friendly alternatives to those functions.

â­ One of the advantages of using the read_csv() function from tidyverse as opposed to read.csv() from base R is that the former will always interpret text as character data, while the latter converts all character variables into factors by default. To avoid this, we generally have to pass the argument stringsAsFactors = FALSE.

â“Given the sometimes not insignificant differences in syntax and compatibility between base R and tidyverse functions, I am wondering whether one should opt to make use of only one of the two approaches in all R scripts, in order to ensure readability and easy modifiability of the scripts by other professionals, or mixing the two approaches can sometimes be a good idea."
85,Adijat Adenaike,"13 Nov 2024 21:02
(Edited by Adijat Adenaike on 13 Nov 2024 21:02)",Badge 03 - Mini-Diary: 3 stars and 1 wish â­ï¸â­ï¸â­ï¸ğŸš€,"â­I enjoyed learning about write.csv and how it is used to export DataFrame in R as a csv file.
â­I had previously been confused about the difference between read.csv and read_csv. Now I know that read.csv is from base R while read_csv is from the tidyverse package, which provides a more robust way for data analysis.
â­ I learnt how to rename columns in R with the names argument. I particularly enjoyed renaming and subsetting columns as it is far easier than regex!
â“I am confused about setting working directories. Some texts say do and some say don't. I wish there is more information in this course about working directories. I will be reading the R documentation on this."
86,Kate Rose,13 Nov 2024 15:36,Badge 03 - Mini-Diary: 3 stars and 1 wish â­ï¸â­ï¸â­ï¸ğŸš€,"â­ Refresher on how to load a CSV file in R Studio using the readr package and read_csv function
â­ Using functions to change column names to something more meaningful and creating subsets of the dataset to filter by a particular value using names() and subset()
â­Saving data frames as a CSV file using the write.csv() function
â“Experimenting with organising a working directory in more detail - learned about this in Intro to Data Science module in Y1 and would like to explore further when time permits"
87,Nasser Gaafar,13 Nov 2024 06:48,Badge 03 - Mini-Diary: 3 stars and 1 wish â­ï¸â­ï¸â­ï¸ğŸš€,"â­ï¸ Reintroduction to tibbles and tidyverse
â­ï¸read.csv and read_csv
â­ï¸ Subsetting
ğŸš€ uploading different file types into R"
88,Robin Nicholls,9 Nov 2024 17:54,Badge 03 - Mini-Diary: 3 stars and 1 wish â­ï¸â­ï¸â­ï¸ğŸš€,"â­ï¸ Viewing and editing data in R
â­ï¸ General brush up on R has been useful as I typically use Python
â­ï¸ Commands such as getwd are useful tips!
ğŸš€ Further detail on DF manipulation would be useful. Although it is detailed well online"
89,Joseph Oxley,5 Nov 2024 11:25,Badge 03 - Mini-Diary: 3 stars and 1 wish â­ï¸â­ï¸â­ï¸ğŸš€,"â­ï¸ The important differences between using read.csv and read_csv, and how data is loaded into different types in base R and the tidyverse.
â­ï¸ How to perform basic data manipulations in base R. Previously I had relied on the tidyverse functions to filter a data frame so seeing the subset approach was useful.
â­ï¸ How to write a data frame to a CSV file, in particular the importance of including the index or specifying the file path if different to the working directory.
ğŸš€How the sentiment feature was created, and if this could be used as a label for a supervised neural network when evaluating model accuracy."
90,Franz KrÃ¤mer,4 Nov 2024 16:20,Badge 03 - Mini-Diary: 3 stars and 1 wish â­ï¸â­ï¸â­ï¸ğŸš€,"Quite detail-oriented today:
â­ï¸ When reading a file via read.<filetype>(), stringsAsFactors is set to FALSE by default.
â­ï¸ The $ operator allows to access a specific column of a dataframe.
â­ï¸ names() saves to the dataframe without explicit assignment but subset() does not.
ğŸš€ Understand how sentiment analysis works. I hope we will get to that at a later stage. Looking forward to trying it out on some of my own text data from work."
91,Kate Browne,3 Nov 2024 19:42,Badge 03 - Mini-Diary: 3 stars and 1 wish â­ï¸â­ï¸â­ï¸ğŸš€,"â­ï¸ Differences between tibbles and dataframes.
â­ï¸ Difference between read.csv from base R and read_csv from the tidyverse for loading data from CSV files.
â­ï¸ Reminder of how to review data and use of functions from dplyr (e.g. mutate, arrange, filter etc) to manipulate the data.
ğŸš€ Uploading other file types (e.g. JSON) other than CSV files into R."
92,Helen Wu,3 Nov 2024 11:40,Badge 03 - Mini-Diary: 3 stars and 1 wish â­ï¸â­ï¸â­ï¸ğŸš€,"â­ï¸ Looking at the features of DataFrame and Tibble in R and the difference between the two
â­ï¸ Loading data into R
â­ï¸ Utilising tidyverse to manipulate the data
ğŸš€ Taking steps and mapping out what I want to do with the data and doing it one small step at a time then bringing it all together at the end. Sometimes I work ahead of myself and get stuck because I try to tackle many things at once."
93,Yi mei Low,1 Nov 2024 12:13,Badge 03 - Mini-Diary: 3 stars and 1 wish â­ï¸â­ï¸â­ï¸ğŸš€,"â­ The differences between Tibbles and the default data frame presentation for R
â­ Organising your dataset, such as renaming columns, creating subsets
â­The difference between read.csv and read_csv
ğŸš€ I hope to learn how to also work and integrate data files of different formats other than csv"
94,Dylan Delmar,30 Oct 2024 19:14,Badge 03 - Mini-Diary: 3 stars and 1 wish â­ï¸â­ï¸â­ï¸ğŸš€,"â­ What the dataset we'll be using for the class looks like, its dimensions, and the information within
â­ How Tibbles differ from data frames and extend their functionality
â­ read_csv() vs. read.csv() and how they differ for text data in particular
ğŸš€ I'd like to better understand the sentiment tagging and how this variable will be used in analysis"
95,Amit Mishra,30 Oct 2024 05:08,Badge 03 - Mini-Diary: 3 stars and 1 wish â­ï¸â­ï¸â­ï¸ğŸš€,"â­ï¸ How to upload CSV files into the R.
â­ï¸Viewing data and renaming columns
â­ï¸ Subsetting data to identify features of interest.
ğŸš€ How can the CSV data be exported into other formats in a presentable form? I feel that there would be functions available for the same and will explore in help."
96,Isabel Santonja,29 Oct 2024 15:59,Badge 03 - Mini-Diary: 3 stars and 1 wish â­ï¸â­ï¸â­ï¸ğŸš€,"ğŸŒŸ .csv files can also be written with the write.csv() function.

ğŸŒŸ A tibble is sometimes referred to as tbl_df.

ğŸŒŸ read_csv() converts columns with text to character variables, while read.csv() converts them to factors.

ğŸ I need to have a look at the functions to import datafile other than .csv"
97,Rebecca Sewell,29 Oct 2024 12:53,Badge 03 - Mini-Diary: 3 stars and 1 wish â­ï¸â­ï¸â­ï¸ğŸš€,"â­ï¸ Advantages to using a Tibble include that it automatically summarizes and prints only the first 10 rows (phew!) and also integrates with tidyverse tools.
â­ï¸ I am excited to explore dplyr further for improving data manipulation skills.
â­ï¸ Writing dataframes to csv as you go seems like it could be useful
ğŸš€ I am excited to learn and practice more techniques in the upcoming sessions. "
98,Pawel Orzechowski,19 Oct 2024 09:34,Badge 03 - Mini-Diary: 3 stars and 1 wish â­ï¸â­ï¸â­ï¸ğŸš€,"â­ï¸ this is an example answer
â­ï¸ write three things you learned (or which brought you joy)
â­ï¸ while watching video and completing the notebook
ğŸš€ and one thing you wish you understood better (and maybe what step you will take to address it)
---
(write your mini-diary in the above format AS A RESPONSE TO THIS THREAD).
Instead of emoji you can use * ? or any other symbol that you like."
99,Swe Lynn,21 Dec 2024 11:35,Badge 04 - Mini-Diary: 3 stars and 1 wish â­ï¸â­ï¸â­ï¸ğŸš€,"â­ï¸ The preparation of carrot for cooking analogy to data preprocessing for algorithm.
â­ï¸ Five areas of preprocessing focused here (cases, numbers, abbreviations + special characters, punctuations, tokenisation)
â­ï¸ Installing and loading appropriate libraries `install.packages(""libraryname"")` , `library(libraryname)`. Here readr, dplyr, tidytext, stringr are used.
ğŸš€ I started to realise the potential usefulness of preprocessing while working on notebook and looking forward to following badges to see if I am right in terms of how this will be useful."
100,Yingqian Tang,9 Dec 2024 08:14,Badge 04 - Mini-Diary: 3 stars and 1 wish â­ï¸â­ï¸â­ï¸ğŸš€,"â­ Text cleaning simplifies analysis and improves NLP outcomes.
â­ Tokenisation breaks text into useful units for analysis.
â­ Preprocessing depends on the task, like keeping uppercase for sentiment.
â“How to balance stop word removal while keeping relevant ones like â€œnot.â€
 Iâ€™ll explore examples and practice customizing stop word lists."
101,Nisha Daniel,4 Dec 2024 16:50,Badge 04 - Mini-Diary: 3 stars and 1 wish â­ï¸â­ï¸â­ï¸ğŸš€,"â­how noisy human language is and how it is to be preprocessed.
â­Normalisation is reducing text case, to reduce noise.
â­ how to tokenise and i have found this wonder. stop_words()
â“I am unable to process gsub along with the learnings of this badge. hope to practice more."
102,Louise Lau,2 Dec 2024 21:55,Badge 04 - Mini-Diary: 3 stars and 1 wish â­ï¸â­ï¸â­ï¸ğŸš€,"ğŸŒŸ Applying the right combination of data cleaning and processing technique based on the use case - e.g. capital letter may be relevant for sentiment analysis.
ğŸŒŸ The concept of stop words and how these noise may potentially impact the study
ğŸŒŸ Methods of tokenization in R and other normalisation techniques.
ğŸš€ The recommended approach for tokenization - the badge notebook cover one example done with regex and selected packages, it would be interesting to learn about the different possibilities and when to use which (and if it make sense to tokenize multiple times using different methods?)"
103,Helen Wu,24 Nov 2024 12:06,Badge 04 - Mini-Diary: 3 stars and 1 wish â­ï¸â­ï¸â­ï¸ğŸš€,"â­ï¸tokenisation breaks texts into smaller units to make it easier to process and analyse the text
â­ï¸utilising regex to clean up data and using positive/negative lookaheads and look behinds
â­ï¸ when cleaning the texts not to remove any critical meaning
ğŸš€ understanding regex and tokenisation more in practicality, looking forward to practicing it more"
104,Paolo Pricoco,22 Nov 2024 13:19,Badge 04 - Mini-Diary: 3 stars and 1 wish â­ï¸â­ï¸â­ï¸ğŸš€,"ğŸŒŸ One of the objectives of cleaning and pre-processing data is to reduce variability and this can be achieved by setting patterns through which our code or other code/software can better interpret and recognize text.

ğŸŒŸSome examples of cleaning and normalisation are: setting words to lower- or uppercase, switching digits or other types of characters with a pre-set word, e.g., ""digit"" or ""symbol"", etc. Of course, we have to pay attention to the potential consequences of such changes. For instance, setting all text to lowercase may reduce alter the meaning of some words/sentences, reducing emphasis on words or hiding headings and titles in a text.
Further examples of cleaning and normalisation are abbreviations, use of special characters, removal of tags from HTML, SQL and other codes, email addresses, etc. Punctuation can also be made consistent in a text.

ğŸŒŸTokenisation is a process that breaks unstructured text into chunks of information, called tokens. These can consist of a letter, a word, a sequence of words or they can be as long as paragraphs, and they deliver a single message or have a common meaning.

â“In this and the following badges, I have learned that very frequent words such as ""the"", ""a"" and ""an"" may have substantial importance. However, in this badge, they were used as examples of words on which to apply cleaning and normalisation. I am wondering in which cases they are more important and why."
105,Isabel Santonja,21 Nov 2024 18:37,Badge 04 - Mini-Diary: 3 stars and 1 wish â­ï¸â­ï¸â­ï¸ğŸš€,"ğŸŒŸ Reducing vocabulary is a key step in the data cleaning process for NLP.

ğŸŒŸ Tokenisation splits the text into discrete elements or tokens. In R, this can be done with the unnest_tokens function, which removes the punctuation by default (i.e. strip_punct = TRUE) and converts strings to lower case (i.e. to_lower = TRUE)

ğŸŒŸMultiple words and phrases are called n-grams, where n is the number of words.

ğŸ I would like to apply this to some data I am working with to clean medication data!"
106,Kate Rose,18 Nov 2024 15:52,Badge 04 - Mini-Diary: 3 stars and 1 wish â­ï¸â­ï¸â­ï¸ğŸš€,"â­ Cleaning & pre-processing helps reduce variability to support better data analysis without losing meaning
â­ Finding & replacing numbers - we can use the gsub command. Need to be mindful of the position of numbers being replaced to use appropriate anchors to avoid unexpected/undesirable results
â­ Tokenisation - splitting a string into multiple meaningful components
â“Lookups - didn't really have time to explore this in detail but would like to in the future when time permits"
107,Adijat Adenaike,14 Nov 2024 00:06,Badge 04 - Mini-Diary: 3 stars and 1 wish â­ï¸â­ï¸â­ï¸ğŸš€,"â­I learnt about tokenization. I think about it as the process of breaking sentences into parts. I enjoyed the R Studio practices on tokenizing by words, by ngram, by sentences and by chapter markers using the unnest_tokens function.
â­Learnt how to use the Random.txt file and the readlines function. I had previously used the Random.int function in another course so it feels nice to learn about Random.txt.
â­ Pre-processing and cleaning of data has a lot of moving parts to it and being vigilant is critical. Removing contractions can be problematic, for instance changing 'won't' in 'won't you?' to 'will not', changes the whole phrase to 'will not you?' I am still learning and look forward to understanding how to go about such things when dealing with large texts.
â“I am still confused about when to use gsub as a stand alone and when to combine it with an str_ function. I guess the understanding will come from engaging in more text analysis, and that is what I intend to do."
108,Nasser Gaafar,13 Nov 2024 06:51,Badge 04 - Mini-Diary: 3 stars and 1 wish â­ï¸â­ï¸â­ï¸ğŸš€,"â­ Tokenisaton in NLP
â­ Cleaning and processing data
â­ Stop words and similar are noisy
ğŸš€ Applying the principles covered to datasets"
109,Robin Nicholls,9 Nov 2024 18:04,Badge 04 - Mini-Diary: 3 stars and 1 wish â­ï¸â­ï¸â­ï¸ğŸš€,"â­ The lookbehinds and lookahead notation was new to me and very useful.
â­ Piping was introduced to me in the practical and makes manipulation very logical.
â­ Unnest_tokens from tidytext is a great tool for word, sentence and ngram tokenisation.
ğŸš€ Improving my knowledge of regex syntax for more complex operations."
110,Rebecca Sewell,9 Nov 2024 15:10,Badge 04 - Mini-Diary: 3 stars and 1 wish â­ï¸â­ï¸â­ï¸ğŸš€,"â­ Stop word removal and how to customize this. Removal of these non-informative words (along with other cleaning and pre-processing tools) is helpful for reducing noise and computational load.
â­ It is important to be careful of default arguments within functions as these can drastically change the output and its meaning. 
â­ The order of steps for cleaning and pre-processing is very important so that meaning or useful context is not lost.
ğŸš€ I am interested in the mention of reducing vocabulary size and am curious about the process for how this is selected and performed. "
111,Franz KrÃ¤mer,"8 Nov 2024 23:05
(Edited by Franz KrÃ¤mer on 8 Nov 2024 23:17)",Badge 04 - Mini-Diary: 3 stars and 1 wish â­ï¸â­ï¸â­ï¸ğŸš€,"â­ The difference between stem and lemma.
â­ Useful visualisation approaches to get a ""feel"" for the text.
â­ Positive and negative lookaheads and lookbehinds in regex.
ğŸš€ I wish I understood better when a lookahead / lookbehind is necessary and when the ""normal"" regex suffices. With regex, one sometimes can get creative with various solutions but is there a rough use case rule?"
112,Kate Browne,5 Nov 2024 17:31,Badge 04 - Mini-Diary: 3 stars and 1 wish â­ï¸â­ï¸â­ï¸ğŸš€,"â­ Positive (?=) & negative (?!) lookaheads and positive (?<=) & negative (?<!) lookbehinds;
â­ The regex pattern \S+ to match any sequence of non-whitespace characters;
â­ Tokenisaton in NLP, including word, sentence and ngram tokenisation and the syntax for unnest_tokens;
ğŸš€ Gettign used to the tokenisation functions; understanding what each parameter does and when to custom/configure them."
113,Joseph Oxley,5 Nov 2024 14:41,Badge 04 - Mini-Diary: 3 stars and 1 wish â­ï¸â­ï¸â­ï¸ğŸš€,"â­ How to use R to tokenize text data into different formats, such as words, phrases or sentences. The separation can be defined using regex for more complex separators.
â­ How care needs to be taken when cleaning text data, removing stop words, case or punctuation may be appropriate in some cases but not always. (For example, a capital letter may mean something different to a lowercase letter.)
â­ Learned some different regex approaches / methods to identify patterns in text. I found the example of removing all the different types of added space helpful to practice writing regular expressions.
easily
ğŸš€ How to decide how to tokenize data? For example, do some types of model perform better on phrases than words? How is this related to overfitting vs underfitting in machine learning models when considering how finely to split the data."
114,Dylan Delmar,4 Nov 2024 21:55,Badge 04 - Mini-Diary: 3 stars and 1 wish â­ï¸â­ï¸â­ï¸ğŸš€,"â­ Different types of normalization of text data and why they're important
â­ The effect of stop words on noise and dimensions and how to remove them
â­ Tokenisation and how to use tidyverse packages to separate blocks of text data into tokens easily
ğŸš€ I'd like to get better at creating universal regex to normalize text data, and create normalizations more efficiently rather than chains of code"
115,Amit Mishra,4 Nov 2024 19:27,Badge 04 - Mini-Diary: 3 stars and 1 wish â­ï¸â­ï¸â­ï¸ğŸš€,"â­ï¸Steps in cleaning and processing data and how to address issues like numbers, special characters, punctuations, etc.
â­ï¸Tokenisation using unnest token and N-gram
â­ï¸How Tibble is helpful in cleaning data
ğŸš€ I am thinking of how this word splitting would make sense. I hope to get clarity along the course."
116,Yi mei Low,4 Nov 2024 09:21,Badge 04 - Mini-Diary: 3 stars and 1 wish â­ï¸â­ï¸â­ï¸ğŸš€,"â­ï¸ Learning more about Tibble format and how it can be used for further functions such as tokenisation
â­ï¸ Tokenisation and being able to specifically break up a large text chunk into the desired n-grams would be very useful and applicable for clinical research!
â­ï¸ I had fun exploring the other packages/functions recommended such as cleantext
ğŸš€The codes for contractions are quite long and repetitive, I will look into other functions to see if they can be more efficient in this"
117,Pawel Orzechowski,31 Oct 2024 18:14,Badge 04 - Mini-Diary: 3 stars and 1 wish â­ï¸â­ï¸â­ï¸ğŸš€,"â­ï¸ this is an example answer
â­ï¸ write three things you learned (or which brought you joy)
â­ï¸ while watching video and completing the notebook
ğŸš€ and one thing you wish you understood better (and maybe what step you will take to address it)
---
(write your mini-diary in the above format AS A RESPONSE TO THIS THREAD).
Instead of emoji you can use * ? or any other symbol that you like."
118,Swe Lynn,21 Dec 2024 20:41,Badge 05 - Mini-Diary: 3 stars and 1 wish â­ï¸â­ï¸â­ï¸ğŸš€,"â­ï¸ Term Frequency â€“ Inverse Document Frequency (TF-IDF) concept.
â­ï¸ Different types of visualisation. Word cloud is quite familiar one. Word network is quite interesting to further explore.
â­ï¸ Usefulness for high level overview of the day by summarising (count, word frequencies, lexical diversity, stop word counts)
ğŸš€ I got confused around word co occurrence and visual networking. I wish to have time to come to read more about this. Work cloud function is very interesting and I believe it will be useful. "
119,Yingqian Tang,9 Dec 2024 08:28,Badge 05 - Mini-Diary: 3 stars and 1 wish â­ï¸â­ï¸â­ï¸ğŸš€,"â­Regular Expressions: I enjoyed seeing how regex can find and replace text patterns precisely, like replacing numbers with â€œDIGIT.â€
â­String Manipulation in R: I learned how powerful functions like gsub, tolower, and stringr are for cleaning and processing text.
â­ Applications: It was exciting to see how these techniques can be used in real-world NLP tasks.
â“ To practice and review more examples to deepen my understanding."
120,Amit Mishra,7 Dec 2024 09:00,Badge 05 - Mini-Diary: 3 stars and 1 wish â­ï¸â­ï¸â­ï¸ğŸš€,"â­Word frequency analysis is helpful to identify keywords and their number of occurrences in the text. It can also be applied to phrases to identify key themes.
â­Term frequencies can be converted into term matrix document, which can be visualized to have better understating of the terms that appear in the document.
â­Term frequencies can be visualized using a variety of ways such as bar graphs, word clouds etc.
ğŸš€ Creation of codes to correctly identify combinations of various stop words, dates, etc. will require practice. "
121,Louise Lau,4 Dec 2024 17:57,Badge 05 - Mini-Diary: 3 stars and 1 wish â­ï¸â­ï¸â­ï¸ğŸš€,"â­ Great introduction to various term counting & visualization methods
â­How tokenization and the frequency analysis works in R using R packages and pipeline
â­ Connected to the above, this reinforced the takeaway about stop words from the previous badge, as stop words are likely at the top of the term frequency list if not properly taken care of.
â“Learnt from the course content that some of these counting approach are used in early phase to guide data cleaning. It would be nice to share further details on this topic (on the why & how)."
122,Nisha Daniel,4 Dec 2024 16:56,Badge 05 - Mini-Diary: 3 stars and 1 wish â­ï¸â­ï¸â­ï¸ğŸš€,"â­TF-IDF is very interesting and helping.
â­it came to me as a surprise as i got to learn how to do my favouriste visualisation WordCloud. thats amazing!
â­lexical diversity and other ways of summarising text data is awesome.
â“I am having troubles using these codes in an independent manner, although i aim to improve by practice."
123,Helen Wu,24 Nov 2024 12:48,Badge 05 - Mini-Diary: 3 stars and 1 wish â­ï¸â­ï¸â­ï¸ğŸš€,"â­ï¸ TF-IDF analysis helps identify the importance of words by balancing the frequency against their rarity across a corpus
â­ï¸ Visualisation techniques like word clouds, network graphs and bar charts
â­ï¸ customised stop-word lists and using functions like anti_join() to more effectively clean texts
ğŸš€ to practice more to see how to effectively manage stop-word removal without losing context"
124,Paolo Pricoco,"22 Nov 2024 13:31
(Edited by Paolo Pricoco on 24 Nov 2024 09:40)",Badge 05 - Mini-Diary: 3 stars and 1 wish â­ï¸â­ï¸â­ï¸ğŸš€,"ğŸŒŸTerm frequency (tf) analysis - i.e., counting how many times a term appears in a text - allows us to create ""Document Term Matrices"", which can be converted into heatmaps or other types of visuals in order to have an overall look on preponderant co-occurrence of words and sparsity. This means we can identify words that often go together and sparse words, which are often stray. The TM package is a good R package to work with document term matrices for term frequency. It is possible to count words for long texts by using the function unnest_tokens(), and then count(..., sorted=TRUE) to have the final count of each word in the text.

ğŸŒŸ There are different approaches to counting word frequencies are: binary weights (answers the question ""does the word appear in the text? Yes or no""); term frequency (tf); Inverse Document Frequency (IDF). In IDF, we assign higher scores to words which are most unusual, that is, they appear less in the text. It is dividing the total nÂ° of documents by the nÂ° of documents containing the corpus, and taking the natural logarithm of it. The fewer the documents that contain the word (denominator), the rarer the word.

ğŸŒŸ Term frequency inverse document frequency (TF-IDF) is a measure of how a word is important in a corpus relative to how important it is in all documents. Therefore, TF-IDF is: lower for words that appear frequently in many documents; higher for words that appear frequently in the documents of a corpus (i.e., few documents).
TF-IDF has proven to be very useful in internet search, where search engine were able to identify documents in which a word was particularly relevant compared to other documents.

â“I wonder when we are supposed to look for higher frequencies in the occurrence of words, as opposed to looking for unusual words (lower frequencies). Are there algorithms that help us establish whether a word is to be searched for based on its rarity in the text as opposed to its abundance?"
125,Isabel Santonja,21 Nov 2024 18:46,Badge 05 - Mini-Diary: 3 stars and 1 wish â­ï¸â­ï¸â­ï¸ğŸš€,"ğŸŒŸThe TF-IDF (term frequency-inverse document frequency) method helps to find important words for the content of each document by decreasing the weight of commonly used words in a corpus of documents and increasing the weight of words that are rarely used.


ğŸŒŸThe library graph can be used to visualise networks of common combination of words.

ğŸŒŸ These networks can be used to predict which words are likely to come after!

ğŸ I need to have a look at igraph documentation to better understand the different options."
126,Adijat Adenaike,20 Nov 2024 00:46,Badge 05 - Mini-Diary: 3 stars and 1 wish â­ï¸â­ï¸â­ï¸ğŸš€,"â­I learnt about word frequency analysis and terms like Term Frequency (TF), Inverse Document Frequency (IDF) and lexical diversity. These are used to understand the prevalence and importance of words.
â­I learnt about visualization techniques such as network graphs and word clouds including using ngrams in word clouds.
â­I learnt to use the tidyverse anti_join function to remove stopwords and the rbind function to add custom stopwords.
â“I am unclear on how IDF and Document Term frequency actually works. I will be searching the internet to find out more about this. I wish this course comes with further resources for each week or badge."
127,Kate Rose,18 Nov 2024 17:44,Badge 05 - Mini-Diary: 3 stars and 1 wish â­ï¸â­ï¸â­ï¸ğŸš€,"â­ An alternative approach to filtering out stop words & specific words to exclude is to use anti_join (to filter out stop words) plus a custom list for removal, by creating a dataframe of the custom list and then adding into (right-bind) stop_words, rather than performing these two actions separately
â­TD-IDF method helps find important words for the content of each document by decreasing the weight of commonly used words and increasing the weight of words that are rarely used
â­Seeds help to fully control behaviour in a wordcloud through removing random chance; makes it easier to troubleshoot when needed
â“Understand network arrangement of words when creating visual connection of words - need more time to explore this in more depth"
128,Nasser Gaafar,13 Nov 2024 07:38,Badge 05 - Mini-Diary: 3 stars and 1 wish â­ï¸â­ï¸â­ï¸ğŸš€,"â­ï¸ Using visualisation techniques to explore datasets
â­ï¸ Removing stop words
â­ï¸ inverse document frequency
ğŸš€ More practice of all the concepts"
129,Robin Nicholls,9 Nov 2024 18:17,Badge 05 - Mini-Diary: 3 stars and 1 wish â­ï¸â­ï¸â­ï¸ğŸš€,"â­ TF-IDF shows word importance for a document in a collection of documents and is common in a range of NLP tasks such as summarization or information retrieval.
â­ Word clouds are great way to represent word frequency in a visual form.
â­ Removal of stop words can sometimes help in analysis as they are often insignificant to the meaning/context.
ğŸš€ Use of libraries like ggraph to visualise the data."
130,Franz KrÃ¤mer,"9 Nov 2024 16:26
(Edited by Franz KrÃ¤mer on 9 Nov 2024 20:56)",Badge 05 - Mini-Diary: 3 stars and 1 wish â­ï¸â­ï¸â­ï¸ğŸš€,"â­ï¸ Expanding stop_words with custom words.
â­ï¸ Use stop words on bigrams.
â­ï¸ Usage for arrows in network graphs for basic predictions, very interesting.
ğŸš€ I wish I understood the header of the bigram_graph object better. It's probably not that important but I think one detail from it can be required as an argument for the ggraph() plotting function."
131,Rebecca Sewell,9 Nov 2024 15:49,Badge 05 - Mini-Diary: 3 stars and 1 wish â­ï¸â­ï¸â­ï¸ğŸš€,"â­ The use of summarisation and visualisation techniques to understand the characteristics of a data set and potential issues before cleaning and pre-processing is undertaken.
â­ TF-IDF â€“ lowest for words that appear frequently in many documents, highest for words that appear frequently in just a few documents, useful for information retrieval tasks like search engines.
â­ Set seed in a word cloud to â€˜fixâ€™ the position of words and make it reproducible.
ğŸš€ How to measure and analyse co-occurrence of words or phrases."
132,Dylan Delmar,7 Nov 2024 23:32,Badge 05 - Mini-Diary: 3 stars and 1 wish â­ï¸â­ï¸â­ï¸ğŸš€,"â­ TF-IDF and how it can help identify the subject or relevance of a document
â­ Ways to visualize text data, including word clouds and network graphs
â­ How to filter stop words and create custom lexicons to filter text data with
ğŸš€ I'd like to better understand the process of summarizing text data and if there's a different way to remove stop words from n-grams (such as an R equivalent of list comprehensions in python)"
133,Joseph Oxley,6 Nov 2024 15:53,Badge 05 - Mini-Diary: 3 stars and 1 wish â­ï¸â­ï¸â­ï¸ğŸš€,"â­ï¸ How to visualise text frequency in R through the use of bar charts to show counts or word clouds.
â­ï¸ How to split sentences into terms of different length, and how to remove stop-words in this context. I found it quite difficult to remove stop words from phrases, so using the hint to split the phrase into columns was helpful!
â­ï¸ How to calculate inverse document frequency and what this means mathematically - to represent the unusual terms in a text relative to other texts in a corpus. So this is a way to find rare terms within the same context as the text itself.
ğŸš€ If there is a more direct approach to remove stop words from sentences before they are split into ngrams, instead of having to attribute each ngram word to a column. (Not sure if this would cause a potential loss of meaning?)"
134,Kate Browne,5 Nov 2024 20:37,Badge 05 - Mini-Diary: 3 stars and 1 wish â­ï¸â­ï¸â­ï¸ğŸš€,"â­ï¸ Ways to visualise text data (e.g. barcharts, word clouds, network graphs);
â­ï¸ How to remove stop words by default using the anti_join function and also by filtering;
â­ï¸ Inverse document frequency (Idf) to measure the rarity of a word/token across a collection of documents;
ğŸš€ More practice in applying the visualisation techniques across different scenarios."
135,Yi mei Low,5 Nov 2024 12:43,Badge 05 - Mini-Diary: 3 stars and 1 wish â­ï¸â­ï¸â­ï¸ğŸš€,"â­ï¸ Reviewing not only frequencies and inverse frequencies but also word relationships
â­ï¸ Adding custom words to stop words, especially useful for common but unneeded words such as https
â­ï¸ The multitude of ways we can present the data (worldcloud, igraph, bigram)
ğŸš€ Still not quite clear on the concept of seeds, will keep doing some further reading on the topic"
136,Pawel Orzechowski,31 Oct 2024 18:15,Badge 05 - Mini-Diary: 3 stars and 1 wish â­ï¸â­ï¸â­ï¸ğŸš€,"â­ï¸ this is an example answer
â­ï¸ write three things you learned (or which brought you joy)
â­ï¸ while watching video and completing the notebook
ğŸš€ and one thing you wish you understood better (and maybe what step you will take to address it)
---
(write your mini-diary in the above format AS A RESPONSE TO THIS THREAD).
Instead of emoji you can use * ? or any other symbol that you like."
137,Swe Lynn,21 Dec 2024 22:47,Badge 06 - Mini-Diary: 3 stars and 1 wish â­ï¸â­ï¸â­ï¸ğŸš€,"â­ï¸ Stemming {SnowballC library, wordstem( ), stem_words( ), stem_strings( ) }, and Lemmatisation{textstem library, lemmatize_words( ), lemmatize_strings( )}. The differences (Stemming: fast, simple, not always meaningful, Lemmatisation: complex, meaningful, can take time, importance in machine translation)
â­ï¸ Part of Speech (POS) is essentially assigning the grammatical syntax of each word and there are different algorithms. It is important for machine learning.
â­ï¸ Preprocessing steps depend on what we want to do with data. It is essential to understand the data and the desired outcomes so that preprocessing can be meaningfully done.
ğŸš€ In visualisation of POS, the NOUNS and ADJ visualisation codes seem to be wrong at data source selection and change data source (data = head (nouns, 20) seems to fix that. I would like to know more about visualising in different ways (e.g. ggplot, base function)"
138,Yingqian Tang,9 Dec 2024 09:30,Badge 06 - Mini-Diary: 3 stars and 1 wish â­ï¸â­ï¸â­ï¸ğŸš€,"ğŸŒŸStemming vs Lemmatization: Stemming removes word endings to get the base form, while lemmatization uses dictionaries to find valid base forms.
ğŸŒŸPOS Tagging: It assigns words to grammatical categories (noun, verb), aiding tasks like word sense disambiguation.
ğŸŒŸPreprocessing in Context: The choice of preprocessing depends on the analysis goal, so understanding the data context is key.
â“ I want to explore their effects on text data by testing both methods on sample datasets."
139,Amit Mishra,7 Dec 2024 17:32,Badge 06 - Mini-Diary: 3 stars and 1 wish â­ï¸â­ï¸â­ï¸ğŸš€,"â­Stemming is the process of removing suffixes of a word at the beginning or end to reduce dimensionality, improve generalization, and enhance search and retrieval.
â­Lemmatization uses morphological analysis to determine the base form of a word to preserve the meaning of the word.
â­Part of speech tagging assigns every word its syntactic category and is used before lemmatization or grammatical analysis of text. 
â“ What is the use of POS in the context of clinical data? Will explore on the internet."
140,Louise Lau,"5 Dec 2024 20:32
(Edited by Louise Lau on 5 Dec 2024 20:33)",Badge 06 - Mini-Diary: 3 stars and 1 wish â­ï¸â­ï¸â­ï¸ğŸš€,"â­Technique of lemmatisation and steaming to deliver normalization and their respective strength and considerations
â­ Example of pre-processing pipeline from initial text to the state that it is ready for analysis or other input. The combination of steps are chosen based on the dataset and the targeted outcome.
â­ The two schema for part of speech tagging : universal or language-dependent. (upos and xpos under the UDPipe package)
â“Would love to learn more idea of dimension reduction (reduces dimensionality) in NLP"
141,Nisha Daniel,4 Dec 2024 17:01,Badge 06 - Mini-Diary: 3 stars and 1 wish â­ï¸â­ï¸â­ï¸ğŸš€,"â­Lemmatisation is to brign the word to it's root form.
â­the preprocessing flowchart is very helpful to remember how to go ahead with processing the data available.
â­how to improve generalisation and thereby reduce complexity by stemming
â“I am not confident about POS tagging."
142,Helen Wu,24 Nov 2024 13:14,Badge 06 - Mini-Diary: 3 stars and 1 wish â­ï¸â­ï¸â­ï¸ğŸš€,"â­ï¸ Stemming vs Lemmatisation one reduces words to their root form and the other reduces the words to the dictionary-based form. Stemming is less accurate but faster whereas lemmatisation is more complex but is more accurate
â­ï¸ POS tagging helps with sentence analysis, translation and understanding of how words function in a sentence
â­ï¸ textstem and UDPipe facilitate stemming, lemmatisation, and POS tagging in R with functions like stem_words(), lemmatize_words(), and udpipe_annotate()
ğŸš€ practising the utilisation of POS tagging in more depth and understanding which scenarios will benefit better using POS tagging and stemming/lemmatisation"
143,Paolo Pricoco,"24 Nov 2024 11:40
(Edited by Paolo Pricoco on 24 Nov 2024 11:53)",Badge 06 - Mini-Diary: 3 stars and 1 wish â­ï¸â­ï¸â­ï¸ğŸš€,"ğŸŒŸ Stemming is a normalisation technique that reduces words down to their root (called stem), by removing prefixes and suffixes. Sometimes, root or stem words are not meaningful (e.g., â€œstudiâ€ from â€œstudiesâ€), as stemming does not always respect grammar rules. On the other hand, lemmatisation is a more complex technique that interprets text in order to preserve the meaning of the words and follow grammar rules. It analyses also other parts of speech to improve this interpretation process. Fortunately, the ""textstem"" package in R is very helpful to carry out lemmatisation process.

ğŸŒŸ Part of Speech (POS) tagging consists in assigning labels to words based on their role in the sentence as parts of speech, such as nouns, verbs, adjectives, adverbs, but also subjects, direct objects, indirect objects, etc. It is generally preceded by tokenisation, since words or groups of words have to be considered separately. Since POS uses labels, it can benefit from supervised ML, like support vector machines (SVMs) and neural networks. We can use R packages like UDPipe or openNLP both for POS tagging and lemmatisation, and for this we need to make use of pre-created language models for POS labelling. For example, UDPipe (Universal Dependencies Pipeline) produces groups of labels that are called ""upos"" (universial POS) and ""xpos"" (specific POS).

ğŸŒŸA fairly good example of a text pre-processing pipeline could be: (1) lowercase, (2) removal of punctuation, (3) stop word and non-meaningful word, (4) special characters, (5) stemming or lemmatisation (preferably the latter, if feasible), (6) tokenisation.

â“I was wondering if tokenisation should come before or after stemming/lemmatisation. Probably I need to review both video- and coding classes to better understand this."
144,Isabel Santonja,22 Nov 2024 09:26,Badge 06 - Mini-Diary: 3 stars and 1 wish â­ï¸â­ï¸â­ï¸ğŸš€,"ğŸŒŸ Stemming is the algorithm-based process of removing suffixes and prefixes of a word, reducing it to its â€˜stemmâ€™.

ğŸŒŸ Lemmatisation is a more complex process, which uses morphological analysis to determine the valid base form of a word. It is more accurate, but also limited, as it needs proper speech tagging.

ğŸŒŸ Part of speech (POS) tagging is about assigning every word their syntactic category and is often conducted before lemmatisation. Some tagging systems are language agnostic (e.g. UPOS) and other are language-specific (XPOS) 

ğŸ I wonder what else can be done using the other variables generated by udpipe_annotateâ€¦"
145,Adijat Adenaike,"20 Nov 2024 20:00
(Edited by Adijat Adenaike on 20 Nov 2024 20:01)",Badge 06 - Mini-Diary: 3 stars and 1 wish â­ï¸â­ï¸â­ï¸ğŸš€,"â­ This week I learnt about normalization techniques such as stemmatisation (turning words into their root forms), lemmatisation(reducing words into their dictionary form) and part of speech tagging which assigns a part of speech label to each word in a sentence.
â­Lemmatisation can be done through the lemmatization_words()/lemmatision_strings() functions while one of the many ways stemmatisation can be done is through the stem_words()/stem_strings() funtions.
â­Part of Speech Tagging (POS) can be done using UDpipe package, which is one of many POS tagging tools.
â“I don't quite get how POS tagging fits into the whole picture in text analysis. I will be searching the internet to see if I can get an answer to this."
146,Kate Rose,18 Nov 2024 19:48,Badge 06 - Mini-Diary: 3 stars and 1 wish â­ï¸â­ï¸â­ï¸ğŸš€,"â­Stemming - goal is to group words which share the same meaning. Less accurate than Lemmatisation
â­Lemmatisaton - focuses on stripping word down to its dictionary form. Tries to ensure the reduced word is a proper word. More complex and slower
â­Interesting to see the differences in output when using stem_strings vs lemmatize_strings commands - observed the theory in practice when lemmatisation turnrf 'is; into 'be', which didn't make grammatical sense but did output a 'proper' word with attempted meaning, rather than stemming which transformed 'is' to 'i' and consequently lost all meaning in the context of the sentence
â“More time to explore POS tagging in greater detail and better understand useful application of this"
147,Nasser Gaafar,13 Nov 2024 07:42,Badge 06 - Mini-Diary: 3 stars and 1 wish â­ï¸â­ï¸â­ï¸ğŸš€,"â­ Stemming and lemmatisation
â­ The different approaches to cleaning and preprocessing based on the analytical method
â­ POS tagging
ğŸš€ practice, practice, practice..."
148,Dylan Delmar,11 Nov 2024 23:59,Badge 06 - Mini-Diary: 3 stars and 1 wish â­ï¸â­ï¸â­ï¸ğŸš€,"â­ The difference between a stem and a lemma and how they can be used differently in NLP
â­ How outcome and type of analysis will affect the preprocessing pipeline
â­ Functions to use for getting stems, lems, and pos tagging
ğŸš€ I'd like to practice the combination of pos tagging and frequency graphing with identifying word areas of interest to graph particular nouns, proper nouns, etc that relate to that target. For instance, graphing the most common proper nouns that aren't COVID-19, coronavirus, or anything related to the virus itself, or most common nouns relating to isolation, etc."
149,Yi mei Low,10 Nov 2024 15:25,Badge 06 - Mini-Diary: 3 stars and 1 wish â­ï¸â­ï¸â­ï¸ğŸš€,"â­ï¸ What stemming and lemmatisation are and the specific differences that make them differ in their utility
â­ï¸ Using POS to tag words and how this can be customised by using different methods of tagging
â­ï¸ Within stemming there are also differences in the results even with a very minor change to the functions used
ğŸš€The content covered in this week's 3 badges were significantly more in volume as well as technical difficulty (however understanding how these topics are linked). I wonder if it will be easier to digest if it can be further split up"
150,Franz KrÃ¤mer,"9 Nov 2024 21:35
(Edited by Franz KrÃ¤mer on 9 Nov 2024 21:56)",Badge 06 - Mini-Diary: 3 stars and 1 wish â­ï¸â­ï¸â­ï¸ğŸš€,"â­ï¸ Interesting: Different stemming functions produce different stems. E.g., â€œtodayâ€ was stemmed to â€œtodayâ€ with wordStem() and to â€œtodaiâ€ with stem_strings().
â­ï¸ There are universal and language specific POS tags.
â­ï¸ Annotation with UDPipe provides not only upos and xpos but also other linguistic information.
ğŸ˜‚ Bonus: Reading stemmed and lemmatized sentences about cats is actually quite funny.
ğŸš€ I wish I understood better how POS tagging can fit into a NLP pipeline. I looked into an explanatory list of the upos (https://universaldependencies.org/u/pos/) but I think it would be helpful to look into a few lighter use cases for grammatical analysis. Obviously this can get complex quickly for non-linguists."
151,Robin Nicholls,9 Nov 2024 18:28,Badge 06 - Mini-Diary: 3 stars and 1 wish â­ï¸â­ï¸â­ï¸ğŸš€,"â­ï¸ Stemming is an important step for some tasks as it aligns words of the same 'stem' for better analysis.
â­ï¸ Lemmatisation is more accurate than stemming due to it's use of POS and sentence context but this is at a complexity cost.
â­ï¸ Part-of-speech tagging (POS) is the process of deriving a tree of markers for each word that represent the word type (nouns, verbs, etc...)
ğŸš€There are many options for POS tagging and many tag sets. How can we know what is best for a specific domain?"
152,Rebecca Sewell,9 Nov 2024 17:25,Badge 06 - Mini-Diary: 3 stars and 1 wish â­ï¸â­ï¸â­ï¸ğŸš€,"â­ï¸ Stemming removes suffixes/prefixes but resultant â€˜rootâ€™ form may not be a valid word.
â­ï¸ Lemmatization transforms a word to its base dictionary form, is more accurate than stemming but also more complex and slower.
â­ï¸ POS tagging assigns part of speech labels to each word based on context and definition, methods can be rule-based or statistical/ML-based. Essential for lemmatization.
ğŸš€How do BERT models for POS tagging work to contextualise information better. Also how are spelling errors or other unexpected word forms accounted for in pre-processing generally."
153,Kate Browne,8 Nov 2024 15:45,Badge 06 - Mini-Diary: 3 stars and 1 wish â­ï¸â­ï¸â­ï¸ğŸš€,"â­ï¸ Packages and their respective functions for stemming in r (i.e. SnowballC=>wordStem(), textstem =>stem_words(), stem_strings();
â­ï¸ Packages and their respective functions for lemmatisation in R (i.e. textstem=> lemmatize_words(), lemmatize_strings();
â­ï¸ The udpipe package in R for POS tagging and key functions such as udpipe_download_model(), to download a pre-trained model, udpipe_load_model(), to load the model into R and udpipe_annotate(), to annotate text and perform POS tagging
ğŸš€ Overall I found that there was a lot in this week's badges and I think it would be beneficial to have videos on the coding as currently the videos just focus on the theory. In addition, I am using RStudio as opposed to noteable as I think it will help me use this more once I have completed the course but had some issues that I had to troubleshoot."
154,Joseph Oxley,7 Nov 2024 14:55,Badge 06 - Mini-Diary: 3 stars and 1 wish â­ï¸â­ï¸â­ï¸ğŸš€,"â­ï¸ Understand the differences between stemming and lemmas. How stemming reduces a word to the stem which can produce words not in the language, while lemmatisation aims to reduce words to their base form that is valid.
â­ï¸ Understand the purpose of stemming and lemmatisation to reduce the number of terms in the text which may help an algorithm run more accurately or efficiently.
â­ï¸ The importance of considering steps within a pre-processing pipeline within the context of the analysis. Some approaches (e.g removing stop words) may lead to a loss of meaning in some contexts but not others. This process may need to be refined when moving from exploring the data, to building a more robust model for analysis.
ğŸš€Example of POS tagging being used in a supervised machine learning model. Seeing the working of a real world translation model that relies on identification of syntactic category would be cool!"
155,Pawel Orzechowski,31 Oct 2024 18:16,Badge 06 - Mini-Diary: 3 stars and 1 wish â­ï¸â­ï¸â­ï¸ğŸš€,"â­ï¸ this is an example answer
â­ï¸ write three things you learned (or which brought you joy)
â­ï¸ while watching video and completing the notebook
ğŸš€ and one thing you wish you understood better (and maybe what step you will take to address it)
---
(write your mini-diary in the above format AS A RESPONSE TO THIS THREAD).
Instead of emoji you can use * ? or any other symbol that you like."
156,Swe Lynn,"22 Dec 2024 13:05
(Edited by Swe Lynn on 22 Dec 2024 18:09)",Badge 07 - Mini-Diary: 3 stars and 1 wish â­ï¸â­ï¸â­ï¸ğŸš€,"â­ï¸ Feature extraction: turning text into features computer understand (Document vectors, Bag of Words, TF-IDF, Word embedding)
â­ï¸ Algorithms in NPL: Logistic regression, NaÃ¯ve Baynes, Support Vector Machines, Decision Trees/Random Forrest, Neural Based Models
â­ï¸ Supervised vs Unsupervised Learning
ğŸš€ tm package is another entirely new package I would like to be more familiar with. I am confused the terminologies (term frequency, document frequency, document term matrix, document frequency matrix). Using functions to expend R language is also something I need to spend more time on. "
157,Nasser Gaafar,12 Dec 2024 10:40,Badge 07 - Mini-Diary: 3 stars and 1 wish â­ï¸â­ï¸â­ï¸ğŸš€,"I'm so sorry I thought I had attempted all of them! Hope this one is accepted.

â­ï¸ Bag of words and TF-IDF methods to transform text
â­ï¸Classification algorithms
â­ï¸ DTMs
â“ further applications of these principles on text data"
158,Yingqian Tang,9 Dec 2024 10:09,Badge 07 - Mini-Diary: 3 stars and 1 wish â­ï¸â­ï¸â­ï¸ğŸš€,"â­ï¸ Learned feature extraction methods like BoW and TF-IDF to transform text into machine-readable features.
â­ï¸ Gained insight into NLP algorithms such as Naive Bayes, SVM, and Decision Trees.
â­ï¸ Understood the difference between supervised and unsupervised learning.
â“Still confusing about how TF-IDF impacts classification models. "
159,Louise Lau,5 Dec 2024 22:11,Badge 07 - Mini-Diary: 3 stars and 1 wish â­ï¸â­ï¸â­ï¸ğŸš€,"â­ï¸The various classification algorithm used in NLP, covering how they works and some potential consideration (e.g. speed consideration for SVM) 
â­ï¸Some of the common methods to perform feature extraction for text, from those more suitable for smaller dataset such as one-hot-encoding, to more commonly used ones such as word embedding
â­ï¸Experiment with GLoVE for word embedding tasks in R with example of how word embedding matrix + distance finding may look like.
â“would appreciate further details of how the classification algorithm work with the feature extracted (e.g. for decision tree - a node (feature) could be a word? )"
160,Nisha Daniel,4 Dec 2024 17:06,Badge 07 - Mini-Diary: 3 stars and 1 wish â­ï¸â­ï¸â­ï¸ğŸš€,"â­ï¸ Feature extraction techniques such as one-hot encoding, bag of words and TF-IDF
â­ï¸ the Different algorithms used in NLP
â­ï¸ supervised vs unsupervised learning
â“i wish i had more time to practice these codes. i take the help of LLMs. but i wish to learn more about Neural networks and Random Forest."
161,Helen Wu,26 Nov 2024 20:19,Badge 07 - Mini-Diary: 3 stars and 1 wish â­ï¸â­ï¸â­ï¸ğŸš€,"â­ï¸ Different feature extraction techniques such as one-hot encoding, bag of words and TF-IDF
â­ï¸ GloVe can be used to create word embeddings to capture semantic meaning
â­ï¸ Document vectors are numeric representations of documents!
ğŸš€taking time to understand the variety of libraries and to practice using different techniques methods to better understand when to use them"
162,Paolo Pricoco,24 Nov 2024 16:12,Badge 07 - Mini-Diary: 3 stars and 1 wish â­ï¸â­ï¸â­ï¸ğŸš€,"ğŸŒŸDocument Vectors (DVs) are numeric vectors whose complex structure contains the meaning of a document. Thanks to DV, we can measure similarity between documents, we can classify them and group them into cluster, and we can apply information retrieval on them.

ğŸŒŸIt's important not to assume that relatively simple Feature Extraction approach, such as One-hot encoding (OHE) and Bag of Words (BoW), have been completely superseded by more cutting-edge methods like Decision Trees/Random Forest or Neural Network-based methods. In fact, OHE and BoW are still pretty widely used for datasets that are not too large or in which word order and relationships do not play a significant role.

ğŸŒŸAlso logistic regression can be used in ML for Feature Extracion. NaÃ¯ve Bayes method is based on the Bayes theorem and it assumes that features are independent of each other (this is sometimes called â€œnaÃ¯veâ€ assumption). It works well with sparse data, without significant relationships with each other. It is used in spam classification, as it can classify documents, messages or emails as spam or non-spam simply based on the presence or absence of certain words.

â“I am very curious to know more about how Decision Trees/Random Forest methods can be effectively applied in healthcare data science, and what differences there would be in terms of output an applicability with neural network-based approaches."
163,Isabel Santonja,23 Nov 2024 15:12,Badge 07 - Mini-Diary: 3 stars and 1 wish â­ï¸â­ï¸â­ï¸ğŸš€,"ğŸŒŸ Document-Term Matrix (DTM) is a matrix representation of the text corpus, in which the rows represent the documents and the columns the terms, and each of the elements correspond to the frequency. 

ğŸŒŸBy default, the DocumentTermMatrix function of the tm package gives absolute frequencies and does some preprocessing steps, like removing common words or turning words to lower case. By contrast, the dfm function of the quanteda package does not remove common words, such as â€œtoâ€ or â€œinâ€.

ğŸŒŸThe Bayesian approach is also used in NLP (!): naÃ¯ve Bayes. However, its greatest limitation is that it assumes conditional independence of features.

ğŸ I wonder which of these techniques are used in Turnitin to check for for similarity and use of AI â€“ I did a quick search, but it seems it is a propietary algorithm and there is no much information around."
164,Adijat Adenaike,22 Nov 2024 15:44,Badge 07 - Mini-Diary: 3 stars and 1 wish â­ï¸â­ï¸â­ï¸ğŸš€,"â­ Feature extraction, (transforming text into numerical representations) and Document Vectors. With techniques such as one-hot encoding, Bag of Words (BoW), TF-IDF and Word Embeddings with GloVe.
â­ Explored document representations with 3 packages (tm, quanteda, and tidytext) and how TF-IDF is used on Document Term Matrix.
â­ The tm package appeared to be the most straight forward but I enjoyed learning the tidytext the most as it shows the process involved which helps me understand the whole text classification pipeline better.
â“I wish there was a bit more explanation around the Neural Based Models. I will be searching the internet to get more understanding around this"
165,Kate Rose,20 Nov 2024 20:54,Badge 07 - Mini-Diary: 3 stars and 1 wish â­ï¸â­ï¸â­ï¸ğŸš€,"â­Multiple methods of feature extraction: document vectors (eg. one hot encoding); TF-IDF; word embeddings (GloVe)
â­Using tm, quanteda and tidytext packages to create document vector representations
â­Using the apply function in R to compare matrix values for each word with a target word
â“I wish I had more time to practise writing R code to convert GloVe embeddings into a matrix"
166,Dylan Delmar,18 Nov 2024 20:23,Badge 07 - Mini-Diary: 3 stars and 1 wish â­ï¸â­ï¸â­ï¸ğŸš€,"â­ What are document vectors and examples of different types, such as bag of words and tf-idf
â­ Types of learning models and pros and cons of each type
â­ Creating unnamed temporary functions and their use within the apply() functions
ğŸš€ I'd like a better grasp on creating multiple functions to interact with each other"
167,Robin Nicholls,17 Nov 2024 20:21,Badge 07 - Mini-Diary: 3 stars and 1 wish â­ï¸â­ï¸â­ï¸ğŸš€,"â­ï¸ How to derive different document vectors (bag-of-words, TF-IDF, one-hot encoding)
â­ï¸ GloVe, an unsupervised algorithm for vector representations of words. Used to derive meaning from word-word cooccurrences from real-world dataset (common-crawl, Twitter, Wikipedia)
â­ï¸ Cosine similarity can be computed for word similarity of word document vectors
ğŸš€ There is a lot of libraries used in this bade/notebook and I'd like to spend more time to get used to them."
168,Rebecca Sewell,16 Nov 2024 14:54,Badge 07 - Mini-Diary: 3 stars and 1 wish â­ï¸â­ï¸â­ï¸ğŸš€,"â­ï¸ How to get data into different formats for different packages so we can pick out and use preferred functions from different packages.
â­ï¸ Document vector - single structures numerical representation of an entire document.
â­ï¸ Decision tree and Support Vector Machine algorithms are popular in healthcare due to their transparency with having explainable, traceable outputs.
ğŸš€ I am interested in how unique positions are defined in One-Hot encoding."
169,Franz KrÃ¤mer,14 Nov 2024 22:38,Badge 07 - Mini-Diary: 3 stars and 1 wish â­ï¸â­ï¸â­ï¸ğŸš€,"â­ï¸ Feature representation in matrices with the R packages tm, quanteda and tidytext.
â­ï¸ Anonymous functions in R.
â­ï¸ Use glove word embeddings for word similarity measurement with cosine similarity. Cannot say I fully understood the math behind this but I grasped the concept.
ğŸš€ Wish I knew better how to decide on which feature extraction method to apply depending on the properties of my text corpus."
170,Joseph Oxley,12 Nov 2024 11:17,Badge 07 - Mini-Diary: 3 stars and 1 wish â­ï¸â­ï¸â­ï¸ğŸš€,"â­ï¸ Learned different features that can be derived from text data, such as One-Hot encoded vectors and Bag of Words vectors. I also understand the improvements and limitations of each method.
â­ï¸ Word Embedding - I found the concept of this incredibly interesting, and the use of the dot product relationship to quantify the similarity between two high dimensional vectors.
â­ï¸ How to code functions in R - I had experience in coding functions in python but had not wrote my own R functions, so writing my own functions to calculate word similarity and to search for similar words was a very useful exercise.
ğŸš€More about the mathematics of word embedding and what calculations can be performed on word vectors to obtain useful insight. I wonder what other vector operations, such as the cross product, would represent?"
171,Amit Mishra,11 Nov 2024 18:35,Badge 07 - Mini-Diary: 3 stars and 1 wish â­ï¸â­ï¸â­ï¸ğŸš€,"â­ï¸ Understood various techniques to create document vector (One-Hot encoding, Bag-of-words, TF-IDF and word embedding)
â­ï¸Use of various machine learning techniques in NFL
â­ï¸Different packages (tm, quanteda and tidytext to generate document vector

ğŸš€ Couldnt complete GloVe as in my notebook somehow fread function was not working."
172,Yi mei Low,10 Nov 2024 18:17,Badge 07 - Mini-Diary: 3 stars and 1 wish â­ï¸â­ï¸â­ï¸ğŸš€,"â­ï¸Code R has a find function!! That was a pleasant new thing to learn that I will definitely be using from here on
â­ï¸Converting DTMs into a tidy data/tidytext format so that dplyr workflows can be used instead
â­ï¸Going through the various methods that we can use for feature extraction, and then applying the embedding vectors
ğŸš€Did find the code for word similarities rather confusing, took a long time to get an understanding and even now not quite sure it's totally there. Will continue reading!"
173,Kate Browne,10 Nov 2024 12:35,Badge 07 - Mini-Diary: 3 stars and 1 wish â­ï¸â­ï¸â­ï¸ğŸš€,"â­ï¸ Different methods for feature extraction (e.g. bag of words, one-hot encoding, TF-IDF, word embeddings such as GLoVE;
â­ï¸ After pre-processing & feature extraction is done, a range of machine learning algorithms can be applied depending on the task you want to complete and the nature of your data (e.g. logistic regression, decision trees etc.);
â­ï¸ Creating custom functions in R to compute word similarities and find words similar to a given word using GLoVE embeddings;
ğŸš€ I found GLoVE slightly confusing as it does not entail a package but a workflow (e.g. download relevant packages such as data.table, then download GLoVE embeddings, then load GLoVE vectors, define functions for similarity etc.)."
174,Pawel Orzechowski,"7 Nov 2024 10:44
(Edited by Pawel Orzechowski on 7 Nov 2024 10:45)",Badge 07 - Mini-Diary: 3 stars and 1 wish â­ï¸â­ï¸â­ï¸ğŸš€,"â­ï¸ this is an example answer
â­ï¸ write three things you learned (or which brought you joy)
â­ï¸ while watching video and completing the notebook
ğŸš€ and one thing you wish you understood better (and maybe what step you will take to address it)
---
(write your mini-diary in the above format AS A RESPONSE TO THIS THREAD (not to this example post)).
Instead of emoji you can use * ? or any other symbol that you like."
175,Swe Lynn,22 Dec 2024 18:51,Badge 08 - Mini-Diary: 3 stars and 1 wish â­ï¸â­ï¸â­ï¸ğŸš€,"â­ï¸ Named entity recognition (NER) classify words or phrases into predefined categories and labels.
â­ï¸ Different approaches of NER: ruled base, machine learning, deep learning.
â­ï¸ Challenges of NER: Ambiguity, Context, Language variability & drift, Training data sparsity, Complex nested entities
ğŸš€ I feel this is much more complicated now especially needing knowledge of different packages and quite complex coding to retrieve data from these packages. I will need to learn more about different packages most appropriate to my line of work. "
176,Yingqian Tang,9 Dec 2024 10:44,Badge 08 - Mini-Diary: 3 stars and 1 wish â­ï¸â­ï¸â­ï¸ğŸš€,"ğŸŒŸNER identifies and classifies entities in text (e.g., people, places, dates, organizations) into predefined categories.
ğŸŒŸThe BIO tagging system for labeling entities was insightful.
ğŸŒŸNER's applications in fields like journalism, healthcare, and social media were interesting.
â“I want to explore how Bi-LSTMs and Transformers are used in NER tasks and plan to dive deeper into this in R."
177,Amit Mishra,9 Dec 2024 00:55,Badge 08 - Mini-Diary: 3 stars and 1 wish â­ï¸â­ï¸â­ï¸ğŸš€,"â­ï¸ Named Entity Recognition identifies and classifies text data into predefined categories of people, organization, country etc. and is useful in for text data related to government, finance and healthcare
â­ï¸ Various packages are available in r to perform named entity operations on data. Each of these packages offer different approach.
 â­ï¸For multi word entities, begin-inside-outside tagging scheme is useful
 ğŸš€How to explore techniques to enhance NER accuracy in healthcare "
178,Nasser Gaafar,8 Dec 2024 17:36,Badge 08 - Mini-Diary: 3 stars and 1 wish â­ï¸â­ï¸â­ï¸ğŸš€,"â­ï¸ NER is a powerful tool when used properly
â­ï¸ Using different pipe operators
â­ï¸ NER with R packages
ğŸš€ need to explore BIOs tagging more thoroughly"
179,Louise Lau,6 Dec 2024 15:19,Badge 08 - Mini-Diary: 3 stars and 1 wish â­ï¸â­ï¸â­ï¸ğŸš€,"â­ï¸ the complexity with named entity recognition : as it can be layered, and also needed to consider phases with different meanings
â­ï¸ Application of BIO (Begin-Inside-Ouside) to label tokens in NER by its category and also providing the boundary (B/I/O)
â­ï¸ Examples of NER with R packages such as name tagger or entity
ğŸš€ Would love to learn more about the application of NER in health industry, which was briefly mentioned during the course content"
180,Nisha Daniel,4 Dec 2024 17:10,Badge 08 - Mini-Diary: 3 stars and 1 wish â­ï¸â­ï¸â­ï¸ğŸš€,"â­ï¸ how Named Entity Recognition classifies words/phrases into predefined categories/ labels.
â­ï¸ the diverse use cases of NER from media to government to healthcare, etcetra.
â­ï¸ the BIO tagging schema used for naming entities.
â“i wish I would learn more about BIO tagging."
181,Helen Wu,26 Nov 2024 20:28,Badge 08 - Mini-Diary: 3 stars and 1 wish â­ï¸â­ï¸â­ï¸ğŸš€,"â­ï¸ NER classifies words/phrases into predefined categories or lables
â­ï¸BIO tagging using tokens as Begin, Inside or Outside; helping to identify boundaries
â­ï¸ using %<>% as a different piping operator
ğŸš€ more understanding of fine-tuning on the accuracy of NER"
182,Paolo Pricoco,24 Nov 2024 17:25,Badge 08 - Mini-Diary: 3 stars and 1 wish â­ï¸â­ï¸â­ï¸ğŸš€,"ğŸŒŸName Entity Recognition (NER) is a process that allows for text recognition and interpretation by computers, and it consists in assigning labels to relevant words, which are called entities. Then, entities can be link to other words labelled as entity modifiers, such as location, time, type (e.g., type of disease, type of treatment, etc.). A complex type of entities are nested entities, meaning that each entity can contain one or more sub-entities and that each word or chunk of text can be labelled in more than one way.

ğŸŒŸCase uses of NER range from journalism, to search tools, recommendation and advertising, social media monitoring, sentiment analysis, customer service and healthcare. In healthcare, NER is used to identify data regarding genes, diseases, treatment, ICD code, etc. As it occurs in many fields of NLP, ambiguity of text, context, language variability and drift over time, and sparsity and paucity of training data (together with complex nested entities) are the main challenges of NER.

ğŸŒŸBIO (Begin-Inside-Ouside) tagging scheme labels words as begin, inside or outside, where â€œbeingâ€ is the beginning of an entity, â€œinsideâ€ is a feature related to it or a modifier, â€œoutsideâ€ is a word unrelated to the entity. In this way, less important words (â€œoutsideâ€) are partly ignored (almost filtered out), while entities and their modifiers become preponderant for text interpretation.

â“I havenâ€™t fully grasped how BIO tagging works. As far as I have understood, only some words are chosen as entities, and choice depends on the meaning the words have in the text. I was wondering how we could teach AI to understand which noun is to be considered an entity and which one isnâ€™t, considering that, without context, it is sometimes difficult even for humans to tell the two apart."
183,Isabel Santonja,24 Nov 2024 11:21,Badge 08 - Mini-Diary: 3 stars and 1 wish â­ï¸â­ï¸â­ï¸ğŸš€,"ğŸŒŸName entity recognition is the process of classifying words or phrases into predefined categories or labels.

ğŸŒŸBIO Tagging schema is a commonly used method for labelling tokens in NER, in which B is used to tag the beginning of a label, I the inside and O the outside or entities with no importance.

ğŸŒŸ The performance of the NER models on our data depends on the data they were trained in.

ğŸ The function person_entity from the entity package seems to be slower than location_entity. I wonder why."
184,Adijat Adenaike,22 Nov 2024 19:20,Badge 08 - Mini-Diary: 3 stars and 1 wish â­ï¸â­ï¸â­ï¸ğŸš€,"â­The use of lexicons and packages like gapminder and nametagger to identify named entities.
â­Use of str_flatten() to concatenate texts to a string.
â­New ways of writing the pipe symbol(%>%). i.e. |>, %<>%.
â“How do we improve NER accuracy when models mix-up or outright miss entities."
185,Kate Rose,21 Nov 2024 21:44,Badge 08 - Mini-Diary: 3 stars and 1 wish â­ï¸â­ï¸â­ï¸ğŸš€,"â­ Using a variety of packages and commands collectively to achieve an NER output, eg. utilising the gapminder package to build a lexicon of countries we'll use to search over, str_flatten to manipulate our df text column into one long concatenated string, and str_count to identify frequency of country mention
â­ Lexicon package - wasn't particularly effective in identifying legitimate names due to lack of context
â­ Not all packages are always suitable to all data sets; highly dependent on the data that a NER model was trained on
â“Would like to learn more about how to determine the correct NER model / R packages to use for different types of data"
186,Dylan Delmar,18 Nov 2024 22:10,Badge 08 - Mini-Diary: 3 stars and 1 wish â­ï¸â­ï¸â­ï¸ğŸš€,"â­ Different ways in which NER is used
â­ BIO tagging scheme and how it helps identify boundaries of multi-word entities
â­ Basics of using an NER package and how effective it is on different types of data
ğŸš€ I'd like to better learn methods to clean up the text to make for instance, the name counting work better"
187,Robin Nicholls,17 Nov 2024 20:37,Badge 08 - Mini-Diary: 3 stars and 1 wish â­ï¸â­ï¸â­ï¸ğŸš€,"â­ï¸ Really useful to learn about the power of different types of pipes!
â­ï¸ Proper name look up with a dictionary (lexicon package) can return many false positives as context isn't considered.
â­ï¸ Pre-trained models perform much better, but pre-processing is crucial to get good NER accuracy.
ğŸš€ I am not too familiar with R as I am with Python, so it is good to learn about R specifics such as Piping. I will need to spend some time learning other R features to help with the course."
188,Kate Browne,17 Nov 2024 11:41,Badge 08 - Mini-Diary: 3 stars and 1 wish â­ï¸â­ï¸â­ï¸ğŸš€,"â­ï¸ Use of the nametagger package for identifying and tagging names in text (i.e. personal names, organisation names and locations);
â­ï¸ Use of the entity package in R to identify and classify entities across various categories. This package has more functionality than nametagger (i.e. can ascertain dates, money, percent etc);
â­ï¸ The BIO (beginning inside out) tagging schema in NER labelling;
ğŸš€ How to select the correct model. For instance for the coding for nametagger a specific model was chosen. It would be helpful to know more about the different models that can be used and which are more appropriate for certain types of datasets."
189,Rebecca Sewell,16 Nov 2024 15:25,Badge 08 - Mini-Diary: 3 stars and 1 wish â­ï¸â­ï¸â­ï¸ğŸš€,"â­ï¸ Named entity recognition (NER) focuses on identifying and classifying named entities in text into predefined categories, and doing this automatically.
â­ï¸ NER models are typically trained on labelled text data sets, but it can be difficult (time and cost) to obtain enough labelled data for effective model training particularly in expert domains.
â­ï¸ BIO tagging - this distinguishes not only the entity type, but marks boundaries for multi word entities as well (including words outside the entity).
ğŸš€ I am interested in how language drift can be measured and the uses for this."
190,Franz KrÃ¤mer,15 Nov 2024 22:09,Badge 08 - Mini-Diary: 3 stars and 1 wish â­ï¸â­ï¸â­ï¸ğŸš€,"â­ï¸ The wide range of practice fields in which named entity recognition is used.
â­ï¸ The lexicon package has, for example, lexicons for stopwords (quite a few), emojis, emoticons, cliches, POS tags, slang words, negation terms, frequent names in the USA and lemmas. This is very useful.
â­ï¸ Foundational entity recognition works via word matching or using R packages such as â€œnametaggerâ€ and â€œentityâ€. The packages are not necessarily always the better choice.
ğŸš€ Iâ€™d like to gain a more thorough understanding of the conditions that influence the outcome of NER (why the results are sometimes good and sometimes not that accurate)."
191,Joseph Oxley,"12 Nov 2024 15:42
(Edited by Joseph Oxley on 12 Nov 2024 15:43)",Badge 08 - Mini-Diary: 3 stars and 1 wish â­ï¸â­ï¸â­ï¸ğŸš€,"â­ï¸Learned how Name Entity Recognition is structured, in particular how an entity can become ""nested"" as a higher category entity can be further refined into sub-entities.
â­ï¸Learned about labelling in Named Entity recognition. The ""BIO"" tagging scheme and how this was used in the R code to filter out results with the ""O"" label, meaning they are not associated with a named entity.
â­ï¸The challenges in using names entity recognition in practice - small differences such as capital letters or punctuation can cause a valid entity to be missed. This also highlights the importance of taking care when pre-processing data, as a step such as removing capitalisation may cause an algorithm to perform very differently.
ğŸš€How to improve the performance of the package algorithms? (I was thinking it could be interesting to add conditional checks using regex to search for titles before names etc) Or if there are any best practice steps to prepare data for these popular algorithms?"
192,Yi mei Low,11 Nov 2024 12:30,Badge 08 - Mini-Diary: 3 stars and 1 wish â­ï¸â­ï¸â­ï¸ğŸš€,"â­ï¸It was interesting to see the common problems to work around with NER tagging, for example Scotland. (with a punctuation) affects the tags, uncommon names, names coinciding with country names
â­ï¸Learning how NLP tools would likely not work the best if the model does not fit the context
â­ï¸|> a new way to pipe! And also, the %<>% for applying the pipe to yourself
ğŸš€Even though the model may not be perfectly aligned, words such as ""Crazy"" and ""Honestly"" are still being captured as names (although cases like Will are understandable)"
193,Pawel Orzechowski,7 Nov 2024 10:45,Badge 08 - Mini-Diary: 3 stars and 1 wish â­ï¸â­ï¸â­ï¸ğŸš€,"â­ï¸ this is an example answer
â­ï¸ write three things you learned (or which brought you joy)
â­ï¸ while watching video and completing the notebook
ğŸš€ and one thing you wish you understood better (and maybe what step you will take to address it)
---
(write your mini-diary in the above format AS A RESPONSE TO THIS THREAD (not to this example post)).
Instead of emoji you can use * ? or any other symbol that you like."
194,Swe Lynn,22 Dec 2024 20:16,Badge 09 - Mini-Diary: 3 stars and 1 wish â­ï¸â­ï¸â­ï¸ğŸš€,"â­ï¸ Sentiment analysis: a way for computer understanding the emotion of the text. Approaches, uses and challenges are similar to NER. It is interesting that sentiment analysis can struggle with sarcasms, idioms and expressions. 
â­ï¸ Topic modelling: statistical modelling to find out the topics of the documents. LDA is a common approach. Uses are similar to NER and sentiment analysis. Challenges include sensitivity to the preprocessing, overlapping topics, scalability etc.
â­ï¸ Latent Dirichlet Allocation(LDA) â€“ generative probabilistic model used in R for the course for topic modelling.
ğŸš€ BERTopic seems to be something interesting and will look at the website mentioned in the lecture. "
195,Yingqian Tang,9 Dec 2024 10:57,Badge 09 - Mini-Diary: 3 stars and 1 wish â­ï¸â­ï¸â­ï¸ğŸš€,"ğŸŒŸSentiment Analysis Approaches: The different methods, like lexicon-based, machine learning, and deep learning, and how they can be combined in hybrid models.
ğŸŒŸTopic Modeling: How LDA identifies topics based on word clusters and helps uncover themes in documents.
ğŸŒŸUse Cases: Real-world applications, such as using sentiment analysis in healthcare and product development, made the concepts feel practical.
 â“ I want to better understand how to choose the optimal number of topics and plan to explore LDA in R, testing different numbers of topics and evaluating model performance."
196,Amit Mishra,9 Dec 2024 01:09,Badge 09 - Mini-Diary: 3 stars and 1 wish â­ï¸â­ï¸â­ï¸ğŸš€,"â­ï¸Three methods are used for sentiment analysis- machine learning methods, lexicon-based approach and hybrid models, each has its own strength and weakness.
â­ï¸Use of topic modelling to identify hidden themes in large text datasets
â­ï¸Word cloud and comparison clouds are valuable for presenting sentiment analysis and topic modelling results
ğŸš€Do lexicon models work with psychiatric data? "
197,Nasser Gaafar,8 Dec 2024 17:42,Badge 09 - Mini-Diary: 3 stars and 1 wish â­ï¸â­ï¸â­ï¸ğŸš€,"â­ï¸ Lexicon-based sentiment analysis
â­ï¸LDA is a Bayesian automated topic modelling tool
â­ï¸ Comparing Bing, Afinn and other lexicons and how they assign sentiments to words without context
ğŸš€ More exploring BERT"
198,Louise Lau,6 Dec 2024 15:48,Badge 09 - Mini-Diary: 3 stars and 1 wish â­ï¸â­ï¸â­ï¸ğŸš€,"â­ï¸ Introduction to the main approach of sentiment analysis, such as the lexicon based approaches which is based on a pre-defined list of words.
â­ï¸BERT as one of the commonly used method for topic modeling, which is good for short and structured text.
â­ï¸ Considerations of topic modeling such as that it can be affected by the bias introduced in the training of the model 
ğŸš€ BERT was introduced in the course content as one of the common topic modeling method and I am curious why it that not available in R ( was it language limitation or just happen that we do not have that package yet etc)"
199,Nisha Daniel,4 Dec 2024 17:14,Badge 09 - Mini-Diary: 3 stars and 1 wish â­ï¸â­ï¸â­ï¸ğŸš€,"â­ï¸ how Sentiment Analysis helps computers understands emotions.
â­ï¸ different approaches to Sentiment analysis like lexicon/machine learning /hybrid.
â­ï¸ Topic modelling is an amazing way to get feedback from the clients in diverse sectors.
â“I am apprehensive about LDA method of topic modelling and when to choose which model."
200,Isabel Santonja,29 Nov 2024 12:10,Badge 09 - Mini-Diary: 3 stars and 1 wish â­ï¸â­ï¸â­ï¸ğŸš€,"ğŸŒŸ Generally the methods for NER and sentiment analysis are rule-based, machine learning and deep learning. Additionally, for sentiment analysis mixed methods are also available.

ğŸŒŸ Latent Dirichlet Allocation (LDA) is a common method for topic modelling, in which each document is considered a mix of topics, and each topic a mix of words. As an output, it estimates a probability distribution of topics for each document.

ğŸŒŸA Simple Triplet Matrix is a sparse document term matrix, in which only the non-zero values are stored (i.e. terms that appear in the documents), resulting in a more efficient use of memory.


ğŸ I wonder if there is some kind of â€œruleâ€ or guidance to select the number of topics when conducting LDA."
201,Paolo Pricoco,28 Nov 2024 00:04,Badge 09 - Mini-Diary: 3 stars and 1 wish â­ï¸â­ï¸â­ï¸ğŸš€,"ğŸŒŸFor sentiment analysis, we can rely on different approaches. Lexicon-based methods, machine learning and deep learning are the main three macro-categories, but we can also resort to hybrid methods.

ğŸŒŸCommon challenges for sentiment analysis as well as topic modelling are: colloquial expression, sarcasm, irony, language variation and drift, slangs, memes, and newly coined terms.

ğŸŒŸUse cases of both sentiment analysis and topic modelling are: customer feedback and review interpretation, social media post, brand managing, political opinion collection, healthcare service improvement, product development.

â“I wonder whether sentiment analysis could be used to identify early cases of anxiety and depression among young social media users, in order to prevent worsening of their condition, as well as self-harm and/or suicidal behaviour."
202,Helen Wu,27 Nov 2024 18:22,Badge 09 - Mini-Diary: 3 stars and 1 wish â­ï¸â­ï¸â­ï¸ğŸš€,"â­ï¸ Understanding the concept of sentiment analysis and the challenges it has in analysing different words in different contexts
â­ï¸ Using different lexicons for sentiment analysis and how it can be used in different scenarios based on what we want to achieve
â­ï¸ Using topicmodels to uncover hidden themes
ğŸš€Understanding how to navigate the nuances of sentiment analysis regarding handling sarcasm or contextual ambiguity."
203,Adijat Adenaike,24 Nov 2024 21:08,Badge 09 - Mini-Diary: 3 stars and 1 wish â­ï¸â­ï¸â­ï¸ğŸš€,"â­ Learnt how to use different unigram lexicons for sentiment analysis. Interesting to see the difference in results from Bing, nrc and AFFINN.
â­Learnt how to construct a reproducible workflow using a 'seed'.
â­Learnt how to extract hidden themes or topics from large datasets with Latent Dirichlet Allocation (LDA).
â“I didn't quite get how non-Latin characters in the dataset affect the Document-Term Matrix, and how this issue was resolved? I wish there were more material on how to analyze non English texts. I will be searching the internet for this information"
204,Kate Rose,23 Nov 2024 20:17,Badge 09 - Mini-Diary: 3 stars and 1 wish â­ï¸â­ï¸â­ï¸ğŸš€,"â­ The use cases of sentiment analysis and topic modelling are very similar, and there is some overlap in the challenges to the application of both
â­ Using a variety of unigram lexicons - interesting to explore the differences in how Afinn worked compared with NRC, eg. Afinn used numerical values to measure sentiment whilst NRC used words
â­ Use of the R topicmodels package which contained the functions we needed to 'fit' LDA
â“The final activity in badge 9 was really interesting - it took me a while to get my head around what was happening (maybe I still am ğŸ˜…). If I was interpreting the topic distribution correctly, it appears that articles 1 and 3 both belong to topic 6. This makes sense from reading the text in these articles and comparing with topic 6. I'd then expect article 4 not to belong to topic 6 at all really given the distribution value; it looks like it definitely belongs to topic 1. However, when I read article 4, it matches on quite a few words in topic 6, despite the low distb. score and then I realise there are lots of similar words in topics 6 and 1. How did the model assemble the topics?! There are lots of stop words present and they don't seem to have that much difference in meaning, and somehow the same words feature in many topics. I'd love to learn more about why and how this occurs."
205,Franz KrÃ¤mer,23 Nov 2024 14:39,Badge 09 - Mini-Diary: 3 stars and 1 wish â­ï¸â­ï¸â­ï¸ğŸš€,"â­ï¸ I learned to do foundational sentiment analysis with unigrams by using nrc from the tidytext package. Thatâ€™s something I was very interested in before the course! :-D
â­ï¸ Sentiment analysis is highly context dependent. For example, using the nrc lexicon for unigram word matching on the Corona tweet dataset, the word ""food"" was associated with ""happiness"". The word ""shortage"" was associated with anger. As a bigram (""food shortage""), and if an appropriate lexicon was available or created, these words might represent something different (e.g. fear) in conjunction, as they refer to the lack of an existential good.
â­ï¸ The comparison word cloud is an intriguing visualisation tool.
ğŸš€ I wish I understood better when to prefer which package, Iâ€™m still lacking orientation here."
206,Dylan Delmar,19 Nov 2024 23:33,Badge 09 - Mini-Diary: 3 stars and 1 wish â­ï¸â­ï¸â­ï¸ğŸš€,"â­ What sentiment analysis is and how it is achieved
â­ Different available lexicons for sentiment analysis and how to use them
â­ Topic modeling and LDA models, and challenges that come with
ğŸš€ Learning how to interpret the topic proportions, this wasn't clear to me in the notebook"
207,Robin Nicholls,18 Nov 2024 19:33,Badge 09 - Mini-Diary: 3 stars and 1 wish â­ï¸â­ï¸â­ï¸ğŸš€,"â­ï¸ Different lexicons can be used to categorise different words (outside of common positive/negative) such as anger, fear, joy, etc...
â­ï¸ Topic modelling with LDA preforms well when we have an idea of the number of topics, but it can sometimes to difficult to understand relationship between the words within some topics.
â­ï¸ More on visual representations was useful as it reiterates how useful visualising results can be over manual viewing of data.
ğŸš€ I would like to understand more about deep learning approaches to SA. I used online sources to give further detail."
208,Kate Browne,17 Nov 2024 12:06,Badge 09 - Mini-Diary: 3 stars and 1 wish â­ï¸â­ï¸â­ï¸ğŸš€,"â­ï¸ Different methods for sentiment analysis (e.g. lexicons, machine learning, deep learning and hybrid);
â­ï¸ Applying three general purpose lexicons in the tidytext package (AFINN, Bing, NRC). NRC is more comprehensive and appears to be more useful for tasks such as opinion mining than the other two;
â­ï¸ Use of the topicmodels package in Rfor implementing LDA. The LDA() function fits an LDA model to text data and the input to this function is a DTM which represents the frequency of words in each document;
ğŸš€ I had an issue the final text chunk for the topic modelling in badge 09. Also I think it would be helpful to know a bit more on how to interpret the probability distributions for LDA functions."
209,Rebecca Sewell,16 Nov 2024 16:34,Badge 09 - Mini-Diary: 3 stars and 1 wish â­ï¸â­ï¸â­ï¸ğŸš€,"â­ï¸ Sentiment analysis can be done on both words and phrases. Changes in sentiment within the same document can be difficult to pick up on.
â­ï¸Challenges for topic modelling include unclear or uninterpretable topics, limited information and biases in training data.
â­ï¸Topic modelling can be used for de-identification and finding rare topics which may reveal information because of their uniqueness.
ğŸš€ How do bidirectional encoder representations (BERT) work and why are these only usable with Python and not R?"
210,Joseph Oxley,"13 Nov 2024 11:45
(Edited by Joseph Oxley on 13 Nov 2024 11:49)",Badge 09 - Mini-Diary: 3 stars and 1 wish â­ï¸â­ï¸â­ï¸ğŸš€,"â­ï¸ Different ways to classify sentiment using lexicons within the tidytext package. These lexicons use different categories, such as the nrc lexicon with emotion categories or the bing lexicon with positive and negative categories. The AFINN model uses numeric categories opposed to words to describe the sentiment on a discrete -5 to +5 scale.
â­ï¸Different high level topic modelling methods, the notebook focused on LDA but other models were discussed in the video such as non-negative matrix factorisation and BERTopic. Different methods are better for different types of data, such as shorter text.
â­ï¸How to visualise sentiment through word clouds and bar charts.
ğŸš€ How to apply the other types of sentiment analysis (such as non-negative matrix factorisation) within R, and how the performance of the different algorithms could be quantified and compared."
211,Yi mei Low,12 Nov 2024 14:38,Badge 09 - Mini-Diary: 3 stars and 1 wish â­ï¸â­ï¸â­ï¸ğŸš€,"â­ï¸Going through the different sentiment lexicons and how each are unique in how they score sentiment (for NRC I was also wondering on certain words having double meanings - such as giddy (happy vs dizziness as a symptom)
â­ï¸Bringing back what we previously learned about word clouds - this method of data visualization seems especially apt for the display of sentiment analysis
â­ï¸The process of turning data into a corpus>DTM and then creating our own LDA was a tricky process but is definitely a useful skill for topic analysis
ğŸš€ As mentioned in the video sentiment analysis faces a number of challenges, including the interpretation of sarcasm. I wonder what available measures there are to mitigate this, especially as negative sentiments are more likely to use sarcasm - and likely would lead to more false positives"
212,Pawel Orzechowski,7 Nov 2024 10:46,Badge 09 - Mini-Diary: 3 stars and 1 wish â­ï¸â­ï¸â­ï¸ğŸš€,"â­ï¸ this is an example answer
â­ï¸ write three things you learned (or which brought you joy)
â­ï¸ while watching video and completing the notebook
ğŸš€ and one thing you wish you understood better (and maybe what step you will take to address it)
---
(write your mini-diary in the above format AS A RESPONSE TO THIS THREAD (not to this example post)).
Instead of emoji you can use * ? or any other symbol that you like."
213,Swe Lynn,23 Dec 2024 19:28,Badge 10 - Mini-Diary: 3 stars and 1 wish â­ï¸â­ï¸â­ï¸ğŸš€,"â­ï¸ Annotation is labelling the data. Depending on the NLP task, labelling can be different (entities, phrases, sentences, documents).
â­ï¸ Pros and cons of different types of annotations (manual, automated, semi-automated). Different metrics of measuring Inter-Annotator Agreement (IAA). How BART files work and having two files with same name .ann and .txt.
â­ï¸ Human evaluation is reviewing the outcome of NLP (scoring on either of relevance, accuracy, clarity).
ğŸš€ Ethical considerations is something I did not associate with annotation. This reminds me about ethics especially working with sensitive data in healthcare. "
214,Yingqian Tang,9 Dec 2024 11:01,Badge 10 - Mini-Diary: 3 stars and 1 wish â­ï¸â­ï¸â­ï¸ğŸš€,"ğŸŒŸTypes of Annotation: I enjoyed learning about manual, automated, and semi-automated annotation and their pros and cons.
ğŸŒŸMeasuring Annotation Quality: I found it interesting how Inter-Annotator Agreement (IAA) and metrics like Cohenâ€™s Kappa assess annotation consistency.
ğŸŒŸEthical Considerations: I was surprised by the importance of privacy, bias, and fairness in annotation practices.
â“Automated Annotation Accuracy: I want to better understand how automated systems handle ambiguity and context. I plan to explore more about this through experimentation with NLP tools in R."
215,Nasser Gaafar,8 Dec 2024 17:01,Badge 10 - Mini-Diary: 3 stars and 1 wish â­ï¸â­ï¸â­ï¸ğŸš€,"â­ï¸ The importance of human evaluation of NLP model performance in order for it to be relevant
â­ï¸ Human bias can play a role in annotations, so need to be reflexive
â­ï¸ Various types of annotation software, but Excel sometimes is more than enough
ğŸš€ I would like to explore BRAT in some capacity in the future"
216,Helen Wu,6 Dec 2024 17:54,Badge 10 - Mini-Diary: 3 stars and 1 wish â­ï¸â­ï¸â­ï¸ğŸš€,"â­ï¸ There are different annotation processes, manual, automated and semi-automated.
â­ï¸ Types of annotation tools like BRAT which can use .txt and .ann
â­ï¸ IAA ensures annotation consistency.
ğŸš€ In the future taking time to explore BRAT tool."
217,Louise Lau,6 Dec 2024 16:22,Badge 10 - Mini-Diary: 3 stars and 1 wish â­ï¸â­ï¸â­ï¸ğŸš€,"â­ï¸Examples of tools for annotation, from excel for a more simple tasks to health specific tools like CLAMP. 
â­ï¸Annotation process from data collection to pilot annotation and measurement - guideline creation is an important steps 
â­ï¸Ethical consideration in annotation covers many aspects, such as privacy and bias, also the duty of care for annotators.
ğŸš€It would be interesting to learn more about the health specific annotation tools in addition to the feature summary"
218,Nisha Daniel,4 Dec 2024 17:18,Badge 10 - Mini-Diary: 3 stars and 1 wish â­ï¸â­ï¸â­ï¸ğŸš€,"â­ï¸ Annotation is done to train and test data in NLP
â­ï¸ that domain knowledge and human bias also plays a crucial role in annotations.
â­ï¸ differences between human evaluation and annotations.
â“i wish i knew how to balance privacy and confidentiality in NLP data processing"
219,Kate Rose,1 Dec 2024 12:11,Badge 10 - Mini-Diary: 3 stars and 1 wish â­ï¸â­ï¸â­ï¸ğŸš€,"â­Quality of annotated data directly impacts the effectiveness of an NLP model. Inter-annotator agreement (IAA) measures the reliability of annotations by comparing multiple independent annotators; this helps understand where disagreement occurs
â­Data exploration is a helpful pre-requisite to the annotation process. Enables a more informed conversation which helps build better guidelines. Critical step as guidelines are used to train people on the annotation
â­BRAT annotation files consist of a `.txt` file which contains the original text and `.ann` which contains the annotation. The files must have the same nam `file_a.ann` and `file_a.txt` 
â“More time to explore/learn about different annotation programmes"
220,Isabel Santonja,1 Dec 2024 11:32,Badge 10 - Mini-Diary: 3 stars and 1 wish â­ï¸â­ï¸â­ï¸ğŸš€,"ğŸŒŸ Annotation can be manual (done by a human), automated (based on algorithms), or semi-automated (a combination of the previous). The last retains some advantages of manual annotation but is more cost- and time-efficient.
ğŸŒŸThere are a number of specific annotation software programs, and the choice of tool should be based on the task. Sometimes, the best option might be an Excel sheet.
ğŸŒŸAnnotation can also be used to check if NER worked.

ğŸ It would be lovely to have time to have a look at the BART softwareâ€¦"
221,Adijat Adenaike,30 Nov 2024 16:44,Badge 10 - Mini-Diary: 3 stars and 1 wish â­ï¸â­ï¸â­ï¸ğŸš€,"â­Annotation is the process of labelling data to provide unstructured data that NLP models can be trained and tested against. The 3 types of annotation are manual, automated and semi-automated, with semi-automated being the most used type as it balances speed and accuracy.
â­Inter-annotator agreement (IAA) is used to check for consistency between annotators in a dataset to maintain accuracy and reliability of annotations. Metrics for measuring IAA also serve as a check for ethical use.
â­BRAT contains 2 type of files, (.txt and .ann). A .ann file categorizes data into entities and relationships. Processed a .ann file with R function and loop.
â“I wish the coding example used has a richer dataset which would have shown a more realistic scenario and strengthen my understanding. I will be practicing with a full dataset to have a better understanding of how it all works."
222,Paolo Pricoco,28 Nov 2024 19:48,Badge 10 - Mini-Diary: 3 stars and 1 wish â­ï¸â­ï¸â­ï¸ğŸš€,"ğŸŒŸ Annotation consists in labelling words to transform unstructured data into data that can be interpreted by NLP models. Labels can constitute entities, entity modifiers (like time, location, type, class, type of finding, etc.), parts of speech, words expressing positive or negative sentiments. Different types of annotations can be tailored to the different NLP task at hand, and annotation can be done at a document level, a paragraph level, a sentence level or even at a word level.
 ğŸŒŸ Annotation can be manual, semi-automated, entirely automated. Although manual annotation can potentially be more accurate, as humans can pick up on context more easily than AI, it can be costly, time-consuming and less cost-effective. Also, it is worth remembering that humans can potentially be more prone to bias than algorithms.
 ğŸŒŸ Inter-annotator agreement (IAA) is sort of a quality check tool to measure the reliability of annotations by comparing multiple independent annotators. It is often considered as an upper bound of a systemâ€™s performance. It can be can resort to different methods (such as raw agreement, proportion of labels in agreement, all decisions treated equally) and metrics (Cohenâ€™s kappa (k), Fleissâ€™ kappa). Methods and scores used will depend on the type of annotation and our ultimate goal.

â“I am wondering what the minimum and maximum percentages of words in a text are to be annotated to allow an algorithm to interpret the text. I believe itâ€™s not always necessary to annotate all words in a text, but is it possible to obtain satisfactory results if we annotate only, say, 15% of the words? Has a rough threshold been established by researchers?"
223,Franz KrÃ¤mer,25 Nov 2024 22:10,Badge 10 - Mini-Diary: 3 stars and 1 wish â­ï¸â­ï¸â­ï¸ğŸš€,"â­ï¸ The critical step of human evaluation of the NLP model performance and according scoring measures (relevance of the model outcome, its accuracy, clarity).
â­ï¸ The information that files created with annotation tools like BRAT store: the original text plus the labelling records that store the start and end character of an annotation.
â­ï¸ The structure of a BRAT annotation file and how to parse it into a dataframe, e.g., for machine training and outcome quality assurance.
ğŸš€ I saw that the BRAT tool has some visualisation options, would be interesting to check out how this works."
224,Rebecca Sewell,24 Nov 2024 22:29,Badge 10 - Mini-Diary: 3 stars and 1 wish â­ï¸â­ï¸â­ï¸ğŸš€,"â­ Even with guidelines in place, annotators can often disagree or be inconsistent. Measures of inter-annotator agreement are important.
â­ Annotation steps are often iterative, returning to modify guidelines after pilot results are obtained.
â­ Guidelines should be clear and make the work reproducible, in case you want to come back and do the same annotation again in the future.

ğŸš€ If you make changes to the text you are looking at ( .txt file), is there a way to automatically pull these across to the .ann file as well? I assume this would become clear once we learn the process of creating an .ann file."
225,Kate Browne,23 Nov 2024 18:13,Badge 10 - Mini-Diary: 3 stars and 1 wish â­ï¸â­ï¸â­ï¸ğŸš€,"â­ There are several different types of annotation and the type that should be used depends on the NLP task you want to do;
â­ Different techniques for annotation (e.g. human, automated and semi-automated);
â­ Techniques for measuring the quality and consistency of annotation (e.g. inter-annotator agreement, chance-correct measures such as Cohen's Kappa);
ğŸš€ How to manage complex annotations such as nested annotations or complex schemas defined for specific projects."
226,Dylan Delmar,21 Nov 2024 20:44,Badge 10 - Mini-Diary: 3 stars and 1 wish â­ï¸â­ï¸â­ï¸ğŸš€,"â­ What kind of annotation is done depends on the task and annotation can be applied to individual entities, up to entire documents
â­ How to measure annotation and ensure reliability, with IAA as well as precision, recall, and F1 score
â­ The difference between annotation and human evaluation and the uses of each
ğŸš€ I'd like to see the process of actually making the annotations and how to do that"
227,Yi mei Low,21 Nov 2024 15:07,Badge 10 - Mini-Diary: 3 stars and 1 wish â­ï¸â­ï¸â­ï¸ğŸš€,"â­ï¸ Annotations can be either done by computers or humans, or we can employ both together. While computers are faster and less expensive, human annotation is usually more accurate. A good approach to consider is the cross-checking of computer annotations by humans
â­ï¸ The format of BRAT annotation and the basics of how we can employ it
â­ï¸ The large repository on nlplab that we can refer to in order to search for BART files that we can use
ğŸš€ Learning how to teach a machine learning models what is 'correct' does seem like something that most of us will have to use. Perhaps it could be briefly discussed in the course to give us an idea!"
228,Joseph Oxley,20 Nov 2024 13:13,Badge 10 - Mini-Diary: 3 stars and 1 wish â­ï¸â­ï¸â­ï¸ğŸš€,"â­ï¸ Strengths and weakness of annotation using humans, computers or a combination of the two methods. Typically human annotation is more accurate but also more expensive, so a combined solution is commonly best depending on context.
â­ï¸ Different annotation tools depend on the task. Some tools such as BRAT are web based while more simple data could be annotated in Excel.
â­ï¸ How to deal with BRAT annotation files in R, including separating out the entities and relations into different data frames.
ğŸš€How to use annotated text in supervised machine learning models or how to generate the BRAT annotations directly."
229,Robin Nicholls,19 Nov 2024 18:09,Badge 10 - Mini-Diary: 3 stars and 1 wish â­ï¸â­ï¸â­ï¸ğŸš€,"â­ï¸ Dataset annotation can be carried out by humans, machines, or both. Decisions on which method to choose will be based on cost/resources and accuracy requirements.

â­ï¸ Annotation guidelines are crucial for maintaining transparency, accountability and reproducibility. When using human annotators it is import to remember that everyone sees the 'world' different, and that measures should be put in place to limit individual bias.

â­ï¸ The BRAT rapid annotation tool is used for collaborative text annotation. However, it is a standalone software that needs to be installed.

ğŸš€ I like to learn more about the automated annotation techniques, especially with the advancements with large language models."
230,Amit Mishra,18 Nov 2024 18:45,Badge 10 - Mini-Diary: 3 stars and 1 wish â­ï¸â­ï¸â­ï¸ğŸš€,"â­ï¸ What are Annotation, types of Annotation, and challenges associated with annotation?
â­ï¸How to measure the quality of Annotation - Inter-annotator agreement, metrics, and tools used
â­ï¸ Method of human evaluation and ethical considerations while annotating the data.

ğŸš€ Coding for annotation.. it would have been better to have a video explanation of the coding."
231,Pawel Orzechowski,14 Nov 2024 16:08,Badge 10 - Mini-Diary: 3 stars and 1 wish â­ï¸â­ï¸â­ï¸ğŸš€,"â­ï¸ this is an example answer

â­ï¸ write three things you learned (or which brought you joy)

â­ï¸ while watching video and completing the notebook

ğŸš€ and one thing you wish you understood better (and maybe what step you will take to address it)

---

(write your mini-diary in the above format AS A RESPONSE TO THIS THREAD (not to this example post)).

Instead of emoji you can use * ? or any other symbol that you like."
232,Swe Lynn,"23 Dec 2024 21:08
(Edited by Swe Lynn on 23 Dec 2024 21:11)",Badge 11 - Mini-Diary: 3 stars and 1 wish â­ï¸â­ï¸â­ï¸ğŸš€,"â­ï¸ Performance metrics are used to evaluate NLP tasks. Some metrics are Accuracy, Precision, Recall, F1 score
â­ï¸ Choice of metric will depend on the task and outcome desired.
â­ï¸ Error analysis is important in evolution of the better NLP model.
ğŸš€ I understand more about BIO schema following this badge. I am quite intrigued the metrics of precision, recall and the statistic terms of true positive, true negative etc. used to calculate these and want to spend more time to learn the history of the developments of these metrics."
233,Yingqian Tang,9 Dec 2024 11:08,Badge 11 - Mini-Diary: 3 stars and 1 wish â­ï¸â­ï¸â­ï¸ğŸš€,"ğŸŒŸ Understood metrics like accuracy, precision, recall, and F1 score for model evaluation.
ğŸŒŸLearned how to identify and fix model mistakes to improve performance.
ğŸŒŸ Grasped how BIO tagging aids in boundary detection and entity recognition.
â“ I want to explore how to select the best metric for different NLP tasks. I'll practice with R to improve my understanding.

4o mini"
234,Amit Mishra,9 Dec 2024 01:23,Badge 11 - Mini-Diary: 3 stars and 1 wish â­ï¸â­ï¸â­ï¸ğŸš€,"â­ Metrices to measure performance are accuracy, precision, recall and F1score and selection is dependent on goals, tasks and ethical consideration.
â­ MLmetrics package in r helps in calculation of these metrics allowing comparison between manual and automated results.
â­ Error analysis helps to understand model performance beyond metrices
â“ Application of these metrices in real clinical text data would be important to understand usability "
235,Nasser Gaafar,8 Dec 2024 17:05,Badge 11 - Mini-Diary: 3 stars and 1 wish â­ï¸â­ï¸â­ï¸ğŸš€,"â­ Accuracy, precision, recall and F1 metrics
â­ Mlmetrics package will help do it, but manual checking is also important
â­ False positives and negatives are not just for lab tests
â“ Using and troubleshooting BIOS schema"
236,Helen Wu,6 Dec 2024 17:37,Badge 11 - Mini-Diary: 3 stars and 1 wish â­ï¸â­ï¸â­ï¸ğŸš€,"â­ï¸ Precision, Recall, F1 are metrics that tell you how good your model is at finding the right answers.
â­ï¸ Error analysis, looking at how to improve hidden biases and other areas.
â­ï¸BIO Schema making works as beginning, inside or outside helps with understanding how the model finds information in the text
ğŸš€Practicing error analysis within our own codes"
237,Louise Lau,6 Dec 2024 16:30,Badge 11 - Mini-Diary: 3 stars and 1 wish â­ï¸â­ï¸â­ï¸ğŸš€,"â­- Key metrics for performance measurement, such as accuracy, precision, recall and F1 score. 
â­- Other NLP metrics designed for other tasks that precision or recall may not be too applicable. Example would be for text summarization 
â­- Error analysis to understand what may have went wrong. Be aware of the potential bias - issue can be revealed by breaking-down performance analysis into smaller sub-group. 
ğŸš€- Options for evaluation of BIO schema, include exact match and those that provide partial credit, which enable better understanding of the model"
238,Nisha Daniel,5 Dec 2024 11:25,Badge 11 - Mini-Diary: 3 stars and 1 wish â­ï¸â­ï¸â­ï¸ğŸš€,"â­ Accuracy, precision, recall and F1 scores are key NLP performance metrics
â­ how BIO schemas can be used to simplify Named Entity REcognition.
â­Error analysis part
â“Practicing F1 scores and other error analysis methods."
239,Adijat Adenaike,1 Dec 2024 15:26,Badge 11 - Mini-Diary: 3 stars and 1 wish â­ï¸â­ï¸â­ï¸ğŸš€,"â­Major metrics used when measuring healthcare related NLP are Precision, Recall and F1 Score. Precision shows how accurate the positive predictions of a model are. Recall indicate how good a model is at finding all the positive instances. And F1 score is the harmonic mean of the two, and is useful for balancing between precision and recall.
â­Performance metrics are calculated using True Positives (TP: as correctly predicted positives), False Positives (FP: Incorrectly predicted as positive), False Negatives (FN: Incorrectly predicted as negative) and True Negatives (TN: Correctly predicted negatives).
â­Used the MLmetrics to compare actual vs predicted labels to evaluate model performance, evaluating how well the model matches the true labels. Also enjoyed the BIO Schema tagging.
â“I wish the course had worked through how the actual and predicted values were arrived at, using an actual dataset. Also, I am confused as to how the F1 Score was multiplied by 2 in the coding exercise while the formular shown in the video recording did not have that. I will be asking this question in the next session together."
240,Kate Rose,1 Dec 2024 13:17,Badge 11 - Mini-Diary: 3 stars and 1 wish â­ï¸â­ï¸â­ï¸ğŸš€,"â­ Accuracy, precision, recall and F1 are key NLP metrics when evaluating the performance of an NLP model 
â­ Using the Mlmetrics package to call precision, recall and F1 score functions. It was fun to compare manually calculating the metrics vs having the functions do it in our code during the Notebook exercises
â­ BIO tags allow for capturing for capturing specific sequences in the correct context, therefore the BIO schema is a helpful tool to measure the effectiveness of NLP performance
â“Would like to practise determination of a false positives vs false negatives (got myself a bit confused when I was labelling the inconsistencies I found in one of the Notebook exercises). Also just want to explore and check the F1 score equation - from the badge video it looked like F1 score = (precision * recall) / (precision + recall), but in the exercises we used 2 * (precision * recall) / (precision + recall)"
241,Isabel Santonja,1 Dec 2024 13:08,Badge 11 - Mini-Diary: 3 stars and 1 wish â­ï¸â­ï¸â­ï¸ğŸš€,"ğŸŒŸRecall is the same as sensitivity and precision is the same as positive predictive value. F1 is the harmonic measure of the previous two.
ğŸŒŸ Even if the performance metrics are high, it is worth doing the error analysis to check how the model is doing in user represented groups.
ğŸŒŸBIO Schema is a useful tool to evaluate NLP model performance.

ğŸ I need to find an strategy to remember all the different names the performance metrics haveâ€¦ "
242,Paolo Pricoco,30 Nov 2024 17:43,Badge 11 - Mini-Diary: 3 stars and 1 wish â­ï¸â­ï¸â­ï¸ğŸš€," ğŸŒŸ Performance evaluation in NLP can leverage different metrics, the most common of which are Accuracy, Precision, Recall and F1 Score. The choice of a metric is generally dictated by the goal of our task, along with practical and ethical considerations. For instance, if our aim is reduce false negatives as much as possible, we can opt for adopting Recall as a metric, since if factors in false negatives in its formula.

ğŸŒŸ Error analysis is another important part of performance analysis in NLP. Breaking down the dataset and the outcomes can help us identify in what conditions a model can be more prone to making mistakes. For instance, younger people may use a language that is different in terminology or sentiment to the one used by older generations.

ğŸŒŸ BIO Schema (Begin-Inside-Outside) can also be leveraged for performance analysis in NLP. It can help us identify in which step we are most likely to have made a mistake: tokenisation, annotation, model structure, labelling individual tokens, labelling entities, etc.

â“I am wondering how BIO Schema can, on a practical level, be used as a tool for performance analysis. Given its relatively simple structure, it is hard to imagine how such BIO Schema can be effectively used to evaluate model performance in NLP."
243,Kate Browne,29 Nov 2024 13:42,Badge 11 - Mini-Diary: 3 stars and 1 wish â­ï¸â­ï¸â­ï¸ğŸš€,"â­ Different metrics can be used to validate model predictions for different NLP tasks;
â­ Different NLP metrics (e.g. accuracy, precision, recall, F1 score) and how to
choose the most appropriate one (e.g. ethical and clinical considerations);
â­ The MLmetrics package can be used to calculate NLP metrics such as precision, F1 score in R. actual and predicted class labels are required. For binary classifications these are 0 or 1;
ğŸš€ Ensuring that the positive class is correctly specified when MLmetrics is used."
244,Franz KrÃ¤mer,26 Nov 2024 23:08,Badge 11 - Mini-Diary: 3 stars and 1 wish â­ï¸â­ï¸â­ï¸ğŸš€,"â­ï¸ The BIO tagging schema can be used as a diagnostic tool in performance measuring because it gives a fine-grained picture of the tagging at the level of the single tokens.
â­ï¸ For example, it might be used to evaluate if an NER tool works well on my text corpus.
â­ï¸ The package MLmetrics has useful functions to calculate performance metrics for binary classification and this can be further automated by using the merge() function to count true positives, false positives and false negatives.
ğŸš€ I think I will look into metrics evaluating text summarizations, this could prove very useful for a current work project."
245,Yi mei Low,26 Nov 2024 06:14,Badge 11 - Mini-Diary: 3 stars and 1 wish â­ï¸â­ï¸â­ï¸ğŸš€,"â­ï¸ The different modalities we can use for measuring NLP performance, an important consideration as in health and social care our outcomes may affect patients

â­ï¸ What are the differences between precision, recall and F1 scores and how we can calculate them

â­ï¸ Depending the intended purpose of model we are intending to use, we have to be able to determine which are the better/best performance scores to evaluate our model

ğŸš€ Although we probably don't need to go too in-depth regarding metrics for unstructured evaluation, it would be useful to briefly cover a little more on the topic!"
246,Rebecca Sewell,24 Nov 2024 22:26,Badge 11 - Mini-Diary: 3 stars and 1 wish â­ï¸â­ï¸â­ï¸ğŸš€,"â­ BIO schema matching is good for recognising multi-word entities and partial matches can inform annotation guidelines.
â­ Recall â€“ is an important metric in things like disease detection where missing positives (false negatives) could be costly.
â­ High metric scores for performance can mask underperformance in certain demographics or biases in the model which is why it is important to undertake bias analysis.

ğŸš€ I am interested in learning more about methods for assessing semantic similarity and context. "
247,Dylan Delmar,21 Nov 2024 20:49,Badge 11 - Mini-Diary: 3 stars and 1 wish â­ï¸â­ï¸â­ï¸ğŸš€,"â­ Different ways of measuring NLP model performance and the formulas for precision, recall, and F1 score
â­ How to choose the correct metric and error analysis based on the main goal of your model (are FP or FN more of a priority, etc)
â­ Using merge to evaluate true positives, false positives, and false negatives based on if they're in the identical merged data frame, only in the predictions, or only in the actuals
ğŸš€ Using and fixing BIO schema when it misses entities"
248,Joseph Oxley,20 Nov 2024 13:45,Badge 11 - Mini-Diary: 3 stars and 1 wish â­ï¸â­ï¸â­ï¸ğŸš€,"â­ï¸ How to write my own functions to calculate key NLP performance measures, such as precision, accuracy and f1 score.

â­ï¸The most important performance score depends on the purpose of the model. For example, a model used to make clinical predictions may want to have a low false negative rate above a low false positive rate, so recall would be a better measure than precision.

â­ï¸Error analysis of where the model is differing from true values can be used to improve the model. For example, are more results from a certain patient demographic incorrect?

ğŸš€ How precision and accuracy scores relate to sensitivity and specificity, and if it is possible or useful to use ROC curves to evaluate model performance in more detail."
249,Robin Nicholls,19 Nov 2024 18:32,Badge 11 - Mini-Diary: 3 stars and 1 wish â­ï¸â­ï¸â­ï¸ğŸš€,"â­ï¸ Key traditional metrics are accuracy, precision, recall, f1. For unstructured evaluation, metrics like BLEU or ROUGE may be used.

â­ï¸ F1-score is the harmonic mean between precision and recall.

â­ï¸ Different tasks will require different metrics for evaluation. In some cases a modification may be suitable (example F0.5 - F2-score).

ğŸš€ I'd like to spend more time looking into other metrics for evaluating unstructured data."
250,Pawel Orzechowski,14 Nov 2024 16:08,Badge 11 - Mini-Diary: 3 stars and 1 wish â­ï¸â­ï¸â­ï¸ğŸš€,"â­ï¸ this is an example answer

â­ï¸ write three things you learned (or which brought you joy)

â­ï¸ while watching video and completing the notebook

ğŸš€ and one thing you wish you understood better (and maybe what step you will take to address it)

---

(write your mini-diary in the above format AS A RESPONSE TO THIS THREAD (not to this example post)).

Instead of emoji you can use * ? or any other symbol that you like."
251,Swe Lynn,23 Dec 2024 21:56,Badge 12 - Mini-Diary: 3 stars and 1 wish â­ï¸â­ï¸â­ï¸ğŸš€,"â­ï¸ Ethics: helps prevent harm, ensure equitable treatment.
â­ï¸ Bias: in data, in algorithm. As in ethic, this can lead to patient harm.
â­ï¸ Ethical practitioner: safeguard, accountability, consent/permission. For patient safety, health equity and trust in healthcare system.
ğŸš€ I have listened to a talk about AI creations can stereotype in terms of gender, race etc. and when the lecture talked about algorithms can reinforced the bias in the data, this makes more sense. I will take more time to the mitigating steps to reduce bias. "
252,Yingqian Tang,9 Dec 2024 11:13,Badge 12 - Mini-Diary: 3 stars and 1 wish â­ï¸â­ï¸â­ï¸ğŸš€,"ğŸŒŸEthical Considerations: NLP must prioritize patient safety, privacy, and fairness.
ğŸŒŸBias in Data/Algorithms: Biased healthcare data can cause harm and discrimination.
ğŸŒŸBias Mitigation: Techniques like data diversity and fairness adjustments can reduce bias.
 â“I want to explore algorithm fairness adjustments more.

4o mini"
253,Nasser Gaafar,8 Dec 2024 17:16,Badge 12 - Mini-Diary: 3 stars and 1 wish â­ï¸â­ï¸â­ï¸ğŸš€,"â­The cumulative effect of bias on data and possibly on patient outcomes
â­ Responsible use of data
â­ Good practice dictates that ethics, informed consent, and patient considerations must be integrated into projects from the start
ğŸš€ I'd like to explore algorithms for detecting bias"
254,Helen Wu,6 Dec 2024 18:05,Badge 12 - Mini-Diary: 3 stars and 1 wish â­ï¸â­ï¸â­ï¸ğŸš€,"â­ï¸ Important to consider ethics in NLP practices as it helps to promote fairness, transparency, patient safety and inclusivity.
â­ï¸ Algorithms trained on biased data can amplify the bias. It's important to identify bias before using it to train the model
â­ï¸Methods to mitigate bias using techniques like resampling, data augmentation etc.
ğŸš€explore and read up on real-life scenarios of training algorithms and how the trainers try to mitigate these biases."
255,Louise Lau,6 Dec 2024 16:42,Badge 12 - Mini-Diary: 3 stars and 1 wish â­ï¸â­ï¸â­ï¸ğŸš€,"â­- Various aspects of ethical consideration when it comes to NLP for health and social care, such as promoting diversity and inclusivity.
â­- Important reminder of always being mindful of bias, from the sources of data to the later development phases.
â­- Potential bias mitigation method, such as algorithm adjustment and bias detection with resampling. 
ğŸš€- Would like to learn more about how to develop model that based on these bias exists, (i.e text resampling and balancing & the algorithm adjustment mentioned"
256,Nisha Daniel,5 Dec 2024 11:27,Badge 12 - Mini-Diary: 3 stars and 1 wish â­ï¸â­ï¸â­ï¸ğŸš€,"â­ Using Data responsibly is ethical for a data scientist 
â­ Models can pick up bias while training data
â­how unethical practices can lead to risks
â“how to eliminate bias while training data"
257,Adijat Adenaike,1 Dec 2024 16:22,Badge 12 - Mini-Diary: 3 stars and 1 wish â­ï¸â­ï¸â­ï¸ğŸš€,"â­The importance of ethics in NLP and an accurate understanding of what might constitute bias in ones data. i.e. Gender, socio-economic and racial or ethnic bias.
â­Mitigate against bias by using strategies that include representative data, fairness-aware algorithms, explainable models among others.
â­Be an ethical practitioner by ensuring fairness, transparency and maintain privacy.
â“I wish the course had shown how to use one of the tools used in mitigating against bias on an actual dataset."
258,Isabel Santonja,1 Dec 2024 15:28,Badge 12 - Mini-Diary: 3 stars and 1 wish â­ï¸â­ï¸â­ï¸ğŸš€,"ğŸŒŸ Ethics in NLP involves thinking about reducing the risk of bias.
ğŸŒŸ Algorithms are likely to pick up and reinforce biases.
ğŸŒŸThere are techniques that try to mitigate biases in NLP modes, such as debiasing embeddings (mitigating unwanted biases present in word embeddings, such as â€œwomanâ€ and â€œnurseâ€) or adversarial weighting (assigning weights to examples in the training data based how difficult these cases are to predict for a ML model).

ğŸ I was curious if some of these techniques were being used in ChatGPTâ€¦so I asked it about that!"
259,Paolo Pricoco,30 Nov 2024 19:37,Badge 12 - Mini-Diary: 3 stars and 1 wish â­ï¸â­ï¸â­ï¸ğŸš€,"ğŸŒŸ NLP practitioners should always guarantee patient safety, privacy, fairness and inclusivity in our profession as NLP practitioner and healthcare professionals. Overlooking these crucial aspects in health care can lead to an exacerbation of cognitive bias and inequalities in healthcare, which can ultimately cause harm to patients and a decrease in trust in healthcare systems.

ğŸŒŸ To avoid bias related to demographic characteristics of patients, such as age, gender, ethnicity/race and religion, it is good practice to ensure our data is balanced among different demographic groups. Also, it is paramount to consider how we source data, making sure it doesnâ€™t exclude relevant groups and minorities in the population.

ğŸŒŸ To mitigate the risk of bias, we have several tools at our disposal, such as disparity analysis; resampling; algorithm fairness adjustment like adversarial weighting, fair embedding techniques, fairness constraints. Moreover, regular auditing and monitoring of the sourced data and employed algorithms are fundamental to keep our models fair and inclusive.

â“ I am curious to know which tools to assess fairness of our models are best suited for an intermediate level NLP practitioner and to what extent they can assure us that ethical pre-requisites are satisfied."
260,Franz KrÃ¤mer,26 Nov 2024 23:56,Badge 12 - Mini-Diary: 3 stars and 1 wish â­ï¸â­ï¸â­ï¸ğŸš€,"â­ï¸ Poor bias and risk mitigation can not only lead to patient harm but also loss of trust in a whole system.
â­ï¸ There are many risk mitigation techniques and strategies, including making NLP models explainable, working on algorithms to make them fairer (e.g., by applying weighting techniques) and evaluating not only the correctness of the output but also its fairness, for instance with regard to groups underrepresented in the data.
â­ï¸ Auditing and monitoring needs to be done continually.
ğŸš€ Explainable AI concepts are very interesting, for instance, token probability in neural nets, and also the question of how to represent this rather â€œtechnicalâ€ information to users in a non-overwhelming way."
261,Yi mei Low,26 Nov 2024 07:36,Badge 12 - Mini-Diary: 3 stars and 1 wish â­ï¸â­ï¸â­ï¸ğŸš€,"â­ï¸ As we proceed with our work, it is important to remember the implications on the patients and that we always make sure to use ethical practices

â­ï¸ In addition to the potential effects on patients, if NLP is not used appropriately it can result in the erosion of the public's trust in our AI-based technology and NLP

â­ï¸ The fact that just because we can do something, does not mean we should. We must have an appropriate reason and intent before moving forward

ğŸš€ It would be nice to delve further into the methods for reducing bias"
262,Rebecca Sewell,24 Nov 2024 22:19,Badge 12 - Mini-Diary: 3 stars and 1 wish â­ï¸â­ï¸â­ï¸ğŸš€,"â­ It is important to identify demographic makeup of dataset before designing models to anticipate biases.
â­ Continuous model evaluation and bias detection is important to ensure models remain fair over time.
â­ Accountability about reasoning for undertaking a particular study is important. Just because we can and are well meaning, isnâ€™t valid enough reason to go ahead with analysis.
ğŸš€ I am interested in learning more about adversarial de-biasing methods. "
263,Kate Rose,24 Nov 2024 20:59,Badge 12 - Mini-Diary: 3 stars and 1 wish â­ï¸â­ï¸â­ï¸ğŸš€,"â­ Critical to ensure ethical considerations are central to NLP model design to ensure equitable treatment whilst avoiding risks to patient safety, as well as loss of patient trust in healthcare systems if there is a perception of bias in models
â­ As an ethical NLP practitioner it is important to understand the data you are working with and consequently any bias that may exist in the data
â­ Variety of ways to mitigate bias, such as: bias detection (data audit, disparity analysis, re-sampling); algorithm fairness adjustments (adversarial de-biasing); explainable models
â“I am keen to learn more about how models account for bias in data and the instruments to measure the effectiveness of bias reduction"
264,Kate Browne,23 Nov 2024 17:19,Badge 12 - Mini-Diary: 3 stars and 1 wish â­ï¸â­ï¸â­ï¸ğŸš€,"â­Importance of understanding the dataset's demographics to mitigate the risk of the algorithm perpetuating bias;
â­Techniques, such as SHAP (SHapley Additive exPlanations) and LIME (Local Interpretable Model-agnostic Explanations), that leverage explainable models to mitigate bias in NLP models;
â­Techniques such as debiased embedding and adversarial debiasing, for algorithm fairness adjustments;
ğŸš€Bias can intersect in complex ways. How do models account for these intersections to ensure fairness across multiple dimensions."
265,Joseph Oxley,"22 Nov 2024 09:46
(Edited by Joseph Oxley on 22 Nov 2024 09:46)",Badge 12 - Mini-Diary: 3 stars and 1 wish â­ï¸â­ï¸â­ï¸ğŸš€,"â­ Some methods for measuring model bias in a model, such as testing to see if there's a difference between true positive or false positive rates between different demographic groups.
â­ Aware that any bias in training data may cause model performance to differ between groups and that this can have significant clinical impact in the health & social care setting.
â­ How bias in clinical models can erode public trust in AI and Natural Language Processing, and how its the responsibility of the researcher to prevent this.

ğŸš€ Some example of how to deal with this in practice, for example if using a national dataset that proportionally represents a country, how would you ensure small demographic groups are not biased by the model?"
266,Dylan Delmar,21 Nov 2024 20:56,Badge 12 - Mini-Diary: 3 stars and 1 wish â­ï¸â­ï¸â­ï¸ğŸš€,"â­ How bias in data accumulates and can affect the model, which in turn affects patient outcomes and treatments
â­ Some ways in which to account for or compensate for bias that exists in the data, including bias detection, algorithm fairness adjustments, and fairness evaluation
â­ Ethics, consent, and patient consideration must be built into the project from the beginning
ğŸš€ I'd like to see the algorithms for detecting bias and accounting for it in actions"
267,Robin Nicholls,19 Nov 2024 18:39,Badge 12 - Mini-Diary: 3 stars and 1 wish â­ï¸â­ï¸â­ï¸ğŸš€,"â­ï¸ Ethics should be built into every process to limit harm and help promote fairness.
â­ï¸Data often contains bias that must be identified and actioned to limit the chance of our models reinforcing that bias.
â­ï¸ Bias in data can be mitigated through steps like data diversity, bias detection, fairness adjustments, etc...

ğŸš€ I'd like to explore explainable models and understand how they are able to confidently explain reasoning."
268,Amit Mishra,18 Nov 2024 16:45,Badge 12 - Mini-Diary: 3 stars and 1 wish â­ï¸â­ï¸â­ï¸ğŸš€,"â­ï¸ Importance of ethics in NLP
â­ï¸ How to mitigate bias in health data- various methods
â­ï¸ What harm can biased data bring to the model and erode public trust on AI

ğŸš€ It would have been great to present at least one example of how bias can impact output in a model. Anyways Google should help with this."
269,Pawel Orzechowski,14 Nov 2024 16:08,Badge 12 - Mini-Diary: 3 stars and 1 wish â­ï¸â­ï¸â­ï¸ğŸš€,"â­ï¸ this is an example answer

â­ï¸ write three things you learned (or which brought you joy)

â­ï¸ while watching video and completing the notebook

ğŸš€ and one thing you wish you understood better (and maybe what step you will take to address it)

---

(write your mini-diary in the above format AS A RESPONSE TO THIS THREAD (not to this example post)).

Instead of emoji you can use * ? or any other symbol that you like."
270,Swe Lynn,24 Dec 2024 09:46,Badge 13 - Mini-Diary: 3 stars and 1 wish â­ï¸â­ï¸â­ï¸ğŸš€,"â­ï¸ Privacy risk: Direct identifiers (identity) and indirect identifiers (social, disease, crime).
â­ï¸ Fascinating to learn how to minimise the privacy risks (direct/indirect identifiers).
â­ï¸ Understanding data is key. I believe this is recurring theme in NLP. Understanding through preprocessing, visualisation, cleaning etc. will help choose appropriate NLP models.
ğŸš€ It is so inspiring to hear from fellow clinician who has expertise in the computer science. I do agree that there should be a specialty with dual interest (tech and clinical). Thank you for arranging the chat. "
271,Yingqian Tang,"9 Dec 2024 11:23
(Edited by Yingqian Tang on 9 Dec 2024 11:26)",Badge 13 - Mini-Diary: 3 stars and 1 wish â­ï¸â­ï¸â­ï¸ğŸš€,"ğŸŒŸ Indirect identifiers like age or occupation can be just as critical as direct ones in privacy risk detection.
ğŸŒŸ NLP helps automate the identification of privacy risks in clinical text, making the process more efficient.
ğŸŒŸ Labeling datasets (e.g., discharge summaries) is key to identifying privacy risks.
â“I want to read more essays relevant to indirect privacy risk."
272,Helen Wu,8 Dec 2024 19:50,Badge 13 - Mini-Diary: 3 stars and 1 wish â­ï¸â­ï¸â­ï¸ğŸš€,"â­ Privacy risks in clinical texts (indirect identifiers, exploring the importance of understanding the nuance for data protection and confidentiality
â­Using NLP to identify and mitigate privacy risks which can help automating risk detection
â­ Annotation and Data labelling is important, learnt about the Scottish standards for labelling direct identifiers
â“Leaning about the real-world application of using NLP to automate risk detection."
273,Kate Browne,8 Dec 2024 18:07,Badge 13 - Mini-Diary: 3 stars and 1 wish â­ï¸â­ï¸â­ï¸ğŸš€,"â­ Categorisaton of privacy risks for text data (i.e. direct versus indirect identifiers);
â­ Approaches to use annotation to de-identify confidential clinical data(e.g. start by looking at what others have done etc.)
â­ To develop NLP algorithms, labelled data is essential for training the model and evaluating it's performance;
â“ Use of BERT for sentiment analysis and topic modelling."
274,Nasser Gaafar,"8 Dec 2024 17:24
(Edited by Nasser Gaafar on 8 Dec 2024 17:25)",Badge 13 - Mini-Diary: 3 stars and 1 wish â­ï¸â­ï¸â­ï¸ğŸš€,"â­Privacy risks in clinical text due to potential exposure of sensitive personal health information even without identifiers
â­ Addressing risks posed by direct and indirect identifiers in NLP in real world settings is still a work in progress.
â­ Advancements need to occur in collaboration with clinicians and patients
â“ I need to pick my brain to see how all this could be applied to a real world setting"
275,Louise Lau,6 Dec 2024 21:50,Badge 13 - Mini-Diary: 3 stars and 1 wish â­ï¸â­ï¸â­ï¸ğŸš€,"â­- The difference between direct identifiers and indirect identifiers, common indirect identifiers are grouped into five main categories, such as location related or mental health related
â­ - Record accumulation can also pose potential concerns
â­- Privacy risk mitigation tool needs to be customized and is not one-size-fit-all - example is that people from different age group may speak and write differently 
ğŸš€- Would love to learn more about how automation (or NLP technique) can be used to help with identifying privacy risks "
276,Nisha Daniel,5 Dec 2024 11:31,Badge 13 - Mini-Diary: 3 stars and 1 wish â­ï¸â­ï¸â­ï¸ğŸš€,"â­ What are direct and indirect identifiers 
â­ Automation or NLP can reduce privacy risks
â­Case study approach is mind-boggling!
â“Would like to practice them in our assessment"
277,Paolo Pricoco,5 Dec 2024 11:25,Badge 13 - Mini-Diary: 3 stars and 1 wish â­ï¸â­ï¸â­ï¸ğŸš€,"ğŸŒŸIn NLP practice, it is fundamental to keep data private. Data privacy can be jeopardized by direct and indirect identifiers. While the former contain clear data that is supposed to be kept confidential (e.g., address, phone number, religion, sexual orientation), the latter consist in clues that can help identify private data (e.g., the position of a userâ€™s relative to a certain building, or a high number of yearly haematological clinical check-ups; these bits of information could help narrow down the list of possible subjects by identifying their accommodation or the presence of a haematological disease).

ğŸŒŸUnfortunately, many NLP algorithms designed to spot direct and indirect identifiers do not seem to work very well in real contexts, as they struggle with picking up on language nuances, shifts, jargon, etc. It comes as no surprise that this phenomenon is enhanced when trying to detect indirect identifiers, as they may be less likely to have a clear recurrent pattern (for example, as opposed to a direct identifiers such as age, where we generally encounter a number followed by â€œyear oldâ€).

ğŸŒŸ In Scotland, a topic modelling analysis on hospital discharge reports was conducted and five main macro-area topics were found: (1) unique events (events that happened, like an incident); (2) social information: parents, children, family, school, etc; (3) location and organizations (mainly places); (4) mental health: details about psychiatric disorders; (5) police and crime: criminal record.
I found very interesting that when privacy risk was higher for one topic, it was also likely to be higher for other topics.

â“ With regards to the aforementioned topic modelling analysis on hospital discharge reports, I wonder if a NLP tool could be created, which helps the professional identify the causes of increased privacy risk across more than one topic as opposed to one specific topic. For example, in some cases this tool could detect malpractice in data recording that affects all area of interest, while in other cases it could spot a flaw in data collection related to only one area (and perhaps promote training and education for the staff responsible for that area)."
278,Kate Rose,"3 Dec 2024 21:49
(Edited by Kate Rose on 3 Dec 2024 21:51)",Badge 13 - Mini-Diary: 3 stars and 1 wish â­ï¸â­ï¸â­ï¸ğŸš€,"â­ Lovely to be walked through the case study on practical application of an NLP tool and the journey to find patterns in the discharge summaries & radiology reports, as well as learning about direct identifiers and indirect identifiers and the possible risks to not having a robust mechanism in place to mitigate their presence in unstructured data
â­ Fascinating fireside chat - interesting to hear about desired outcomes from NLP technology. Interestingly in my professional role, I have worked on creating what we call an 'After Visit Summary' (AVS) which is specifically generated for a patient following a clinic appointment or inpatient admission. Much of this document is generated through a combination of structured information ( eg. discrete home medications list) and non structured text (including written post-discharge/follow-up instructions for the patient). Whilst not fully automated (since it relies on a clinician writing instructions and prescribing/amending drugs as needed) the document is generated into a structured, patient-friendly one-pager which we send to their patient portal if they signed up for an account. I'm so curious if NLP would be able to produce the same document, or better, with less up front work involved
â­Another interesting point - importance of engaging clinicians in system/NLP design. I completely agree, but I'd also add that it is beneficial if the clinicians involved also have at least a baseline understanding of how the core technology works (whether NLP, an EPR, analytics, etc.) The stark reality of working in underfunded health sectors is that we don't have endless time, resources and money to design clinical workflows which we then have to rebuild or completely unpick due to a lack of understanding of the technology from project commencement, and the risk of the just 'replicate what we did on paper' mindset. Digital systems absolutely need to be designed by clinicians with patients at the heart, but the pre-digital process/pathway also needs to be robust
â“""Garbage in, garbage out"" - fascinating point. I find this particularly interesting because I hear the term ""garbage"" floated around fairly frequently when used to describe the impact of poor quality documentation on trying to extract discrete data (let alone unstructured data) from databases. The question I have on this point is, are we saying ""garbage"" documentation is down to poor practice of clinicians documenting superfluous/inappropriate data, or is this subjective because ""superfluous"" to one specialty is ""beneficial"" to another. If the latter, this seems like a massive consideration in NLP model design; how do we define what is important and what is not if clinicians cannot agree or have differing opinions on this?

(apologies for the very long post for this badge, I just had so many thoughts)"
279,Isabel Santonja,2 Dec 2024 11:27,Badge 13 - Mini-Diary: 3 stars and 1 wish â­ï¸â­ï¸â­ï¸ğŸš€,"ğŸŒŸ There are direct and indirect identifiers, and we need to think about both to ensure data privacy.
ğŸŒŸAnnotation can help understand privacy risk and automation
ğŸŒŸA lot of the NLP algorithms developed to remove direct identifiers donâ€™t perform that well with real-world clinical data.

ğŸ I would love to hear about uses of NLP in clinical epidemiology or clinical trialsâ€¦"
280,Adijat Adenaike,"1 Dec 2024 19:36
(Edited by Adijat Adenaike on 1 Dec 2024 20:27)",Badge 13 - Mini-Diary: 3 stars and 1 wish â­ï¸â­ï¸â­ï¸ğŸš€,"â­Understanding of the importance of privacy risks in clinical text which arises from the potential exposure of sensitive personal health information even when identifiers are removed.
â­Identifiers comes in 2 categories namely direct and indirect identifiers. There are NLP algorithms that address these problem with a lot of available tools for direct identifiers. But these algorithms often do not translate to clinical settings as they fail to capture issues such as local nuances that are typical in health based reports.
â­ Steps to develop new approaches that address privacy risks include the use of annotation as a foundation to build ones model on. Sentence similarity based topic modelling is one of the approaches that can be used in NLP algorithm model development. The fireside chat is quite interesting. Learning about the challenges of bridging theories with the practicality of real life deployment.
â“I don't fully get how the model will be applied in reality. I wish there was a coding section for this that showed a step by step practical example."
281,Robin Nicholls,1 Dec 2024 18:58,Badge 13 - Mini-Diary: 3 stars and 1 wish â­ï¸â­ï¸â­ï¸ğŸš€,"â­ï¸ It was interesting to learn and be aware of the number of indirect risks that can be present in the data, and the information they can potentially expose when combined.
â­ï¸ Great to listen to Dr Samuel describe examples of aiding patients and clinicians through the use of NLP approaches, specifically when thinking about utilising transformers.
â­ï¸ Really important to be conscious of how much unstructured valuable data is in in health care and how useful/important NLP tools can be (when designed correctly with user input!).
ğŸš€ It would be great to hear of more details (see more examples of success) on the problems around notes (or any unstructured data) in health care and approaches to improving through NLP."
282,Rebecca Sewell,1 Dec 2024 14:14,Badge 13 - Mini-Diary: 3 stars and 1 wish â­ï¸â­ï¸â­ï¸ğŸš€,"â­ï¸ Indirect privacy risks often come when information can be combined to make someone identifiable. Eg. language that gives away age, location, unique events.
â­ï¸ There are few tools which focus on indirect identifiers. These indirect privacy risks can be investigated through sentence similarity and topic modelling to categorize privacy risks.
â­ï¸ The importance of end-user input when designing systems was again reinforced for me.
ğŸš€ I was wondering about ways a new AI or NLP tool for healthcare providers could be truly and thoroughly tested in a real-world environment before roll-out to the high risk environment of actual hospitals/care facilities. Could a â€˜mockâ€™ or simulated hospital be created somehow? If there are going to be a lot of new tools, investment into ways to test them seems very valuable."
283,Franz KrÃ¤mer,28 Nov 2024 14:20,Badge 13 - Mini-Diary: 3 stars and 1 wish â­ï¸â­ï¸â­ï¸ğŸš€,"â­ï¸ It's interesting to see that user-driven or participatory development of digital solutions is crucial for solving real-world problems, yet it's underused in healthcare. And we're facing the same issue in other sectors (education or public administration, for example).
â­ï¸ In healthcare, one valuable application of NLP could be summaries of studies for informational / educational purposes. The summaries could save highly specialised clinicians some â€œadmin workâ€ time and allow patients to understand the science behind their treatment better.
â­ï¸ Explainability of NLP and AI models and outcomes is an important issue that might (hopefully) gain even more traction in the future.
ğŸš€ Iâ€™d find it interesting to know more about how NLP techniques are implementend in the day-to-day work flow and how organisational barriers (I assume there are a few) can be overcome."
284,Yi mei Low,27 Nov 2024 07:33,Badge 13 - Mini-Diary: 3 stars and 1 wish â­ï¸â­ï¸â­ï¸ğŸš€,"â­ It is important that we keep in mind indirect identifiers, it is easy to forget how much information can be gleaned from these, such as in the example of having a cafe across one's house
â­While a large utility of NLP is in research, we can also look at direct patient care and improving our communications with patients, such as summaries after the consult
â­Much of the data that is received is much messier and likely full of identifiers (both direct and indirect), and then how we can build an NLP method to help with de-identification
ğŸš€It was great hearing from Dr Samuel, it would be good to also hear how we can discuss NLP in more front-line fields such as primary care and the emergency departments!"
285,Dylan Delmar,27 Nov 2024 00:02,Badge 13 - Mini-Diary: 3 stars and 1 wish â­ï¸â­ï¸â­ï¸ğŸš€,"â­ How annotation can assist in making more informed decisions about data access and help understand how to ensure confidentiality
â­ The importance of testing on real, raw data and how models that work for research data that is already more sanitized and free of identifiable information is not always a good measure
â­ Layers of indirect privacy risk and how these things can be tied back to a particular person
ğŸš€ Incorporation of NLP analysis with shiny dashboards for visualization in order to assist clinicians"
286,Amit Mishra,24 Nov 2024 13:18,Badge 13 - Mini-Diary: 3 stars and 1 wish â­ï¸â­ï¸â­ï¸ğŸš€,"â­ï¸ Importance of privacy risk in clinical risk and how to ensure protection of direct and indirect identifiers.
â­ï¸ Methods of protecting privacy through label annotations of direct identifiers. Five indirect privacy risks and how to approach to build an NLP solution for privacy protection.
â­ï¸ Utility of NLP in building a clinical story of a patient, identifying more granular information to inform clinical decision-making, understanding and improving quality of care and research.

ğŸš€ Specific NLP examples from current work happening in healthcare."
287,Joseph Oxley,22 Nov 2024 11:11,Badge 13 - Mini-Diary: 3 stars and 1 wish â­ï¸â­ï¸â­ï¸ğŸš€,"â­ï¸ Gained an appreciation for the way text data is currently used within NHS clinical settings and the challenges this poses to the best quality patient care and how NLP can be used to enable clinicians to deliver better treatment.

â­ï¸The importance of annotation rules, how in the example a standard was created that allowed the annotation categories to be defined.

â­ï¸Value of reading literature around a problem to identify what has and hasn't been done before. This includes how models perform in different context, as the literature may contain many models that perform well in some contexts but fail to generalise.

ğŸš€ More about visualising the identifiable data model output, and how output from a NLP model could be meaningfully presented."
288,Swe Lynn,24 Dec 2024 10:10,Badge 14 - Mini-Diary: 3 stars and 1 wish â­ï¸â­ï¸â­ï¸ğŸš€,"â­ï¸ Healthcare, terminology, language, practice is constantly changing, and the models need to be retrained for longevity.
â­ï¸ Sustainability needs to be considered at point of designing the model, including funding and expertise to sustain.
â­ï¸ I learned the idea of â€œbalance automation with oversightâ€.
ğŸš€ It is reassuring to learn the future of NLP in health and social care as one school of thought I have heard was NLP might become obsolete with evolvement of generative AI. I feel itâ€™s worth investing time in NLP. "
289,Yingqian Tang,9 Dec 2024 11:31,Badge 14 - Mini-Diary: 3 stars and 1 wish â­ï¸â­ï¸â­ï¸ğŸš€,"ğŸŒŸ NLP models can degrade over time as clinical practice and language change, which means regular retraining is essential.
ğŸŒŸ how important it is for NLP tools to integrate smoothly into healthcare workflows, balancing automation with human oversight.
ğŸŒŸ NLP has the potential to address health inequalities and improve collaboration across healthcare systems in the future.
â“I want to understand more about how to design NLP models that are sustainable and adaptable over time."
290,Rebecca Sewell,9 Dec 2024 11:13,Badge 14 - Mini-Diary: 3 stars and 1 wish â­ï¸â­ï¸â­ï¸ğŸš€,"â­ Importance of considering long term sustainability of design and considering funding and availability of people to maintain the model.
â­ It is essential that tools used in the healthcare setting complement human judgement, provide explanations for recommendations, and do not make tasks more complicated.
â­ Consideration of potential impact and implementation of future regulation changes
ğŸš€ Would models need to be fully re-trained in different countries or regions with differing populations and differing potential biases. "
291,Helen Wu,8 Dec 2024 19:56,Badge 14 - Mini-Diary: 3 stars and 1 wish â­ï¸â­ï¸â­ï¸ğŸš€,"â­ Importance of monitoring and performance maintenance in adapting to new information so we can keep up with the ever-changing landscape in healthcare, language and research.
â­ NLP tools should be user-friendly and be able to integrate into the healthcare setting with minimal friction in the day-to-day workings of healthcare professionals and administrators
â­ NLP tools should focusing on improving patient-centered care by working alongside clinical experts and compliment their day to day performance.
ğŸš€ Lean about what challenges are out there to make NLP adaptable in different healthcare settings"
292,Kate Browne,8 Dec 2024 18:28,Badge 14 - Mini-Diary: 3 stars and 1 wish â­ï¸â­ï¸â­ï¸ğŸš€,"â­ Specific factors that must be taken into account for the maintenance of NLP algorithms over time (e.g. concept drift, data distribution changes, ethical considerations, testing aginst edge cases etc);
â­ Importance of interoperability of NLP tools in user friendly interfaces to enhance clinical decision-making by providing timely access to relevant information;
â­ NLP tools should compliment not replace judgement and expertise (importance of ensuring that clinicians understand why a decision was made with the tool and can override that);
ğŸš€ Security vulnerabilities of open source NLP tools (e.g. compliance with regulations such as GDPR)."
293,Nasser Gaafar,8 Dec 2024 17:29,Badge 14 - Mini-Diary: 3 stars and 1 wish â­ï¸â­ï¸â­ï¸ğŸš€,"â­ Monitor, monitor, monitor and adapt, improve, and upgrade
â­ Interoperability across real-world systems should improve acceptance and utilisation
â­ NLP can be harnessed to support patients in multiple ways
ğŸš€ Open source tools for NLP developed in a way similar to Rstudio is food for thought"
294,Louise Lau,6 Dec 2024 22:00,Badge 14 - Mini-Diary: 3 stars and 1 wish â­ï¸â­ï¸â­ï¸ğŸš€,"â­- Regular monitoring is important for many aspects of NLP development - such as monitoring of regulation, and monitoring of concept drift
â­- Consideration of interoperability across real-world system - such as being user-friendly, providing simplification and workload reduction
â­- Future of NLP in the domain of health and social care, such as mobility assistant or mental health support for more holistic care
ğŸš€- Would love to learn more about the topic of future proof design in NLP tool - for example the modular design and the use of open source tool "
295,Paolo Pricoco,5 Dec 2024 11:42,Badge 14 - Mini-Diary: 3 stars and 1 wish â­ï¸â­ï¸â­ï¸ğŸš€,"ğŸŒŸ As both daily spoken language and medical terminology shift and change over time, it is important that NLP algorithms are designed to easily be kept up to date. That means they should be easily modifiable, implementable, and interpretable. This required a long-term vision that entails both the structure of the code, which should be divided into clearly identifiable sections to help coders interpret it, and a gaze into how social and linguistic trends, models, ideologies and pattern are changing and are likely to change in the near and not-so-near future.

ğŸŒŸ Also, it is important to make NLP algorithm output interpretable and explainable by human operators. As we know, AI methods (including NLP algorithms) run the risk of creating poorly interpretable or explainable outputs, as the coded decision-making process is sometimes so complicated and so many times iterated that it could be lost as the code is running. Especially in healthcare, it is important that our NLP algorithms are accompanied by a description of how the major decisions were made and major findings were obtained. In this way, AI will not dictate our decisions and the resulting policy, but rather it will be a support in our decision-making.

ğŸŒŸ NLP should also make user experience easier and increase efficiency. Often times very complicated algorithms may result in user experience becoming less intuitive and both coders and healthcare professionals may get lost in the sea of options and sub-settings that sophisticated NLP tools provide. On the contrary, NLP should integrate into human experience as a â€œsilent collaboratorâ€, increasing efficiency where humans lack tools and computing (brain) power, and ensuring clarity and readability of procedures and outcomes.

â“ Iâ€™m wondering if a gigantic NLP algorithm could be created, which periodically surfs the internet and learns about ever-new trends and shifts in languages, in order to gives us a report about major shifts or even being able to upload and modify pre-existing NLP algorithms utilized, for instance, in the healthcare sector."
296,Nisha Daniel,5 Dec 2024 11:34,Badge 14 - Mini-Diary: 3 stars and 1 wish â­ï¸â­ï¸â­ï¸ğŸš€,"â­ Regular updating of data is essential for successful modelling 
â­Interoperability across systems must be given importance
â­Balance automation with oversight
â“Curious about Interoperability"
297,Kate Rose,"2 Dec 2024 21:36
(Edited by Kate Rose on 2 Dec 2024 21:37)",Badge 14 - Mini-Diary: 3 stars and 1 wish â­ï¸â­ï¸â­ï¸ğŸš€,"â­ The importance of maintenance (including updates) of NLP models over time in a backdrop of changing variables over time, such as diseases, terminology and policies
â­ The criticality of NLP tools earning the trust of clinicians as they become more widespread. Thus, it is important that these tools are transparent, and can objectively demonstrate how conclusions are reached and why, as a prerequisite to adoption as clinical decision making tools
â­ The 'silent collaborator' - NLP tools should seamlessly integrate with clinician or administrative professionals' workflows as a supplementary add-on, enhancing user experience rather than hindering user experience
â“Gaining a deeper understanding of the concerns/ reservations/ barriers to adopting NLP technologies in the health and social care space from my clinician colleagues (if we ever have capacity in our working days to do so.....)"
298,Isabel Santonja,2 Dec 2024 12:19,Badge 14 - Mini-Diary: 3 stars and 1 wish â­ï¸â­ï¸â­ï¸ğŸš€,"ğŸŒŸIt is crucial to monitor performance of our NLP model over time and perform regular updates.
ğŸŒŸ An algorithm is only as good as its ability to be adopted in a real-world setting.
ğŸŒŸNLP tools should support clinicians and administrative personell, not replace their judgement and expertise!

ğŸ I wish I can use some of the ideas in this badge to calm some of the anxieties clinical staff about being replaced with AI!"
299,Adijat Adenaike,1 Dec 2024 21:51,Badge 14 - Mini-Diary: 3 stars and 1 wish â­ï¸â­ï¸â­ï¸ğŸš€,"â­Performance maintenance through continuous monitoring and updates, ensuring compliance with legal standards, and adhering to privacy regulations.
â­Usability and integration to ensure tools are user-friendly and compatible with existing healthcare systems, making them easy for healthcare professionals to use in their daily workflows.
â­NLP will improve how health data are processed and interpreted, leading to better patient outcomes and more efficient healthcare systems.
â“I wish there's a practical example of how performance maintenance is actually carried out."
300,Robin Nicholls,1 Dec 2024 19:20,Badge 14 - Mini-Diary: 3 stars and 1 wish â­ï¸â­ï¸â­ï¸ğŸš€,"â­ï¸ Time - There are lots of considerations that should be made when developing and deploying NLP tools with regards to time; content and data drift being one such issue.
â­ï¸ Include the user at every step of development (no matter how good the functionality is, if the interface is poorly designed or integrated, it will not be used)
â­ï¸ Great to hear about the impact NLP is and can have in the future!
ğŸš€ Have there been any initiatives to use LLMs in the NHS specifically? if so, what considerations around data security have been made? NHS locally deployed models over APIs owned by model owners?"
301,Franz KrÃ¤mer,28 Nov 2024 15:38,Badge 14 - Mini-Diary: 3 stars and 1 wish â­ï¸â­ï¸â­ï¸ğŸš€,"â­ï¸ NLP models must be constantly be maintained to keep up with language changes over time, changes in concepts, changes in data distributions, regulatory changes, changes in compliance etc.
â­ï¸ Future applications of NLP could break down silos between health and social care (e.g., by analysing social factors of health like housing or access to certain services), contributing to a more holistic view on health.
â­ï¸ NLP tools are for humans and therefore, we need to think about how they can be built to fit into workflows, integrate with existing systems and conditions and to be trustworthy.
ğŸš€ It would be interesting to know more about real world cases of NLP tool development and implementation in an organizational context and learn from them about which actors to involve, how to involve them, what to avoid."
302,Yi mei Low,27 Nov 2024 09:06,Badge 14 - Mini-Diary: 3 stars and 1 wish â­ï¸â­ï¸â­ï¸ğŸš€,"â­ï¸The way we think about and use NLP must be flexible and dynamic to keep up with the rapidly evolving nature of healthcare
â­ï¸ It was interesting to think about how we also have to consider the hardware behind the technology, whether we have the budget for things such as powerful GPUs - something we tend to forget as we learn the theory side of things
â­ï¸Being able to increase transparency behind NLP/AI decision makings would help to reduce mistrust in this technology
ğŸš€ This is more for the future but if/when there are laws (rather than guidelines) developed for the usage of AI in healthcare, it would be good to discuss it!"
303,Dylan Delmar,27 Nov 2024 00:15,Badge 14 - Mini-Diary: 3 stars and 1 wish â­ï¸â­ï¸â­ï¸ğŸš€,"â­ Multiple ways an algorithm must be maintained, not exclusive to language changes but also the rapidly changing and growing field of medicine
â­ True goals of NLP tools and their usefulness or lack thereof depending on meeting those goals
â­ Future applications including continuous analysis for better predictive medicine and population health analysis
ğŸš€ I'd like to know more about the process of widespread data analysis for public health and policy decisions"
304,Amit Mishra,24 Nov 2024 13:54,Badge 14 - Mini-Diary: 3 stars and 1 wish â­ï¸â­ï¸â­ï¸ğŸš€,"â­ï¸ Monitoring the performance of NLP models over time is important since the technology, data, and healthcare knowledge base changes with research.
â­ï¸ In the future, NLP tools must evolve into a comprehensive solution that integrates with existing healthcare processes and solves some of the current problems.
â­ï¸Need to consider funding, resources, hardware and IT infrastructure, usability regulation, and many other aspects when updating or designing a future NLP solution.
ğŸš€ Is there a repository of NLP solutions being used in healthcare? One needs to search and compile this as an important resource."
305,Joseph Oxley,22 Nov 2024 11:35,Badge 14 - Mini-Diary: 3 stars and 1 wish â­ï¸â­ï¸â­ï¸ğŸš€,"â­ï¸ Importance of being able to explain how a NLP model made a decision, so that trust in NLP increases and models are able to be more widely used in a clinical setting.

â­ï¸Importance of issues such as bias to ensure NLP models are equally applicable and that the performance of the models are not different between groups. (As this could worsen health outcomes for some groups as models become more widely used.)

â­ï¸Importance of the user interface and how any model or tool fits into a workflow. This could involve connecting directly to clinical systems so data does not need to be manually input to a model separately, or the model being able to return a prediction quickly instead of relying on external processing.

ğŸš€Any information on if the NHS in the UK has a formal strategy to adopt and implement NLP or Machine learning approaches going into the future? Or if it is still very much the case of development and implementation of these models varying across the system."
306,Swe Lynn,24 Dec 2024 13:45,Badge 15 - Mini-Diary: 3 stars and 1 wish â­ï¸â­ï¸â­ï¸ğŸš€,"â­ï¸ Process of text data to NLP outcome (text data Ã pre process (clean, tokenise) Ã  document vector (explore/visualise) Ã NLP models Ã classify/ outcome)
â­ï¸Measuring the performance (Accuracy, Precision, Recall, F1 score) and keeping the NLP models sustainable
â­ï¸ Ethics, bias, privacy, patient safety ( these are essential part for me especially around patient safety).
ğŸš€ I will most like spend more time to go through the coding sections in the course. I feel I need to explore more on what area of NLP is best for my current career and interest and then learn further around that."
307,Rebecca Sewell,9 Dec 2024 11:38,Badge 15 - Mini-Diary: 3 stars and 1 wish â­ï¸â­ï¸â­ï¸ğŸš€,"â­ï¸Great pulling together and overview of learnings and challenges in this field, helped me with knowing what I want to investigate next.
â­ï¸Definitely made me feel the importance of being up-to-date with data security and information privacy regulations!
â­ï¸It has been great to hear from people already engaged in using NLP in a healthcare setting during this course, this helped me to think more about context and what the current challenges are, and made it feel like the learning was up-to-date and relevant.
ğŸš€I will definitely be exploring more about transformer-based models and am excited keep building on my NLP skills in general."
308,Yingqian Tang,9 Dec 2024 11:37,Badge 15 - Mini-Diary: 3 stars and 1 wish â­ï¸â­ï¸â­ï¸ğŸš€,"ğŸŒŸI learned how crucial it is to address ethical issues and bias in NLP models, especially in healthcare, to ensure fairness and accuracy.
ğŸŒŸI enjoyed discovering how practical experience in NLP can be applied to real-world challenges in health and social care settings.
ğŸŒŸI appreciated the emphasis on continually deepening both technical expertise and domain knowledge to stay relevant in healthcare sector.
â“ I want to better understand the specific policies and guidelines that govern NLP applications in health and social care. "
309,Helen Wu,8 Dec 2024 20:06,Badge 15 - Mini-Diary: 3 stars and 1 wish â­ï¸â­ï¸â­ï¸ğŸš€,"â­ï¸Recognising the nuances of NLP and how domain-specific data is important for effective NLP applications
â­ï¸Good reminder to always think about the bias and ethical considerations behind developing and utilising NLP applications
â­ï¸Overall a good wrap up but also understanding where to go from here, what are more topics to look into.
ğŸš€Going to do more research on the practical applications of NLP within the healthcare spaces and the barriers or complications people have to overcome to improve the NLP application."
310,Kate Browne,8 Dec 2024 18:41,Badge 15 - Mini-Diary: 3 stars and 1 wish â­ï¸â­ï¸â­ï¸ğŸš€,"â­ Recognising challenges of working with text data (e.g. nuances, unstructured, context-heavy, domain-specific etc) is the first step of effective NLP applications;
â­ Overview of some of the NLP techniques that I learned on the course;
â­ Reminder to proactively address potential biases in data and algorithms to mitigate risks of discrimination and promote trustworthiness;
â“I would like to learn more about tranformer based models such as BERT or Chat GPT and fine tuning these models and also relevant use cases for these."
311,Nasser Gaafar,8 Dec 2024 17:32,Badge 15 - Mini-Diary: 3 stars and 1 wish â­ï¸â­ï¸â­ï¸ğŸš€,"â­ NLP is a rapidly evolving and expanding field
â­ Great wrap up
â­ Bias minimisation and ethical considerations are incredibly important
â“ I think I need a new laptop!"
312,Louise Lau,7 Dec 2024 19:09,Badge 15 - Mini-Diary: 3 stars and 1 wish â­ï¸â­ï¸â­ï¸ğŸš€,"â­ Techniques such as federated learning as the potential options for privacy protection.
â­ Great summary of the key aspects of NLP, which will guide further learning to deepen the knowledge in a more structured way
â­ Reminder that bias and ethics issues should always be considered throughout the development and application of NLP tools
â“ From the badge exercise I noticed that some of the NLP tools are computational heavy and hard to run without crushing - would be curious to learn how real-world development take care of these problem"
313,Paolo Pricoco,5 Dec 2024 11:59,Badge 15 - Mini-Diary: 3 stars and 1 wish â­ï¸â­ï¸â­ï¸ğŸš€,"ğŸŒŸ Technical coding skill improvement does not guarantee an NLP professional to be well-trained and effective in a particular sector (e.g., healthcare). The specific, often fast-pace-changing contingencies of healthcare (as well as many areas of work) require NLP practitioners to keep up to date not only with the new coding languages and packages, but also with healthcare policies, epidemics and spread of new diseases, socio-economical issues affecting healthcare services in various countries, etc. Also, thanks to the advancements of research, treatment and diagnosis improve and change over time.

ğŸŒŸ Sensitivity towards social causes, too, should not be overlooked by NLP practitioners. Ignoring these may lead to enhance disparities and biases based on social groups. Inequalities based on gender, economic status, ethnicity and religion among others should be considered when creating NLP algorithms, as the risk for this type of biases being already present in the analysed text is very high.

ğŸŒŸ With regards to merely technical aspects, coders can hone their coding skills by subscribing to courses specifically designed for intermediate or advanced NLP online. Several online courses are also offered for free or at a very low price.

â“ I am wondering if there are any courses explaining how to address social biases in language and outlining what the main challenges are for NLP practitioners in relation to social bias are. I think it would help creating better-informed NLP coders, which would be especially important for areas, such as healthcare, where very sensitive data is managed."
314,Nisha Daniel,5 Dec 2024 11:39,Badge 15 - Mini-Diary: 3 stars and 1 wish â­ï¸â­ï¸â­ï¸ğŸš€,"â­ Good wholistic technical revision
â­ Curious about policy guidelines and infusing ethical aspects into it.
â­ NLP is the trend of future healthcare and our expertise is essential to mulltiple health innovations.
â“Excited about real-world scenarios"
315,Kate Rose,2 Dec 2024 21:08,Badge 15 - Mini-Diary: 3 stars and 1 wish â­ï¸â­ï¸â­ï¸ğŸš€,"â­ Nice to reflect on all the different aspects of NLP we have explored during each badge and week of the course, recognising that NLP is a very rapidly evolving space within health and social care
â­ The impact of ethics and bias and the criticality to ensure bias is accounted for when building NLP models, as this has the potential to directly impact patient outcomes
â­ The importance of understanding the contextual background on the data you are working with to create/train NLP models to ensure the end product is both safe and effective
â“I would love to have more time to explore many elements of the course in further detail, particularly BIO tagging and also further understanding of how bias can be eliminated when building models. It would be great to have time to get involved in a shared project"
316,Isabel Santonja,2 Dec 2024 12:54,Badge 15 - Mini-Diary: 3 stars and 1 wish â­ï¸â­ï¸â­ï¸ğŸš€,"ğŸŒŸ Foundations in NLP are crucial to build effective NLP tools.
ğŸŒŸ Domain expertise is as important as technical expertise.
ğŸŒŸDeepening technical knowledge in a particular area or technique I am interested might be the next step in my NLP learning journey!

ğŸ I have to look to some uses of NLP in epidemiology to make use of routinely collected data!"
317,Adijat Adenaike,1 Dec 2024 23:24,Badge 15 - Mini-Diary: 3 stars and 1 wish â­ï¸â­ï¸â­ï¸ğŸš€,"â­Recommended progression for the course which include deep learning for NLP, Transformer Based Models like GPT amongst others.
â­Be proficient in what we do, as domain knowledge is as important as technical skill.
â­Federated Learning and differential privacy are techniques that can be applied to health data to ensure privacy and security are maintained.
â“I really enjoyed this course but I wish there is an option to take it with python as I have always used R in all my courses which would have been a great opportunity for me to learn some python syntax."
318,Robin Nicholls,1 Dec 2024 19:28,Badge 15 - Mini-Diary: 3 stars and 1 wish â­ï¸â­ï¸â­ï¸ğŸš€,"â­ï¸ Lately I have almost exclusively been using Python for data science, so using R and learning about the NLP libraries it has was great.
â­ï¸ It was great to learn about NLP problems in health and social care, and how they might be solved.
â­ï¸ Good suggestions for next steps, especially ethics and Bias which I will be studying next semester!
ğŸš€ I use NLP techniques frequently in my current role, and I hope to choose a thesis that will allow me to incorporate what I have learned in this course."
319,Franz KrÃ¤mer,28 Nov 2024 16:02,Badge 15 - Mini-Diary: 3 stars and 1 wish â­ï¸â­ï¸â­ï¸ğŸš€,"â­ï¸ There is so much more in NLP than developing tools, for example, considering contexts, involving domain experts and future users, integrating with work flows and systems, thinking about how to maintain an NLP tool for the future and considering ethical challenges and mitigating risks.
â­ï¸ Enjoyed learning about NLP applications and challenges in the healthcare field!
â­ï¸ Practical tools to use, and ideas and sources of information to look further.
ğŸš€ I will look into how to incorporate NLP into my thesis (in the Data Science, Technology and Innovation programme) as I found the contents of this course very interesting and well applicable to my field."
320,Yi mei Low,27 Nov 2024 16:22,Badge 15 - Mini-Diary: 3 stars and 1 wish â­ï¸â­ï¸â­ï¸ğŸš€,"â­ The breadth of possibilities when it comes to how NLP-based solutions can be applied to the health and social care sector
â­When starting/charting out an NLP project, what are the initial steps - from ensuring the ethical standards are met to ensuring there is a valid justification for doing the project
â­Subsequently integrating all the steps to then build the appropriate model and assessing its performance
ğŸš€Some potential NLP projects that we can look at for our dissertation! :)"
321,Dylan Delmar,27 Nov 2024 00:18,Badge 15 - Mini-Diary: 3 stars and 1 wish â­ï¸â­ï¸â­ï¸ğŸš€,"â­ How important recognizing challenges is as a first step in NLP
â­ Ideas for how to continue gaining knowledge on NLP and deepening understanding of how it works within the health and social care world
â­ Areas of expertise within NLP and how they can contribute to the same field
ğŸš€ Specific projects to work on and the potential for an NLP dissertation project"
322,Joseph Oxley,25 Nov 2024 10:35,Badge 15 - Mini-Diary: 3 stars and 1 wish â­ï¸â­ï¸â­ï¸ğŸš€,"â­ï¸ Ideas of how to continue to learn NLP, such as learning more about the detail behind the techniques introduced in the course.
â­ï¸ Area of work where NLP skills can be applied, such as shared projects on publicly released data.
â­ï¸ How NLP, particularly in health & social care, remains an emerging field so new opportunities to use NLP algorithms will continue to emerge in the future.
ğŸš€ If there's any plans to offer NLP based projects for the dissertation next year, as I think using NLP to answer a research question could be very exciting! Or if there are more advanced courses the university runs."
323,Amit Mishra,24 Nov 2024 16:11,Badge 15 - Mini-Diary: 3 stars and 1 wish â­ï¸â­ï¸â­ï¸ğŸš€,"â­ï¸ Summary of functions learned to deal with the text data- preprocessing, cleaning, classification, analysis, visualization, named entity recognition, measuring performance, and ethics and biases in working with text data. 
â­ï¸ Where to look for additional course material and resources to build skills in specific areas. 
â­ï¸ What are the challenges of working with domain-specific text data and why and how standard tools may not be applicable in different domains and settings? 
ğŸš€ If there is a working professionals group that we can join to discuss this topic further and update our knowledge in future. "
